{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:41:35.974087Z",
     "start_time": "2024-11-19T05:41:35.177247Z"
    }
   },
   "source": [
    "# 安装方式: pip install gensim -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Gensim中，有几个核心概念，包括文档（Document）、语料库（Corpus）、向量（Vector）和模型（Model）。文档是一些文本，语料库是文档的集合，向量是文档在数学上的表示形式，而模型则是将一种文档向量表示转换成另一种表示的算法。\n",
    "\n",
    "官网文档：https://radimrehurek.com/gensim/apiref.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字典演示"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:44:38.948576Z",
     "start_time": "2024-11-19T05:44:38.939990Z"
    }
   },
   "source": [
    "import jieba\n",
    "text1 = \"我是来自湖南张家界的小明，我喜好大海\\n我从事IT相关工作\\n我讨厌夏天\"\n",
    "text2 = \"计算机视觉和自然语言我比较喜好自然语言的内容\"\n",
    "text3 = \"我不想上班，我想出去玩\"\n",
    "\n",
    "# 正常的文本构建(针对每个文本进行分词)\n",
    "docs = []\n",
    "for text in [text1, text2, text3]:\n",
    "    # text.replace('\\n', '') 用于移除文本中的换行符\n",
    "    # jieba.lcut 直接返回了一个包含分词结果的列表\n",
    "    docs.append(list(jieba.lcut(text.replace('\\n', ''))))\n",
    "# 构建词典\n",
    "dct = Dictionary(docs)\n",
    "print(docs)\n",
    "print(dct)\n",
    "print(f\"去重后单词数目/词典大小:{len(dct)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['我', '是', '来自', '湖南', '张家界', '的', '小明', '，', '我', '喜好', '大海', '我', '从事', 'IT', '相关', '工作', '我', '讨厌', '夏天'], ['计算机', '视觉', '和', '自然语言', '我', '比较', '喜好', '自然语言', '的', '内容'], ['我', '不想', '上班', '，', '我', '想', '出去玩']]\n",
      "Dictionary<26 unique tokens: ['IT', '从事', '喜好', '夏天', '大海']...>\n",
      "去重后单词数目/词典大小:26\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:44:48.755424Z",
     "start_time": "2024-11-19T05:44:48.747455Z"
    }
   },
   "source": [
    "# 属性\n",
    "vars(dct)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token2id': {'IT': 0,\n",
       "  '从事': 1,\n",
       "  '喜好': 2,\n",
       "  '夏天': 3,\n",
       "  '大海': 4,\n",
       "  '小明': 5,\n",
       "  '工作': 6,\n",
       "  '张家界': 7,\n",
       "  '我': 8,\n",
       "  '是': 9,\n",
       "  '来自': 10,\n",
       "  '湖南': 11,\n",
       "  '的': 12,\n",
       "  '相关': 13,\n",
       "  '讨厌': 14,\n",
       "  '，': 15,\n",
       "  '内容': 16,\n",
       "  '和': 17,\n",
       "  '比较': 18,\n",
       "  '自然语言': 19,\n",
       "  '视觉': 20,\n",
       "  '计算机': 21,\n",
       "  '上班': 22,\n",
       "  '不想': 23,\n",
       "  '出去玩': 24,\n",
       "  '想': 25},\n",
       " 'id2token': {},\n",
       " 'cfs': {8: 7,\n",
       "  9: 1,\n",
       "  10: 1,\n",
       "  11: 1,\n",
       "  7: 1,\n",
       "  12: 2,\n",
       "  5: 1,\n",
       "  15: 2,\n",
       "  2: 2,\n",
       "  4: 1,\n",
       "  1: 1,\n",
       "  0: 1,\n",
       "  13: 1,\n",
       "  6: 1,\n",
       "  14: 1,\n",
       "  3: 1,\n",
       "  21: 1,\n",
       "  20: 1,\n",
       "  17: 1,\n",
       "  19: 2,\n",
       "  18: 1,\n",
       "  16: 1,\n",
       "  23: 1,\n",
       "  22: 1,\n",
       "  25: 1,\n",
       "  24: 1},\n",
       " 'dfs': {8: 3,\n",
       "  9: 1,\n",
       "  10: 1,\n",
       "  11: 1,\n",
       "  7: 1,\n",
       "  12: 2,\n",
       "  5: 1,\n",
       "  15: 2,\n",
       "  2: 2,\n",
       "  4: 1,\n",
       "  1: 1,\n",
       "  0: 1,\n",
       "  13: 1,\n",
       "  6: 1,\n",
       "  14: 1,\n",
       "  3: 1,\n",
       "  21: 1,\n",
       "  20: 1,\n",
       "  17: 1,\n",
       "  19: 1,\n",
       "  18: 1,\n",
       "  16: 1,\n",
       "  23: 1,\n",
       "  22: 1,\n",
       "  25: 1,\n",
       "  24: 1},\n",
       " 'num_docs': 3,\n",
       " 'num_pos': 36,\n",
       " 'num_nnz': 31,\n",
       " 'lifecycle_events': [{'msg': \"built Dictionary<26 unique tokens: ['IT', '从事', '喜好', '夏天', '大海']...> from 3 documents (total 36 corpus positions)\",\n",
       "   'datetime': '2024-11-19T13:44:38.939990',\n",
       "   'gensim': '4.3.0',\n",
       "   'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]',\n",
       "   'platform': 'Windows-10-10.0.22621-SP0',\n",
       "   'event': 'created'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:44:55.117359Z",
     "start_time": "2024-11-19T05:44:55.107596Z"
    }
   },
   "source": [
    "# # # 方法和属性，以及继承基类的方法和属性 # # # \n",
    "dir(dct)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " 'add_documents',\n",
       " 'add_lifecycle_event',\n",
       " 'cfs',\n",
       " 'compactify',\n",
       " 'dfs',\n",
       " 'doc2bow',\n",
       " 'doc2idx',\n",
       " 'filter_extremes',\n",
       " 'filter_n_most_frequent',\n",
       " 'filter_tokens',\n",
       " 'from_corpus',\n",
       " 'from_documents',\n",
       " 'get',\n",
       " 'id2token',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'iterkeys',\n",
       " 'itervalues',\n",
       " 'keys',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'load_from_text',\n",
       " 'merge_with',\n",
       " 'most_common',\n",
       " 'num_docs',\n",
       " 'num_nnz',\n",
       " 'num_pos',\n",
       " 'patch_with_special_tokens',\n",
       " 'save',\n",
       " 'save_as_text',\n",
       " 'token2id',\n",
       " 'values']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:45:01.243403Z",
     "start_time": "2024-11-19T05:45:01.239064Z"
    }
   },
   "source": [
    "dct.token2id"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IT': 0,\n",
       " '从事': 1,\n",
       " '喜好': 2,\n",
       " '夏天': 3,\n",
       " '大海': 4,\n",
       " '小明': 5,\n",
       " '工作': 6,\n",
       " '张家界': 7,\n",
       " '我': 8,\n",
       " '是': 9,\n",
       " '来自': 10,\n",
       " '湖南': 11,\n",
       " '的': 12,\n",
       " '相关': 13,\n",
       " '讨厌': 14,\n",
       " '，': 15,\n",
       " '内容': 16,\n",
       " '和': 17,\n",
       " '比较': 18,\n",
       " '自然语言': 19,\n",
       " '视觉': 20,\n",
       " '计算机': 21,\n",
       " '上班': 22,\n",
       " '不想': 23,\n",
       " '出去玩': 24,\n",
       " '想': 25}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:45:10.266410Z",
     "start_time": "2024-11-19T05:45:10.257489Z"
    }
   },
   "source": [
    "dct.save_as_text('./datas/a.txt')\n",
    "print(dct.token2id['上班'])\n",
    "print(dct[22])\n",
    "print(dct[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "上班\n",
      "IT\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:21:13.734069Z",
     "start_time": "2024-11-14T12:21:13.717663Z"
    }
   },
   "source": [
    "n = len(dct) + 1 # 词典大小 = 实际词典大小 + 1\n",
    "text4 = \"我是来自北京的小明的朋友\"\n",
    "text4_words = list(jieba.lcut(text4.replace('\\n', '')))\n",
    "# 使用 dct.doc2idx 方法将分词后的列表 text4_words 转换为索引列表，未知词汇会被标记为 -1。然后将索引加1，以符合从1开始的索引\n",
    "result = list(np.asarray(dct.doc2idx(text4_words, unknown_word_index=-1)) + 1)\n",
    "print(text4_words)\n",
    "print(\"序号化结果:\")\n",
    "print(result)\n",
    "\n",
    "print(\"OneHot结果:\")\n",
    "result2 = [[0] * n for _ in range(len(result))]\n",
    "for i,_id in enumerate(result):\n",
    "    if _id != -1:\n",
    "        result2[i][_id] = 1\n",
    "print(result2)\n",
    "\n",
    "print(\"词袋法结果:\")\n",
    "# 将One-Hot按列求和\n",
    "result3 = list(np.sum(np.asarray(result2), 0))\n",
    "print(result3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '是', '来自', '北京', '的', '小明', '的', '朋友']\n",
      "序号化结果:\n",
      "[9, 10, 11, 0, 13, 6, 13, 0]\n",
      "OneHot结果:\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "词袋法结果:\n",
      "[2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:24:36.609877Z",
     "start_time": "2024-11-14T12:24:36.605065Z"
    }
   },
   "source": [
    "text4 = \"我是来自北京的小明，我喜好玩游戏\"\n",
    "text4_words = list(jieba.lcut(text4.replace('\\n', '')))\n",
    "result = dct.doc2idx(text4_words)\n",
    "print(text4_words)\n",
    "print(\"序号化结果:\")\n",
    "print(result)\n",
    "\n",
    "# 手动\n",
    "print(\"OneHot结果:\")\n",
    "result2 = [[0] * len(dct) for _ in range(len(result))]\n",
    "for i,_id in enumerate(result):\n",
    "    if _id != -1:\n",
    "        result2[i][_id] = 1\n",
    "print(result2)\n",
    "\n",
    "print(\"词袋法结果:\")\n",
    "result3 = list(np.sum(np.asarray(result2), 0))\n",
    "print(result3)\n",
    "# doc2bow 将 text4_words 中的词汇转换为 (词汇ID, 出现次数) 的列表\n",
    "result4 = dct.doc2bow(text4_words)\n",
    "print(result4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '是', '来自', '北京', '的', '小明', '，', '我', '喜好', '玩游戏']\n",
      "序号化结果:\n",
      "[8, 9, 10, -1, 12, 5, 15, 8, 2, -1]\n",
      "OneHot结果:\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "词袋法结果:\n",
      "[0, 0, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(2, 1), (5, 1), (8, 2), (9, 1), (10, 1), (12, 1), (15, 1)]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:49:36.619255Z",
     "start_time": "2024-11-19T05:49:36.614270Z"
    }
   },
   "source": [
    "print(docs[0])\n",
    "print(docs[1])\n",
    "print(docs[2])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '是', '来自', '湖南', '张家界', '的', '小明', '，', '我', '喜好', '大海', '我', '从事', 'IT', '相关', '工作', '我', '讨厌', '夏天']\n",
      "['计算机', '视觉', '和', '自然语言', '我', '比较', '喜好', '自然语言', '的', '内容']\n",
      "['我', '不想', '上班', '，', '我', '想', '出去玩']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:50:04.982843Z",
     "start_time": "2024-11-19T05:50:04.977370Z"
    }
   },
   "source": [
    "# 创建语料库\n",
    "corpus = [dct.doc2bow(text) for text in docs]\n",
    "print(corpus[0])  # 单词的id和在文档中出现的次数"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 4), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:55:38.512566Z",
     "start_time": "2024-11-19T05:55:38.181905Z"
    }
   },
   "source": [
    "# 将文档列表转换成词袋模型表示形式\n",
    "# dct.doc2bow(doc) 方法将每个文档转换成词袋模型\n",
    "corpus = [dct.doc2bow(doc) for doc in docs]  # 词袋\n",
    "\n",
    "# 使用 corpus 创建 TF-IDF 模型\n",
    "model = TfidfModel(corpus=corpus)            # TF-IDF\n",
    "\n",
    "# 输出第一个文档经过 TF-IDF 转换后的特征向量的维度大小\n",
    "# 注意：model[corpus[0]] 返回的是一个列表，其中元素是 (word_id, tfidf_value) 形式的元组，\n",
    "print(\"维度大小:{}\".format(len(model[corpus[0]])))\n",
    "\n",
    "# 输出词汇表，即每个单词对应的 ID\n",
    "print(\"词汇表：\", dct.token2id)\n",
    "\n",
    "# 输出第一个文档的 TF-IDF 值\n",
    "model[corpus[0]]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度大小:15\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dictionary' object has no attribute 'tokenid'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m维度大小:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(model[corpus[\u001B[38;5;241m0\u001B[39m]])))\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 输出词汇表，即每个单词对应的 ID\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m词汇表：\u001B[39m\u001B[38;5;124m\"\u001B[39m, dct\u001B[38;5;241m.\u001B[39mtokenid)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 输出第一个文档的 TF-IDF 值\u001B[39;00m\n\u001B[0;32m     16\u001B[0m model[corpus[\u001B[38;5;241m0\u001B[39m]]\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Dictionary' object has no attribute 'tokenid'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:56:12.098838Z",
     "start_time": "2024-11-19T05:56:12.095411Z"
    }
   },
   "source": [
    "# 获取第一个文档的TF-IDF值\n",
    "tfidf_values = model[corpus[0]]\n",
    "\n",
    "# 打印TF-IDF值，并映射ID到实际的单词\n",
    "for word_id, value in tfidf_values:\n",
    "    print(f\"{dct[word_id]}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT: 0.28388205020418034\n",
      "从事: 0.28388205020418034\n",
      "喜好: 0.10477241822549672\n",
      "夏天: 0.28388205020418034\n",
      "大海: 0.28388205020418034\n",
      "小明: 0.28388205020418034\n",
      "工作: 0.28388205020418034\n",
      "张家界: 0.28388205020418034\n",
      "是: 0.28388205020418034\n",
      "来自: 0.28388205020418034\n",
      "湖南: 0.28388205020418034\n",
      "的: 0.10477241822549672\n",
      "相关: 0.28388205020418034\n",
      "讨厌: 0.28388205020418034\n",
      "，: 0.10477241822549672\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T05:57:35.390812Z",
     "start_time": "2024-11-19T05:57:35.382141Z"
    }
   },
   "source": [
    "# 获取第一个文档的TF-IDF值\n",
    "tfidf_values = model[corpus[0]]\n",
    "\n",
    "# 创建逆映射\n",
    "id2token = {v: k for k, v in dct.token2id.items()}\n",
    "\n",
    "# 打印TF-IDF值，并映射ID到实际的单词\n",
    "print(\"TF-IDF values for the first document:\")\n",
    "for word_id, value in tfidf_values:\n",
    "    print(f\"{dct[word_id]}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF values for the first document:\n",
      "IT: 0.28388205020418034\n",
      "从事: 0.28388205020418034\n",
      "喜好: 0.10477241822549672\n",
      "夏天: 0.28388205020418034\n",
      "大海: 0.28388205020418034\n",
      "小明: 0.28388205020418034\n",
      "工作: 0.28388205020418034\n",
      "张家界: 0.28388205020418034\n",
      "是: 0.28388205020418034\n",
      "来自: 0.28388205020418034\n",
      "湖南: 0.28388205020418034\n",
      "的: 0.10477241822549672\n",
      "相关: 0.28388205020418034\n",
      "讨厌: 0.28388205020418034\n",
      "，: 0.10477241822549672\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、加载数据(数据预处理)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:05:01.032057Z",
     "start_time": "2024-11-19T06:05:01.025688Z"
    }
   },
   "source": [
    "# 加载数据文件\n",
    "with open('./datas/first.txt', 'r', encoding='utf-8') as reader:\n",
    "    content = reader.read()\n",
    "# 读取文件内容并存储在变量 content 中\n",
    "\n",
    "# 划分单词，并转换为二进制形式\n",
    "# 使用 lambda 函数过滤掉空格和空白字符，并将每个单词编码为 UTF-8 字节串\n",
    "words = list(map(lambda word: word.encode(\"utf-8\"), filter(lambda t: t.strip(), content.split(\" \"))))\n",
    "\n",
    "# 计算总单词数量\n",
    "total_words = len(words)\n",
    "print(\"总单词数目:\", total_words)\n",
    "print(\"总单词数目:{}\".format(total_words))  # format告诉他要把total_words放进{}里\n",
    "print(\"【前10个单词】:{}\".format(words[:10]))\n",
    "\n",
    "# 将其转换为文档的形式（必须，也就是一个文档可能存在多个单词）\n",
    "# 模拟的方式：模拟多个文档\n",
    "# 设置每个文档包含的单词数量\n",
    "word_per_doc = 10000\n",
    "\n",
    "# 初始化文档列表\n",
    "docs = []\n",
    "\n",
    "# 遍历所有单词，按设定的数量分隔成不同的文档\n",
    "for i in range(total_words // word_per_doc + 1):\n",
    "    # 计算当前文档的起始和结束索引\n",
    "    start_idx = i * word_per_doc\n",
    "    end_idx = start_idx + word_per_doc\n",
    "    \n",
    "    # 获取当前文档的单词列表\n",
    "    tmp_words = words[start_idx:end_idx]\n",
    "    \n",
    "    # 如果当前文档的单词列表非空，则保存至文档列表中\n",
    "    if len(tmp_words) > 0:\n",
    "        docs.append(tmp_words)\n",
    "\n",
    "# 输出总文档数目\n",
    "print(\"总文档数目:{}\".format(len(docs)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总单词数目: 1104\n",
      "总单词数目:1104\n",
      "【前10个单词】:[b'In', b'this', b'age', b'of', b'rapid', b'internet', b'growth,', b'influencer', b'marketing', b'has']\n",
      "总文档数目:1\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:08:13.888450Z",
     "start_time": "2024-11-19T06:08:13.884458Z"
    }
   },
   "source": [
    "content[:35]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this age of rapid internet growt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:08:18.642165Z",
     "start_time": "2024-11-19T06:08:18.637314Z"
    }
   },
   "source": [
    "words[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'In', b'this', b'age', b'of', b'rapid']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、构建词典"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:06:10.781330Z",
     "start_time": "2024-11-14T13:06:10.771123Z"
    }
   },
   "source": [
    "# 构建词典\n",
    "# docs中必须是文档，文档内必须是一个一个的单词\n",
    "# eg: docs --> list(list(str)) --> [['a', 'bv', 'c'], ['a', 'c'], ['d', 'f', ' f']]\n",
    "dct = Dictionary(docs)\n",
    "print(f\"词典大小:{len(dct)}\")\n",
    "print(f\"{len(dct.token2id)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小:504\n",
      "504\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、BOW词袋法转换"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:08:33.082119Z",
     "start_time": "2024-11-19T06:08:33.079084Z"
    }
   },
   "source": [
    "# 做一个词袋法转换(以dct中找到的单词作为特征属性，以文本中出现的数量作为特征值)\n",
    "corpus = [dct.doc2bow(line) for line in docs]"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:14:38.557465Z",
     "start_time": "2024-11-19T06:14:38.553320Z"
    }
   },
   "source": "corpus[0]  # (id, frequency)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、TF-IDF构建"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:14:47.132947Z",
     "start_time": "2024-11-19T06:14:47.129922Z"
    }
   },
   "source": [
    "model = TfidfModel(corpus=corpus)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、TF-IDF应用"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T06:14:53.151427Z",
     "start_time": "2024-11-19T06:14:53.146915Z"
    }
   },
   "source": [
    "print(\"维度大小:{}\".format(np.shape(model[corpus[0]])))\n",
    "model[corpus[0]]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度大小:(0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T06:14:57.298077Z",
     "start_time": "2024-11-19T06:14:57.293404Z"
    }
   },
   "source": [
    "# 针对其它字符串进行词向量转换\n",
    "others = [\n",
    "    ['my', 'name','name', 'is', 'gerry'],               # 分词\n",
    "    ['my', 'name', 'is', 'xiaoming']\n",
    "]\n",
    "other_corpus = [dct.doc2bow(line) for line in others]   # 词袋\n",
    "vectors = model[other_corpus]                           # tfidf\n",
    "for vector in vectors:\n",
    "    print(vector)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
