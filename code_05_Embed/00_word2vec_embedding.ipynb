{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T09:49:40.331889Z",
     "start_time": "2025-01-04T09:49:36.751422Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(24)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20f5fd9b3b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T09:49:42.027429Z",
     "start_time": "2025-01-04T09:49:41.993252Z"
    }
   },
   "source": [
    "linear = nn.Linear(in_features=4, out_features=5, bias=False)\n",
    "# 权重矩阵的行数等于输出特征数（5），列数等于输入特征数（4）\n",
    "print (f\"linear weight shape: {linear.weight.shape}\")   # [5, 4]\n",
    "x = torch.rand(2, 4)  # [2, 4]\n",
    "y1 = linear(x)   # [2, 4]*[4,5] - > [2,5]\n",
    "print(y1.shape)\n",
    "\"\"\"\n",
    "x代表两个文本 文本各有四个特征\n",
    "linear代表全连接 将四个特征投射到五个特征\n",
    "特征就是特定的文本\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear weight shape: torch.Size([5, 4])\n",
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nx代表两个文本 文本各有四个特征\\nlinear代表全连接 将四个特征投射到五个特征\\n特征就是特定的文本\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.matmul"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:27:06.699308Z",
     "start_time": "2024-11-20T02:27:06.692029Z"
    }
   },
   "source": [
    "# y2 = x @ linear.weight.T\n",
    "y2 = torch.matmul(x, linear.weight.T) # [2,4]*[4,5] --> [2,5]\n",
    "print(y2.shape)\n",
    "\n",
    "print(torch.mean(y1 - y2).item())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "0.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x_idx & x"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:27:08.181969Z",
     "start_time": "2024-11-20T02:27:08.172440Z"
    }
   },
   "source": [
    "# 3个文档，每个文档2个词\n",
    "x_idx = [\n",
    "    [0, 1],  # 类别序号/下标， 每篇文章有相同数量的单词，切割或者填充到指定长度\n",
    "    [0, 3],  # 若单词的种类很多但是标签大的单词数量又少 此时onehot就不是很好用了\n",
    "    [1, 2]\n",
    "]\n",
    "# onehot\n",
    "x = [\n",
    "    [1, 1, 0, 0],  # 文本1\n",
    "    [1, 0, 0, 1],  # 文本2\n",
    "    [0, 1, 1, 0]  # 文本3\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:30:09.556060Z",
     "start_time": "2024-11-20T02:30:09.540713Z"
    }
   },
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)  # [n,m] n个样本，m维大小(词表的大小)\n",
    "y1 = linear(x)  # [n,m]*[m,o] -> [n,o] o表示输出特征向量大小 --> 计算量n*m*o\n",
    "print(y1)   # 文档集的表征\n",
    "print(linear.weight.T[0] + linear.weight.T[1] )   # 第一篇文章的表征=第一行+第二行\n",
    "print(linear.weight.T[0] + linear.weight.T[3] )   # 第二篇文章的表征=第一行+第四行\n",
    "print(linear.weight.T[1] + linear.weight.T[2] )   # 第三篇文章的表征=第二行+第三行"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1395,  0.2430, -0.2605, -0.5502,  0.2602],\n",
      "        [ 0.2952,  0.8570, -0.0513, -0.5145,  0.3064],\n",
      "        [-0.5499, -0.3858, -0.2917,  0.3419, -0.3963]], grad_fn=<MmBackward0>)\n",
      "tensor([ 0.1395,  0.2430, -0.2605, -0.5502,  0.2602], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.2952,  0.8570, -0.0513, -0.5145,  0.3064], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\五行缺钱\\AppData\\Local\\Temp\\ipykernel_10212\\2876828208.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)  # [n,m] n个样本，m维大小(词表的大小)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:27:11.190153Z",
     "start_time": "2024-11-20T02:27:11.181237Z"
    }
   },
   "source": [
    "w = linear.weight.T  # [4, 5]  在这里，w就表示每个单词对应一个稠密的特征向量\n",
    "print(w.shape)\n",
    "print(w)\n",
    "print (np.reshape(x_idx, -1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "tensor([[ 0.2644,  0.4660, -0.0696, -0.4073,  0.4554],\n",
      "        [-0.1249, -0.2230, -0.1910, -0.1429, -0.1952],\n",
      "        [-0.4249, -0.1628, -0.1007,  0.4848, -0.2011],\n",
      "        [ 0.0308,  0.3910,  0.0183, -0.1072, -0.1490]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "[0 1 0 3 1 2]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:27:12.829991Z",
     "start_time": "2024-11-20T02:27:12.814527Z"
    }
   },
   "source": [
    "w[np.reshape(x_idx, -1)]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2644,  0.4660, -0.0696, -0.4073,  0.4554],\n",
       "        [-0.1249, -0.2230, -0.1910, -0.1429, -0.1952],\n",
       "        [ 0.2644,  0.4660, -0.0696, -0.4073,  0.4554],\n",
       "        [ 0.0308,  0.3910,  0.0183, -0.1072, -0.1490],\n",
       "        [-0.1249, -0.2230, -0.1910, -0.1429, -0.1952],\n",
       "        [-0.4249, -0.1628, -0.1007,  0.4848, -0.2011]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:27:20.136774Z",
     "start_time": "2024-11-20T02:27:20.126211Z"
    }
   },
   "source": [
    "r = w[np.reshape(x_idx, -1)].reshape(-1, 2, 5) # 每个单词对应一个横向的向量\n",
    "print(r.shape)\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n",
      "tensor([[[ 0.2644,  0.4660, -0.0696, -0.4073,  0.4554],\n",
      "         [-0.1249, -0.2230, -0.1910, -0.1429, -0.1952]],\n",
      "\n",
      "        [[ 0.2644,  0.4660, -0.0696, -0.4073,  0.4554],\n",
      "         [ 0.0308,  0.3910,  0.0183, -0.1072, -0.1490]],\n",
      "\n",
      "        [[-0.1249, -0.2230, -0.1910, -0.1429, -0.1952],\n",
      "         [-0.4249, -0.1628, -0.1007,  0.4848, -0.2011]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[ 0.1395,  0.2430, -0.2605, -0.5502,  0.2602],\n",
      "        [ 0.2952,  0.8570, -0.0513, -0.5145,  0.3064],\n",
      "        [-0.5499, -0.3858, -0.2917,  0.3419, -0.3963]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "r = r.sum(dim=1)  # 合并到一起，得到每个文本对应一个向量 即为两个单词对应向量之和\n",
    "print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding\n",
    "\n",
    "nn.Embedding用于将离散的ID映射到连续的向量，而nn.Linear用于将连续的输入特征映射到连续的输出特征。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T05:54:41.919956Z",
     "start_time": "2024-11-27T05:54:41.908287Z"
    }
   },
   "source": [
    "torch.tensor(x_idx, dtype=torch.int64)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m torch\u001B[38;5;241m.\u001B[39mtensor(x_idx, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint64)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'x_idx' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T05:54:40.215484Z",
     "start_time": "2024-11-27T05:54:40.198217Z"
    }
   },
   "source": [
    "embed = nn.Embedding(num_embeddings=4, embedding_dim=5, _weight=w)\n",
    "print(w)\n",
    "r2 = embed(torch.tensor(x_idx, dtype=torch.int64))\n",
    "print(r2.shape)\n",
    "print(r2)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m embed \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(num_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, embedding_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, _weight\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(w)\n\u001B[0;32m      3\u001B[0m r2 \u001B[38;5;241m=\u001B[39m embed(torch\u001B[38;5;241m.\u001B[39mtensor(x_idx, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint64))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'w' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7090, -0.8770],\n",
       "        [ 0.7090,  0.1839],\n",
       "        [-0.8770, -0.4047]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum2 = r2.sum(dim=2)\n",
    "print (sum2.shape)\n",
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7090, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r2[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7089000000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2644+0.4660-0.0696-0.4073+0.4554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1395,  0.2430, -0.2605, -0.5502,  0.2602],\n",
       "        [ 0.2952,  0.8570, -0.0513, -0.5145,  0.3064],\n",
       "        [-0.5499, -0.3858, -0.2917,  0.3419, -0.3963]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum2 = r2.sum(dim=1)\n",
    "print (sum2.shape)\n",
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1395, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r2[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1395"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2644-0.1249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4039,  0.7090, -0.3301, -0.9576,  0.7156],\n",
       "        [-0.5191,  0.0052, -0.2734,  0.2347, -0.5454]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum2 = r2.sum(dim=0)\n",
    "print (sum2.shape)\n",
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4039, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(r2[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40390000000000004"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2644+0.2644-0.1249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runtime: linear vs Embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T02:59:45.028642Z",
     "start_time": "2024-11-20T02:59:43.413378Z"
    }
   },
   "source": [
    "import time\n",
    "n = 100000\n",
    "tx_idx = torch.tensor(x_idx, dtype=torch.int64)\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(n):\n",
    "    linear(x)\n",
    "\n",
    "t2 = time.time()\n",
    "for i in range(n):\n",
    "    embed(tx_idx).sum(dim=1)  # 得到文本向量\n",
    "t3 = time.time()\n",
    "print(t3 - t2, t2 - t1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0124552249908447 0.5925133228302002\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear & nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T03:05:29.012302Z",
     "start_time": "2024-11-20T03:05:29.003546Z"
    }
   },
   "source": [
    "vocab_size = 10000  # 10000个单词\n",
    "# 将每个单词映射到一个128维的向量空间中\n",
    "# 这里的“128”指的是向量的维度，而不是类别的数量。每个单词都被映射到这个高维空间中的一个点，而这个点的坐标（即向量的值）是通过模型学习得到的，\n",
    "# 能够反映单词的语义信息和上下文信息。不同单词的向量可以用于计算它们之间的相似度，或者作为机器学习模型的输入特征。\n",
    "linear = nn.Linear(in_features=vocab_size, out_features=128, bias=False)  # 输入特征数为单词的数量10000 输出为128\n",
    "embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=128, _weight=linear.weight.T)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T03:05:30.394811Z",
     "start_time": "2024-11-20T03:05:30.381255Z"
    }
   },
   "source": [
    "# x = torch.randint(0, vocab_size, size=(16,))  # [16,]\n",
    "x = torch.randint(0, vocab_size, size=(16,4))  # [16,] x将是一个形状为16x4的张量，其中的每个元素都是从0到9999之间的一个随机整数。\n",
    "print (x)\n",
    "x_onehot = F.one_hot(x, num_classes=vocab_size)  # [16,10000]\n",
    "x_onehot = x_onehot.to(torch.float32)\n",
    "x_onehot.shape"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5201, 5015, 8614, 8790],\n",
      "        [5308, 1772, 2039, 6459],\n",
      "        [4190, 2291, 4415, 6457],\n",
      "        [9011, 3615, 3342, 6473],\n",
      "        [ 570,  799, 7698, 7682],\n",
      "        [3532, 2425, 6052, 2040],\n",
      "        [ 310, 9880, 1542,  375],\n",
      "        [2362, 1892, 1865,  612],\n",
      "        [8985, 9904, 9304, 6305],\n",
      "        [4370, 6500, 8489, 6536],\n",
      "        [1830, 9250, 4491, 8797],\n",
      "        [1799, 2333, 5285,  727],\n",
      "        [2347, 1495, 2387,  255],\n",
      "        [1695, 5885, 8576, 3007],\n",
      "        [4671, 3274, 7818, 4286],\n",
      "        [1352,  801, 5166,  442]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 10000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T03:06:23.836819Z",
     "start_time": "2024-11-20T03:06:23.821402Z"
    }
   },
   "source": [
    "y1 = linear(x_onehot)\n",
    "y2 = embed(x)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(torch.mean(torch.abs(y1 - y2)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 128])\n",
      "tensor([[[-9.6898e-03, -5.0855e-03,  6.2885e-03,  ...,  8.3103e-03,\n",
      "          -5.6621e-03,  3.1911e-03],\n",
      "         [ 3.9277e-03,  4.8748e-03, -9.7823e-03,  ..., -8.5736e-03,\n",
      "           6.1870e-04,  9.3422e-03],\n",
      "         [ 2.2417e-03, -8.7106e-03,  6.9648e-03,  ...,  9.5045e-03,\n",
      "          -2.7521e-03,  4.2019e-03],\n",
      "         [-9.2161e-03,  8.3791e-06, -1.7729e-03,  ...,  3.7920e-03,\n",
      "           5.9289e-03,  6.9248e-03]],\n",
      "\n",
      "        [[ 5.8255e-03,  7.9945e-04, -7.2156e-03,  ...,  1.9155e-03,\n",
      "          -7.1027e-03,  9.1937e-03],\n",
      "         [-1.6206e-04, -3.4390e-04,  7.2426e-03,  ...,  3.1221e-03,\n",
      "           3.2408e-03, -6.7443e-03],\n",
      "         [ 7.6800e-03, -7.6522e-03,  9.9162e-03,  ..., -6.8345e-03,\n",
      "          -9.3088e-03,  2.6958e-03],\n",
      "         [ 5.0229e-03,  7.8355e-03, -2.4284e-03,  ..., -4.2366e-04,\n",
      "          -8.4012e-03, -9.5087e-03]],\n",
      "\n",
      "        [[ 6.4452e-03, -1.9305e-03, -3.9011e-03,  ...,  6.8925e-03,\n",
      "          -2.2651e-03, -5.5599e-03],\n",
      "         [-3.2580e-03,  2.0700e-03, -5.8630e-03,  ...,  2.6941e-03,\n",
      "          -1.1704e-03,  9.6810e-03],\n",
      "         [-3.6824e-03, -7.5407e-03,  8.4775e-03,  ..., -1.5156e-03,\n",
      "          -7.6015e-03, -2.8687e-04],\n",
      "         [ 8.7276e-03,  1.2050e-03,  1.6038e-03,  ...,  8.8302e-03,\n",
      "           6.3859e-03, -6.2813e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4858e-03,  6.4481e-04, -4.8107e-03,  ...,  5.8981e-03,\n",
      "           6.1642e-03, -3.3948e-03],\n",
      "         [ 1.1855e-03, -6.7237e-03,  1.9078e-03,  ...,  8.5611e-03,\n",
      "          -6.9346e-03, -8.5819e-04],\n",
      "         [-2.9001e-03, -4.1433e-03, -1.9838e-03,  ..., -1.3217e-03,\n",
      "          -9.7772e-03, -6.7679e-03],\n",
      "         [-6.0872e-03,  1.2252e-04,  5.9546e-03,  ..., -3.9916e-03,\n",
      "           4.3274e-03,  5.7797e-03]],\n",
      "\n",
      "        [[-2.4389e-03,  4.1102e-03, -9.8914e-03,  ...,  7.2575e-03,\n",
      "           4.0491e-03, -1.4260e-03],\n",
      "         [ 6.4597e-03,  2.3448e-04,  2.0431e-03,  ...,  6.8580e-03,\n",
      "           4.3280e-03,  6.0752e-03],\n",
      "         [-9.7783e-03,  1.5971e-03,  2.5336e-03,  ...,  8.1403e-03,\n",
      "          -1.9028e-04,  9.8448e-03],\n",
      "         [-4.6546e-03,  3.3241e-03, -6.8265e-03,  ...,  7.0848e-03,\n",
      "          -5.0040e-03,  5.4207e-03]],\n",
      "\n",
      "        [[ 1.3290e-03,  3.6549e-03, -1.9362e-03,  ..., -2.7790e-03,\n",
      "           1.1517e-03,  7.4006e-03],\n",
      "         [ 8.5626e-03, -5.1572e-03,  8.4236e-03,  ..., -2.8711e-03,\n",
      "          -5.9481e-03,  5.5613e-03],\n",
      "         [ 5.5066e-03, -1.4545e-03, -3.5263e-03,  ...,  2.1396e-03,\n",
      "           3.5488e-03,  6.9513e-03],\n",
      "         [-5.5170e-03, -9.8721e-03,  7.6039e-03,  ...,  8.1578e-03,\n",
      "           4.1667e-03, -4.7414e-03]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor(0., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
