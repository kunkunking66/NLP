{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=6)\n",
    "        self.rnn = nn.RNN(input_size=6, hidden_size=4, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [N,T] LongTensor token id列表\n",
    "        :param mask: [N,T] 实际值位置为1，填充值为0\n",
    "        :return: 置信度 [N,2]\n",
    "        \"\"\"\n",
    "        # 1. embedding的向量提取\n",
    "        z1 = self.embedding_layer(x)  # [N,T] -> [N,T,embedding_dim]\n",
    "        \n",
    "        # 2. rnn的特征提取\n",
    "        if mask is None:\n",
    "            output, _ = self.rnn(z1)  # [N,T,embedding_dim] --> [N,T,hidden_size]\n",
    "        else:\n",
    "            lengths = torch.sum(mask, dim=1).to(torch.long)\n",
    "            # pack_padded_sequence: 从z1中仅提取有效数据\n",
    "            z2: rnn.PackedSequence = pack_padded_sequence(z1, lengths, batch_first=True, enforce_sorted=False)\n",
    "            z2, _ = self.rnn(z2)\n",
    "            output, _ = pad_packed_sequence(z2, batch_first=True)\n",
    "        \n",
    "        # 3. 将T个时刻的向量合并成一个向量\n",
    "        \"\"\"\n",
    "        期望：用一个向量来表示整个文本的特征信息，并不需要针对每个时刻的token进行特征向量描述 --> 如何提取一个向量，并且这个向量可以体现当前这个文本。\n",
    "        方式一: 直接提取最后一个时刻的输出特征向量： output = output[:,-1,:]\n",
    "        方式二：直接将所有时刻的特征向量求均值: \n",
    "            output = torch.mean(output, dim=1) # [N,T,hidden_size] -> [N,hidden_size]\n",
    "            # 将output里面的值看出每个时刻的输出值:hi\n",
    "            output = alpha_1*h_1 + alpha_2*h_2 + alpha_3*h_3 +....+alpha_t*h_t\n",
    "            起始torch.mean操作中，alpha_i等于1/t, t==T\n",
    "        \"\"\"\n",
    "        output = torch.mean(output, dim=1)  # [N,T,hidden_size] -> [N,hidden_size]\n",
    "\n",
    "        # 4. 决策输出\n",
    "        scores = self.proj(output)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## not mast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0626, -0.0744],\n",
      "        [ 0.0156,  0.1935],\n",
      "        [-0.0526, -0.1035],\n",
      "        [-0.2965,  0.1919],\n",
      "        [-0.1860,  0.0129],\n",
      "        [-0.0921, -0.2721],\n",
      "        [-0.0633, -0.0723]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "_x = torch.randint(100, size=(7, 5))\n",
    "scores = net(_x)\n",
    "print(scores)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.argmax(scores, dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4025, -0.1584],\n",
      "        [-0.2626, -0.2636],\n",
      "        [-0.1955, -0.1135]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "_x = torch.tensor([\n",
    "    [2, 3, 5, 4, 6, 2],  # 样本1实际长度为6，填充后大小为6\n",
    "    [2, 3, 5, 65, 0, 0],  # 样本2实际长度为4，填充后大小为6\n",
    "    [1, 3, 5, 0, 0, 0]  # 样本3实际长度为3，填充后大小为6\n",
    "])\n",
    "_mask = torch.tensor([\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 0.0, 0.0],\n",
    "    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "])\n",
    "scores = net(_x, _mask)\n",
    "print(scores)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设有一批序列数据，已经按长度降序排列\n",
    "sequences = [torch.tensor([5,6,7]), torch.tensor([1,2,3,4]), torch.tensor([8,9])]\n",
    "# 使用pad_sequence进行填充\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "\n",
    "# 计算每个序列的实际长度\n",
    "lengths = torch.sum(padded_sequences != 0, dim=1)\n",
    "\n",
    "batch_size, seq_len = padded_sequences.size()\n",
    "# mask = torch.zeros(batch_size, seq_len, dtype=torch.float)\n",
    "# for i, length in enumerate(lengths):\n",
    "#     mask[i, :length] = 1\n",
    "\n",
    "\n",
    "# 矩阵的广播机制\n",
    "mask = (torch.arange(seq_len).unsqueeze(0) < lengths.unsqueeze(1)).float()\n",
    "\n",
    "scores = net(padded_sequences, mask)\n",
    "print(scores)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 0],\n",
       "        [8, 9, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN输入数据的时间T为什么可以不一样？\n",
    "\n",
    "RNN的参数结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pack_padded_sequence与pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([4, 3, 2]),\n",
       "indices=tensor([1, 0, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1, 5, 8, 2, 6, 9, 3, 7, 4]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([1, 0, 2]), unsorted_indices=tensor([1, 0, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 压缩填充序列\n",
    "packed_input = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "packed_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enforce_sorted 参数在 pack_padded_sequence 函数中用于指定输入序列是否已经按长度降序排列。\n",
    "\n",
    "- enforce_sorted=True：（默认值）这意味着输入序列必须按长度降序排列。如果设置为 True，函数会假设输入序列已经按照长度从长到短排序，这可以提高处理效率，因为内部实现可以利用这种排序来优化计算。如果你的数据没有预先排序，使用这个选项可能会导致错误的结果。\n",
    "\n",
    "- enforce_sorted=False：这意味着输入序列不必按长度降序排列。如果设置为 False，函数会自动处理未排序的序列，这通常涉及到在内部对序列进行排序和后续的反排序操作。这提供了便利，因为你不需要在调用 pack_padded_sequence 之前手动对序列进行排序，但可能会稍微降低性能，因为需要额外的排序步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 0],\n",
      "        [8, 9, 0, 0]])\n",
      "tensor([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 解压缩序列\n",
    "unpacked_output, output_lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "\n",
    "print(unpacked_output)\n",
    "print(output_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稠密矩阵与稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Matrix:\n",
      "tensor([[ 1.,  0.,  0.],\n",
      "        [ 0., 23.,  0.],\n",
      "        [ 0.,  0.,  5.],\n",
      "        [ 0.,  0., 56.]])\n",
      "\n",
      "Sparse Matrix:\n",
      "tensor(indices=tensor([[0, 1, 2, 3],\n",
      "                       [0, 1, 2, 2]]),\n",
      "       values=tensor([ 1., 23.,  5., 56.]),\n",
      "       size=(4, 3), nnz=4, layout=torch.sparse_coo)\n",
      "\n",
      "Reconstructed Dense Matrix:\n",
      "tensor([[ 1.,  0.,  0.],\n",
      "        [ 0., 23.,  0.],\n",
      "        [ 0.,  0.,  5.],\n",
      "        [ 0.,  0., 56.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个稠密矩阵\n",
    "dense_matrix = torch.tensor([[1, 0, 0], [0, 23, 0], [0, 0, 5], [0, 0, 56]], dtype=torch.float32)\n",
    "print(\"Dense Matrix:\")\n",
    "print(dense_matrix)\n",
    "\n",
    "# 转换为稀疏矩阵\n",
    "sparse_matrix = dense_matrix.to_sparse()\n",
    "print(\"\\nSparse Matrix:\")\n",
    "print(sparse_matrix)\n",
    "\n",
    "# 转换回稠密矩阵\n",
    "dense_matrix_reconstructed = sparse_matrix.to_dense()\n",
    "print(\"\\nReconstructed Dense Matrix:\")\n",
    "print(dense_matrix_reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
