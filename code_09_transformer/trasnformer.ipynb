{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../images/The Transformer - model architecture.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 50%; height: auto;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# 假设 image_file 是你的图像文件路径\n",
    "image_file = '../images/The Transformer - model architecture.png'\n",
    "\n",
    "# 显示图像并居中，同时设置图像的宽度和高度\n",
    "HTML('<img src=\"{}\" style=\"display: block; margin-left: auto; margin-right: auto; width: 50%; height: auto;\"/>'.format(image_file))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 多头注意力机制\n",
    "通过并行计算多个注意力头来捕捉输入序列中不同位置的信息\n",
    "1. 捕捉长距离依赖\n",
    "\n",
    "功能：多头注意力机制能够捕捉序列中任意两个位置之间的依赖关系，而不仅仅是局部的或相邻的位置。这使得模型能够更好地理解长距离的上下文信息。\n",
    "\n",
    "示例：在机器翻译任务中，一个单词可能依赖于句子中的另一个较远位置的单词。多头注意力机制可以同时考虑这些长距离的依赖关系，从而生成更准确的翻译。\n",
    "\n",
    "2. 并行计算\n",
    "\n",
    "功能：与循环神经网络（RNN）不同，多头注意力机制可以并行处理输入序列中的所有位置，这大大提高了计算效率，特别是在现代 GPU 和 TPU 上。\n",
    "\n",
    "示例：在处理长序列时，RNN 需要逐个处理每个位置，而多头注意力机制可以同时处理所有位置，显著减少了训练时间。\n",
    "\n",
    "3. 捕捉多种特征\n",
    "\n",
    "功能：通过多个不同的注意力头，模型可以同时学习不同子空间中的特征。每个头可以关注不同的信息，从而捕捉到更丰富的上下文信息。\n",
    "\n",
    "示例：一个头可能关注语法结构，另一个头可能关注语义信息，还有一个头可能关注上下文中的实体关系。这种多样性和互补性使得模型能够更全面地理解输入序列。\n",
    "\n",
    "4. 动态权重分配\n",
    "\n",
    "功能：多头注意力机制通过计算注意力权重，动态地分配每个位置的重要性。这使得模型能够根据上下文动态调整对不同位置的关注程度。\n",
    "\n",
    "示例：在处理一个句子时，模型可以动态地决定哪些单词对当前单词的生成更重要，从而生成更自然和准确的输出。\n",
    "\n",
    "5. 支持多种任务\n",
    "\n",
    "功能：多头注意力机制不仅适用于机器翻译任务，还可以广泛应用于其他自然语言处理任务，如文本生成、问答系统、文本分类等。\n",
    "\n",
    "示例：在问答系统中，多头注意力机制可以帮助模型更好地理解问题和上下文之间的关系，从而生成更准确的答案。\n",
    "\n",
    "6. 支持掩码操作\n",
    "\n",
    "功能：多头注意力机制支持掩码操作，可以屏蔽某些位置的注意力权重，防止模型看到未来的信息或不必要的信息。\n",
    "\n",
    "示例：在解码器中，掩码操作可以防止模型看到未来的位置，从而确保生成的序列是自回归的。在某些任务中，掩码操作还可以用于处理不完整的输入或特殊的数据结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qkv_attention_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Value Shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def qkv_attention_value(q, k, v, mask=False):\n",
    "    \"\"\"\n",
    "    计算attention value。\n",
    "    \n",
    "参数解释：英翻法\n",
    "1. 批次大小（N）\n",
    "定义：批次大小 N 表示每次处理的样本数量。\n",
    "示例：假设我们有 2 个样本，那么 N=2。\n",
    "2. 查询序列长度（T1）\n",
    "定义：查询序列长度 T1 表示解码器（Decoder）当前已生成的目标语言序列的长度。\n",
    "示例：假设解码器已经生成了 3 个法文单词，那么 T1=3。\n",
    "3. 键/值序列长度（T2）\n",
    "定义：键/值序列长度 T2 表示编码器（Encoder）输出的源语言序列的长度。\n",
    "示例：假设源语言（英文）序列有 4 个单词，那么 T2=4。\n",
    "4. 特征维度（E）\n",
    "定义：特征维度 E 表示每个 token 的嵌入维度。\n",
    "示例：假设每个 token 的嵌入维度为 512，那么 E=512。\n",
    "5. 头的数量（h）\n",
    "定义：头的数量 h 表示多头注意力机制中的头数。\n",
    "示例：假设我们使用 8 个头，那么 h=8。\n",
    "将 512 维的输入分成 8 份，每份 64 维，分别进行计算，从而实现并行学习\n",
    "输出矩阵中的每个向量包含了该 token 与其他 token 之间的相关性信息\n",
    "\n",
    "    :param q: 查询向量，形状为[N, T1, E]或[N, h, T1, E]，其中N是批次大小，T1是查询序列长度，E是特征维度，h是头数。\n",
    "    :param k: 键向量，形状为[N, T2, E]或[N, h, T2, E]，其中T2是键序列长度。\n",
    "    :param v: 值向量，形状与键向量相同。\n",
    "    :param mask: 可选的掩码，布尔值或张量，用于屏蔽某些位置的注意力权重。\n",
    "    :return: 经过注意力加权的值向量，形状为[N, T1, E]或[N, h, T1, E]。\n",
    "    \"\"\"\n",
    "    # 2. 计算q和k之间的相关性->F函数\n",
    "    k = torch.transpose(k, dim0=-2, dim1=-1)  # [??, T2, E] --> [??, E, T2]，交换最后两个维度以便于矩阵乘法\n",
    "    # matmul([??,T1,E], [??,E,T2])\n",
    "    scores = torch.matmul(q, k)  # [??,T1,T2]，计算查询和键之间的点积\n",
    "\n",
    "    if isinstance(mask, bool):\n",
    "        if mask:\n",
    "            # 如果mask为True，则创建一个上三角掩码矩阵\n",
    "            _shape = scores.shape\n",
    "            mask = torch.ones((_shape[-2], _shape[-1]))\n",
    "            mask = torch.triu(mask, diagonal=1) * -10000\n",
    "            mask = mask[None][None]  # 增加批次和头维度\n",
    "        else:\n",
    "            mask = None\n",
    "    # print (f\"mask: {mask}\")\n",
    "    if mask is not None:\n",
    "        # 将掩码应用到分数上，屏蔽的位置会被赋予一个非常大的负数\n",
    "        scores = scores + mask\n",
    "\n",
    "    # 3. 转换为权重\n",
    "    alpha = torch.softmax(scores, dim=-1)  # [??,T1,T2]，对分数应用softmax得到注意力权重\n",
    "\n",
    "    # 4. 值的合并\n",
    "    # matmul([??,T1,T2], [??,T2,E])\n",
    "    v = torch.matmul(alpha, v)  # [??,T1,E]，使用注意力权重加权值向量\n",
    "    return v\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义查询、键和值的维度\n",
    "    N, T1, T2, E = 2, 3, 4, 5  # 批次大小，查询序列长度，键序列长度，特征维度\n",
    "    # 创建随机数据\n",
    "    q = torch.rand(N, T1, E)\n",
    "    k = torch.rand(N, T2, E)\n",
    "    v = torch.rand(N, T2, E)\n",
    "    \n",
    "    # 调用函数\n",
    "    attention_value = qkv_attention_value(q, k, v)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"Attention Value Shape:\", attention_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[    -0., -10000., -10000., -10000.],\n",
      "        [    -0.,     -0., -10000., -10000.],\n",
      "        [    -0.,     -0.,     -0., -10000.]])\n",
      "tensor([[[[    -0., -10000., -10000., -10000.],\n",
      "          [    -0.,     -0., -10000., -10000.],\n",
      "          [    -0.,     -0.,     -0., -10000.]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask = torch.ones((T1, T2))\n",
    "print (mask)\n",
    "print (torch.triu(mask, diagonal=1))\n",
    "mask = torch.triu(mask, diagonal=1) * -10000\n",
    "print (mask)\n",
    "mask = mask[None][None]\n",
    "print (mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        \"\"\"\n",
    "        初始化多头自注意力模块。\n",
    "        \n",
    "        :param hidden_size: 隐藏层的大小，也就是特征向量的维度E。\n",
    "        :param num_header: 多头注意力机制中的头数。\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"hidden_size必须能被num_header整除，当前值分别为：{hidden_size}, {num_header}\"\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_header = num_header\n",
    "        \n",
    "        # 定义Q, K, V的线性变换层\n",
    "        self.wq = nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        self.wk = nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        self.wv = nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        \n",
    "        # 定义输出的线性变换层和激活函数\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def split(self, vs):\n",
    "        \"\"\"\n",
    "        将输入的特征向量分割成多个头。\n",
    "        \n",
    "        :param vs: 输入的特征向量，形状为[N, T, E]。\n",
    "        :return: 分割后的特征向量，形状为[N, H, T, E/H]。\n",
    "        \"\"\"\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = vs.permute(0, 2, 1, 3)  # 调整维度顺序以匹配多头注意力的输入要求\n",
    "        return vs\n",
    "\n",
    "    def forward(self, x, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param x: 输入特征向量，形状为[N, T, E]。\n",
    "        :param attention_mask: 可选的注意力掩码，形状为[N, T, T]。\n",
    "        :return: 输出特征向量，形状为[N, T, E]。\n",
    "        \"\"\"\n",
    "        # 1. 计算Q, K, V\n",
    "        q = self.wq(x)  # [N, T, E]\n",
    "        k = self.wk(x)  # [N, T, E]\n",
    "        v = self.wv(x)  # [N, T, E]\n",
    "        \n",
    "        # 2. 分割Q, K, V以适应多头注意力\n",
    "        q = self.split(q)  # [N, H, T, E/H]\n",
    "        k = self.split(k)  # [N, H, T, E/H]\n",
    "        v = self.split(v)  # [N, H, T, E/H]\n",
    "        \n",
    "        # 3. 计算多头自注意力的值\n",
    "        v = qkv_attention_value(q, k, v, attention_mask)\n",
    "        \n",
    "        # 4. 合并多头注意力的结果\n",
    "        v = v.permute(0, 2, 1, 3)  # [N, H, T, E/H] -> [N, T, H, E/H]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [N, T, H, E/H] -> [N, T, E]\n",
    "        v = self.wo(v)  # [N, T, E]\n",
    "        \n",
    "        return v\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义隐藏层大小和头数\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    \n",
    "    # 创建多头自注意力模块实例\n",
    "    attention = MultiHeadSelfAttention(hidden_size, num_header)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    N, T, E = 2, 10, hidden_size  # 批次大小，序列长度，特征维度\n",
    "    x = torch.rand(N, T, E)\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = attention(x)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadEncoderDecoderAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadEncoderDecoderAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头编码器-解码器注意力模块。\n",
    "    \n",
    "    该模块实现了Transformer模型中的编码器-解码器注意力机制，允许解码器动态地聚焦于编码器的输出。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        super(MultiHeadEncoderDecoderAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"hidden_size必须能被num_header整除，当前值分别为：{hidden_size}, {num_header}\"\n",
    "        \n",
    "        self.hidden_size = hidden_size  # 隐藏层大小，即特征向量的维度E\n",
    "        self.num_header = num_header  # 头的数目\n",
    "\n",
    "        # 输出层，将多个头的特征组合合并\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def split(self, vs):\n",
    "        \"\"\"\n",
    "        将输入的特征向量分割成多个头。\n",
    "        \n",
    "        :param vs: 输入的特征向量，形状为[N, T, E]。\n",
    "        :return: 分割后的特征向量，形状为[N, H, T, E/H]。\n",
    "        \"\"\"\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims=(0, 2, 1, 3))\n",
    "        return vs\n",
    "\n",
    "    def forward(self, q, encoder_k, encoder_v, encoder_attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param q: 查询向量，形状为[N, T1, E]。\n",
    "        :param encoder_k: 编码器的键向量，形状为[N, T2, E]。\n",
    "        :param encoder_v: 编码器的值向量，形状为[N, T2, E]。\n",
    "        :param encoder_attention_mask: 布尔值或张量，编码器的注意力掩码，形状为[N, 1, T1, T2]。\n",
    "        :return: 输出特征向量，形状为[N, T1, E]。\n",
    "        \"\"\"\n",
    "        q = self.split(q)  # [N, T1, E] --> [N, H, T1, E/H]\n",
    "        k = self.split(encoder_k)  # [N, T2, E] --> [N, H, T2, E/H]\n",
    "        v = self.split(encoder_v)  # [N, T2, E] --> [N, H, T2, E/H]\n",
    "\n",
    "        # 计算注意力值\n",
    "        v = qkv_attention_value(q, k, v, mask=encoder_attention_mask)\n",
    "\n",
    "        # 合并多头注意力的结果\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # [N, H, T1, E/H] --> [N, T1, H, E/H]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [N, T1, H, E/H] --> [N, T1, E]\n",
    "        v = self.wo(v)  # 应用输出层\n",
    "        return v\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义隐藏层大小和头数\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    \n",
    "    # 创建多头编码器-解码器注意力模块实例\n",
    "    attention = MultiHeadEncoderDecoderAttention(hidden_size, num_header)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    N, T1, T2 = 2, 10, 15  # 批次大小，解码器序列长度，编码器序列长度\n",
    "    q = torch.rand(N, T1, hidden_size)  # 查询向量\n",
    "    encoder_k = torch.rand(N, T2, hidden_size)  # 编码器键向量\n",
    "    encoder_v = torch.rand(N, T2, hidden_size)  # 编码器值向量\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = attention(q, encoder_k, encoder_v, encoder_attention_mask=False)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    位置前馈网络（Feed Forward Network），也称为全连接前馈网络。\n",
    "    \n",
    "    该网络在Transformer模型中用于对序列中的每个位置进行相同的操作，通常位于自注意力层之后。\n",
    "    它由两个线性变换和一个ReLU激活函数组成，第一个线性层将输入特征扩展四倍，第二个线性层将特征还原。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        初始化FFN模块。\n",
    "        \n",
    "        :param hidden_size: 输入和输出特征的维度。\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        # 定义前馈网络的结构\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),  # 第一个线性层，将特征维度扩展四倍\n",
    "            nn.ReLU(),  # ReLU激活函数\n",
    "            nn.Linear(4 * hidden_size, hidden_size)  # 第二个线性层，将特征维度还原\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param x: 输入特征，形状为[batch_size, seq_len, hidden_size]。\n",
    "        :return: 输出特征，形状为[batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        return self.ffn(x)\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义隐藏层大小\n",
    "    hidden_size = 512\n",
    "    \n",
    "    # 创建FFN模块实例\n",
    "    ffn = FFN(hidden_size)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    x = torch.rand(batch_size, seq_len, hidden_size)  # 输入特征\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = ffn(x)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualsNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualsNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    残差连接和层归一化模块。\n",
    "    \n",
    "    该模块结合了残差连接和层归一化（Layer Normalization），常用于Transformer模型中。\n",
    "    残差连接有助于缓解深层网络中的梯度消失问题，而层归一化则有助于稳定训练过程。\n",
    "    \"\"\"\n",
    "    def __init__(self, block, hidden_size):\n",
    "        \"\"\"\n",
    "        初始化ResidualsNorm模块。\n",
    "        \n",
    "        :param block: 一个子模块，其输出将与输入相加（残差连接）。\n",
    "        :param hidden_size: 归一化层的特征维度。\n",
    "        \"\"\"\n",
    "        super(ResidualsNorm, self).__init__()\n",
    "        self.block = block  # 子模块\n",
    "        self.norm = nn.LayerNorm(normalized_shape=hidden_size)  # 层归一化\n",
    "        self.relu = nn.ReLU()  # ReLU激活函数\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param x: 输入特征。\n",
    "        :return: 经过残差连接和层归一化后的特征。\n",
    "        \"\"\"\n",
    "        z = self.block(x, **kwargs)  # 通过子模块处理输入\n",
    "        z = self.relu(x + z)  # 应用残差连接和ReLU激活函数\n",
    "        z = self.norm(z)  # 应用层归一化\n",
    "        return z\n",
    "\n",
    "# 测试示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义隐藏层大小\n",
    "    hidden_size = 512\n",
    "    \n",
    "    # 创建一个示例子模块，例如一个简单的线性层\n",
    "    example_block = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    # 创建ResidualsNorm模块实例\n",
    "    residuals_norm = ResidualsNorm(example_block, hidden_size)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    x = torch.rand(batch_size, seq_len, hidden_size)  # 输入特征\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = residuals_norm(x)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoderLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder attention_mask.shape: torch.Size([2, 1, 10, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer编码器层组合模块。\n",
    "    \n",
    "    该模块将多个编码器层（包含多头自注意力和前馈网络）组合在一起，构成完整的Transformer编码器。\n",
    "    每个编码器层包括一个残差连接和层归一化的多头自注意力模块，以及一个残差连接和层归一化的前馈网络模块。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_header, encoder_layers):\n",
    "        \"\"\"\n",
    "        初始化Transformer编码器层组合模块。\n",
    "        \n",
    "        :param hidden_size: 隐藏层大小，即特征向量的维度。\n",
    "        :param num_header: 多头自注意力机制中的头数。\n",
    "        :param encoder_layers: 编码器层的数量。\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayers, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param x: 输入特征，形状为[batch_size, seq_len, hidden_size]。\n",
    "        :param attention_mask: 注意力掩码，形状为[batch_size, seq_len]。\n",
    "        :return: 经过所有编码器层处理后的特征。\n",
    "        \"\"\"\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)  # 增加header维度\n",
    "        print (f\"encoder attention_mask.shape: {attention_mask.shape}\")\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "# 示例代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义隐藏层大小、头数和编码器层数\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    encoder_layers = 6\n",
    "    \n",
    "    # 创建Transformer编码器层组合模块实例\n",
    "    encoder_layers_module = TransformerEncoderLayers(hidden_size, num_header, encoder_layers)\n",
    "    \n",
    "    # 创建随机输入数据和注意力掩码\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    x = torch.rand(batch_size, seq_len, hidden_size)  # 输入特征\n",
    "    attention_mask = torch.ones(batch_size, seq_len, seq_len)  # 注意力掩码，全1表示无掩码\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = encoder_layers_module(x, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderLayers(\n",
      "  (layers): ModuleList(\n",
      "    (0): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (6): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (7): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (8): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (9): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (10): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (11): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (encoder_layers_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder attention_mask.shape: torch.Size([2, 1, 10, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer编码器模块。\n",
    "    \n",
    "    该模块实现了Transformer模型中的编码器部分，包括嵌入层、位置编码和编码器层。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, encoder_layers):\n",
    "        \"\"\"\n",
    "        初始化Transformer编码器模块。\n",
    "        \n",
    "        :param vocab_size: 词汇表大小，用于定义嵌入层的词汇表大小。\n",
    "        :param hidden_size: 隐藏层大小，即特征向量的维度。\n",
    "        :param num_header: 多头自注意力机制中的头数。\n",
    "        :param max_seq_length: 序列的最大长度，用于定义位置嵌入的大小。\n",
    "        :param encoder_layers: 编码器层的数量。\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)  # 嵌入层\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)  # 位置嵌入层\n",
    "        self.layers = TransformerEncoderLayers(hidden_size, num_header, encoder_layers)  # 编码器层\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param input_token_ids: 输入的token id，形状为[N, T]，long类型。\n",
    "        :param input_position_ids: 输入的位置id，形状为[N, T]，long类型。\n",
    "        :param input_mask: 输入的mask矩阵，形状为[N, T, T]，float类型。\n",
    "        :return: 编码器的输出特征。\n",
    "        \"\"\"\n",
    "        # 1. 获取token的embedding\n",
    "        inp_embedding = self.input_emb(input_token_ids)  # [N, T, E]\n",
    "\n",
    "        # 2. 获取位置embedding\n",
    "        position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "        # 3. 合并embedding\n",
    "        emd = inp_embedding + position_embedding\n",
    "\n",
    "        # 4. 输入到编码器层提取特征\n",
    "        feat_emd = self.layers(emd, attention_mask=input_mask)\n",
    "\n",
    "        return feat_emd\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义参数\n",
    "    vocab_size = 10000\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    max_seq_length = 100\n",
    "    encoder_layers = 6\n",
    "    \n",
    "    # 创建Transformer编码器模块实例\n",
    "    encoder = TransformerEncoder(vocab_size, hidden_size, num_header, max_seq_length, encoder_layers)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    input_token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 输入的token id\n",
    "    input_position_ids = torch.arange(seq_len).repeat(batch_size, 1)  # 输入的位置id\n",
    "    input_mask = torch.ones(batch_size, seq_len, seq_len)  # 输入的mask矩阵\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = encoder(input_token_ids, input_position_ids, input_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerDecoderLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoderLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer解码器层组合模块。\n",
    "    \n",
    "    该模块将多个解码器层（包含自注意力、编码器-解码器注意力和前馈网络）组合在一起，构成完整的Transformer解码器层。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_header, decoder_layers):\n",
    "        \"\"\"\n",
    "        初始化Transformer解码器层组合模块。\n",
    "        \n",
    "        :param hidden_size: 隐藏层大小，即特征向量的维度。\n",
    "        :param num_header: 多头自注意力机制中的头数。\n",
    "        :param decoder_layers: 解码器层的数量。\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayers, self).__init__()\n",
    "\n",
    "        # 定义键和值的线性变换层\n",
    "        self.wk = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wv = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(decoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadEncoderDecoderAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, encoder_outputs=None, encoder_attention_mask=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param x: 输入特征，形状为[N, T2, E]。\n",
    "        :param encoder_outputs: 编码器的输出，形状为[N, T1, E]。\n",
    "        :param encoder_attention_mask: 编码器的注意力掩码，形状为[N, 1, T1]。\n",
    "        :param attention_mask: 解码器的自注意力掩码，形状为[N, T2, T2]。\n",
    "        :return: 解码器层的输出特征。\n",
    "        \"\"\"\n",
    "        # 增加维度以匹配多头注意力的输入要求\n",
    "        if not isinstance(attention_mask, bool):\n",
    "            attention_mask = torch.unsqueeze(attention_mask, dim=1)  # [N, T2, T2] -> [N, 1, T2, T2]\n",
    "            print (f\"decoder attention_mask.shape: {attention_mask.shape}\")\n",
    "        if not isinstance(encoder_attention_mask, bool):\n",
    "            encoder_attention_mask = torch.unsqueeze(encoder_attention_mask, dim=1)  # [N, 1, T1] -> [N, 1, 1, T1]\n",
    "            print (f\"decoder encoder_attention_mask.shape: {encoder_attention_mask.shape}\")\n",
    "        \n",
    "        # 通过线性层计算编码器输出的键和值\n",
    "        k = self.wk(encoder_outputs)  # [N, T1, E]\n",
    "        v = self.wv(encoder_outputs)  # [N, T1, E]\n",
    "        \n",
    "        # 遍历每个解码器层\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                encoder_k=k, encoder_v=v, encoder_attention_mask=encoder_attention_mask,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        return x\n",
    "\n",
    "# 示例代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义参数\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    decoder_layers = 6\n",
    "    vocab_size = 10000\n",
    "    max_seq_length = 100\n",
    "    \n",
    "    # 创建Transformer解码器层组合模块实例\n",
    "    decoder_layers_module = TransformerDecoderLayers(hidden_size, num_header, decoder_layers)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    x = torch.rand(batch_size, seq_len, hidden_size)  # 输入特征\n",
    "    encoder_outputs = torch.rand(batch_size, seq_len, hidden_size)  # 编码器的输出\n",
    "    # encoder_attention_mask = torch.ones(batch_size, 1, seq_len)  # 编码器的注意力掩码\n",
    "    # attention_mask = torch.ones(batch_size, seq_len, seq_len)  # 解码器的自注意力掩码\n",
    "    encoder_attention_mask = True\n",
    "    attention_mask = True\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = decoder_layers_module(x, encoder_outputs, encoder_attention_mask, attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayers(\n",
      "  (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (6): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (7): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (8): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (9): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (10): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (11): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (12): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (13): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (14): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (15): ResidualsNorm(\n",
      "      (block): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (16): ResidualsNorm(\n",
      "      (block): MultiHeadEncoderDecoderAttention(\n",
      "        (wo): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (17): ResidualsNorm(\n",
      "      (block): FFN(\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (decoder_layers_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder attention_mask.shape: torch.Size([2, 1, 10, 10])\n",
      "decoder encoder_attention_mask.shape: torch.Size([2, 1, 10, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer解码器模块。\n",
    "    \n",
    "    该模块实现了Transformer模型中的解码器部分，包括嵌入层、位置编码和解码器层。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, decoder_layers):\n",
    "        \"\"\"\n",
    "        初始化Transformer解码器模块。\n",
    "        \n",
    "        :param vocab_size: 词汇表大小，用于定义嵌入层的词汇表大小。\n",
    "        :param hidden_size: 隐藏层大小，即特征向量的维度。\n",
    "        :param num_header: 多头自注意力机制中的头数。\n",
    "        :param max_seq_length: 序列的最大长度，用于定义位置嵌入的大小。\n",
    "        :param decoder_layers: 解码器层的数量。\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)  # 嵌入层\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)  # 位置嵌入层\n",
    "        self.layers = TransformerDecoderLayers(hidden_size, num_header, decoder_layers)  # 解码器层\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask, encoder_outputs, encoder_attention_mask):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param input_token_ids: 输入的token id，形状为[N, T]，long类型。\n",
    "        :param input_position_ids: 输入的位置id，形状为[N, T]，long类型。\n",
    "        :param input_mask: 输入的mask矩阵，形状为[N, T, T]，float类型。\n",
    "        :param encoder_outputs: 编码器的输出状态信息，形状为[N, T1, E]。\n",
    "        :param encoder_attention_mask: 编码器的输入mask信息，形状为[N, T1, T1]。\n",
    "        :return: 解码器的输出特征。\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 1. 获取token的embedding\n",
    "            inp_embedding = self.input_emb(input_token_ids)  # [N, T, E]\n",
    "\n",
    "            # 2. 获取位置embedding\n",
    "            position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "            # 3. 合并embedding\n",
    "            emd = inp_embedding + position_embedding\n",
    "\n",
    "            # 4. 输入到解码器层提取特征\n",
    "            feat_emd = self.layers(\n",
    "                emd, encoder_outputs=encoder_outputs,\n",
    "                encoder_attention_mask=encoder_attention_mask, attention_mask=input_mask\n",
    "            )\n",
    "\n",
    "            return feat_emd\n",
    "        else:\n",
    "            raise ValueError(\"当前模拟代码不实现推理过程，仅实现training过程\")\n",
    "\n",
    "# 示例代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义参数\n",
    "    vocab_size = 10000\n",
    "    hidden_size = 512\n",
    "    num_header = 8\n",
    "    max_seq_length = 100\n",
    "    decoder_layers = 6\n",
    "    \n",
    "    # 创建Transformer解码器模块实例\n",
    "    decoder = TransformerDecoder(vocab_size, hidden_size, num_header, max_seq_length, decoder_layers)\n",
    "    \n",
    "    # 创建随机输入数据\n",
    "    batch_size, seq_len = 2, 10  # 批次大小，序列长度\n",
    "    input_token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 输入的token id\n",
    "    input_position_ids = torch.arange(seq_len).repeat(batch_size, 1)  # 输入的位置id\n",
    "    encoder_outputs = torch.rand(batch_size, seq_len, hidden_size)  # 编码器的输出状态信息\n",
    "    input_mask = torch.ones(batch_size, seq_len, seq_len)  # 输入的mask矩阵\n",
    "    encoder_attention_mask = torch.ones(batch_size, seq_len, seq_len)  # 编码器的输入mask信息\n",
    "    \n",
    "    # 调用前向传播\n",
    "    output = decoder(input_token_ids, input_position_ids, input_mask, encoder_outputs, encoder_attention_mask)\n",
    "    \n",
    "    # 打印输出形状\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoder & TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder attention_mask.shape: torch.Size([2, 1, 5, 5])\n",
      "torch.Size([2, 5, 512])\n",
      "decoder attention_mask.shape: torch.Size([2, 1, 6, 6])\n",
      "decoder encoder_attention_mask.shape: torch.Size([2, 1, 1, 5])\n",
      "torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, encoder_layers=6)\n",
    "decoder = TransformerDecoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, decoder_layers=6)\n",
    "\n",
    "input_token_ids = torch.tensor([\n",
    "    [100, 102, 108, 253, 125],  # 第一个样本实际长度为5\n",
    "    [254, 125, 106, 0, 0]  # 第二个样本实际长度为3\n",
    "])\n",
    "input_position_ids = torch.tensor([\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [0, 1, 2, 3, 4]\n",
    "])\n",
    "input_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    ],\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, 0.0],\n",
    "    ],\n",
    "])\n",
    "encoder_attention_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0]  # 表示第一个样本的解码器中第一个时刻和编码器的各个时刻之间的mask值\n",
    "    ],\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0]  # 是因为编码器的输入中，最后两个位置是填充\n",
    "    ],\n",
    "])\n",
    "\n",
    "input_decoder_token_ids = torch.tensor([\n",
    "    [251, 235, 124, 321, 25, 68],\n",
    "    [351, 235, 126, 253, 0, 0]\n",
    "])\n",
    "input_decoder_position_ids = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5],\n",
    "    [0, 1, 2, 3, 4, 5]\n",
    "])\n",
    "input_decoder_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    ],\n",
    "    [\n",
    "        [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0]\n",
    "    ],\n",
    "])\n",
    "\n",
    "encoder_outputs = encoder(input_token_ids, input_position_ids, input_mask)\n",
    "print(encoder_outputs.shape)\n",
    "\n",
    "decoder_outputs = decoder(\n",
    "    input_token_ids=input_decoder_token_ids,\n",
    "    input_position_ids=input_decoder_position_ids,\n",
    "    input_mask=input_decoder_mask,\n",
    "    encoder_outputs=encoder_outputs,\n",
    "    encoder_attention_mask=encoder_attention_mask\n",
    ")\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设以下类已经定义好：\n",
    "# TransformerEncoder, TransformerDecoder, TransformerEncoderLayers, TransformerDecoderLayers\n",
    "# MultiHeadSelfAttention, MultiHeadEncoderDecoderAttention, FFN, ResidualsNorm\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的Transformer模型，包含编码器和解码器。\n",
    "    \n",
    "    该模型实现了序列到序列的转换任务，例如机器翻译。\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, hidden_size, num_heads, max_seq_length, encoder_layers, decoder_layers):\n",
    "        \"\"\"\n",
    "        初始化Transformer模型。\n",
    "        \n",
    "        :param encoder_vocab_size: 编码器词汇表大小。\n",
    "        :param decoder_vocab_size: 解码器词汇表大小。\n",
    "        :param hidden_size: 隐藏层大小。\n",
    "        :param num_heads: 多头自注意力机制中的头数。\n",
    "        :param max_seq_length: 序列的最大长度。\n",
    "        :param encoder_layers: 编码器层的数量。\n",
    "        :param decoder_layers: 解码器层的数量。\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 初始化编码器\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size=encoder_vocab_size, hidden_size=hidden_size,\n",
    "            num_header=num_heads, max_seq_length=max_seq_length,\n",
    "            encoder_layers=encoder_layers\n",
    "        )\n",
    "        \n",
    "        # 初始化解码器\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=decoder_vocab_size, hidden_size=hidden_size,\n",
    "            num_header=num_heads, max_seq_length=max_seq_length,\n",
    "            decoder_layers=decoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_input_ids, encoder_input_position_ids, encoder_input_mask,\n",
    "                decoder_input_ids, decoder_input_position_ids, decoder_input_mask, encoder_attention_mask):\n",
    "        \"\"\"\n",
    "        前向传播过程。\n",
    "        \n",
    "        :param encoder_input_ids: 编码器输入的token id，形状为[N, T]。\n",
    "        :param encoder_input_position_ids: 编码器输入的位置id，形状为[N, T]。\n",
    "        :param encoder_input_mask: 编码器输入的mask矩阵，形状为[N, T, T]。\n",
    "        :param decoder_input_ids: 解码器输入的token id，形状为[N, T]。\n",
    "        :param decoder_input_position_ids: 解码器输入的位置id，形状为[N, T]。\n",
    "        :param decoder_input_mask: 解码器输入的mask矩阵，形状为[N, T, T]。\n",
    "        :param encoder_attention_mask: 编码器注意力掩码，形状为[N, 1, T]。\n",
    "        :return: 解码器的输出分数。\n",
    "        \"\"\"\n",
    "        # 编码器前向传播\n",
    "        encoder_outputs = self.encoder(encoder_input_ids, encoder_input_position_ids, encoder_input_mask)\n",
    "        \n",
    "        # 解码器前向传播\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_token_ids=decoder_input_ids,\n",
    "            input_position_ids=decoder_input_position_ids,\n",
    "            input_mask=decoder_input_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            encoder_attention_mask=encoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder attention_mask.shape: torch.Size([2, 1, 5, 5])\n",
      "decoder attention_mask.shape: torch.Size([2, 1, 6, 6])\n",
      "decoder encoder_attention_mask.shape: torch.Size([2, 1, 1, 5])\n",
      "torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 实例化Transformer模型\n",
    "transformer = Transformer(\n",
    "    encoder_vocab_size=1000, decoder_vocab_size=1000, hidden_size=512, num_heads=8,\n",
    "    max_seq_length=1024, encoder_layers=6, decoder_layers=6\n",
    ")\n",
    "\n",
    "# 创建输入数据\n",
    "input_token_ids = torch.tensor([\n",
    "    [100, 102, 108, 253, 125],  # 第一个样本实际长度为5\n",
    "    [254, 125, 106, 0, 0]  # 第二个样本实际长度为3\n",
    "])\n",
    "input_position_ids = torch.tensor([\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [0, 1, 2, 3, 4]\n",
    "])\n",
    "input_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    ],\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, 0.0],\n",
    "    ],\n",
    "])\n",
    "encoder_attention_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0]  # 表示第一个样本的解码器中第一个时刻和编码器的各个时刻之间的mask值\n",
    "    ],\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0]  # 是因为编码器的输入中，最后两个位置是填充\n",
    "    ],\n",
    "])\n",
    "\n",
    "input_decoder_token_ids = torch.tensor([\n",
    "    [251, 235, 124, 321, 25, 68],\n",
    "    [351, 235, 126, 253, 0, 0]\n",
    "])\n",
    "input_decoder_position_ids = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5],\n",
    "    [0, 1, 2, 3, 4, 5]\n",
    "])\n",
    "input_decoder_mask = torch.tensor([\n",
    "    [\n",
    "        [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    ],\n",
    "    [\n",
    "        [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "        [-10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0]\n",
    "    ],\n",
    "])\n",
    "\n",
    "# 执行前向传播\n",
    "decoder_outputs = transformer(\n",
    "    encoder_input_ids=input_token_ids, \n",
    "    encoder_input_position_ids=input_position_ids, \n",
    "    encoder_input_mask=input_mask,\n",
    "    decoder_input_ids=input_decoder_token_ids,\n",
    "    decoder_input_position_ids=input_decoder_position_ids,\n",
    "    decoder_input_mask=input_decoder_mask, \n",
    "    encoder_attention_mask=encoder_attention_mask\n",
    ")\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ait/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型的参数\n",
    "d_model = 512  # 嵌入的维度\n",
    "nhead = 8  # 多头注意力机制中的头数\n",
    "num_encoder_layers = 6  # 编码器层数\n",
    "num_decoder_layers = 6  # 解码器层数\n",
    "dim_feedforward = 2048  # 前馈网络的维度\n",
    "dropout = 0.1  # Dropout 比率\n",
    "\n",
    "# 创建 Transformer 模型\n",
    "transformer_model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# 创建一些随机数据来模拟输入\n",
    "src = torch.rand(10, 32, d_model)  # 假设的源序列长度为10，批次大小为32\n",
    "tgt = torch.rand(20, 32, d_model)  # 假设的目标序列长度为20，批次大小为32\n",
    "\n",
    "# 创建位置编码\n",
    "src_key_padding_mask = torch.zeros(32, 10)  # 源序列的键填充掩码\n",
    "tgt_key_padding_mask = torch.zeros(32, 20)  # 目标序列的键填充掩码\n",
    "memory_key_padding_mask = torch.zeros(32, 10)  # 记忆序列的键填充掩码\n",
    "\n",
    "# 前向传播\n",
    "output = transformer_model(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "print(output.shape)  # 输出的形状将取决于目标序列和批次大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 补充\n",
    "- https://github.com/gordicaleksa/pytorch-original-transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
