{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "====================\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_punctuation(char):\n",
    "    pattern = r'^\\W$'   # 检查一个字符是否是非字母数字字符\n",
    "    return bool(re.match(pattern, char))\n",
    "\n",
    "# 测试字母字符\n",
    "print(is_punctuation('A'))  # 应该返回 False，因为 'A' 是字母\n",
    "print(is_punctuation('z'))  # 应该返回 False，因为 'z' 是字母\n",
    "\n",
    "# 测试数字字符\n",
    "print(is_punctuation('1'))  # 应该返回 False，因为 '1' 是数字\n",
    "\n",
    "# 测试标点符号\n",
    "print(is_punctuation(','))  # 应该返回 True，因为逗号是标点符号\n",
    "print(is_punctuation('.'))  # 应该返回 True，因为句号是标点符号\n",
    "print(is_punctuation('!'))  # 应该返回 True，因为感叹号是标点符号\n",
    "\n",
    "print (\"=\"*20)\n",
    "# 测试特殊字符\n",
    "print(is_punctuation('@'))  # 应该返回 True，因为 '@' 不是字母数字字符\n",
    "print(is_punctuation('&'))  # 应该返回 True，因为 '&' 不是字母数字字符\n",
    "\n",
    "# 测试空字符串\n",
    "print(is_punctuation(''))   # 应该返回 False，因为空字符串不匹配任何模式\n",
    "\n",
    "# 测试空格\n",
    "print(is_punctuation(' ')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparse\n",
    "\n",
    "argparse 是 Python 标准库的一部分，用于编写用户友好的命令行接口。argparse 模块能够处理命令行参数和选项，使得程序能够读取用户输入的命令行参数，并根据这些参数执行相应的操作。\n",
    "\n",
    "主要特点:\n",
    "- 自动生成帮助文档：argparse 能够自动为命令行参数生成帮助文档。\n",
    "- 类型检查：可以指定每个命令行参数应该是什么类型，argparse 会自动进行类型检查。\n",
    "- 自定义错误消息：当用户提供的参数不正确时，argparse 可以显示自定义的错误消息。\n",
    "- 子命令：支持创建子命令，类似于 git 或 docker 这样的复杂命令行工具。\n",
    "- 可选参数和位置参数：可以定义必须提供的参数和可选参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pathlib 是 Python 的一个标准库模块，它提供面向对象的文件系统路径操作。Path 是 pathlib 模块中的一个类，它封装了文件系统路径，并提供了许多方法来处理这些路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/01/logs_tmp\n",
      "output/01/models_tmp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pathlib.PosixPath"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output_dir=Path(\"./output/01\")\n",
    "output_dir = output_dir if isinstance(output_dir, Path) else Path(output_dir)\n",
    "summary_dir = output_dir / \"logs_tmp\"\n",
    "print (summary_dir)\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir = output_dir / 'models_tmp'\n",
    "print (models_dir)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "type(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件存在\n",
      "123\n",
      "345\n",
      "/Users/hayden/Desktop/Hayden/99_Learning/02_DL/lecture06/text_classify/example.txt\n",
      "new_example.txt\n",
      "这是一个文件\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 创建一个Path对象\n",
    "p = Path('example.txt')\n",
    "\n",
    "# 检查文件是否存在\n",
    "if p.exists():\n",
    "    print(\"文件存在\")\n",
    "\n",
    "# 打开文件\n",
    "with p.open('r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "\n",
    "# 获取文件的绝对路径\n",
    "abs_path = p.absolute()\n",
    "print(abs_path)\n",
    "\n",
    "# 构造一个新的路径\n",
    "new_p = p.parent / 'new_example.txt'\n",
    "print(new_p)\n",
    "\n",
    "# 检查是否是文件\n",
    "if new_p.is_file():\n",
    "    print(\"这是一个文件\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # 保存分类集合到JSON文件\n",
    "    with open(str(output_dir / 'cats.json'), 'w', encoding='utf-8') as writer:\n",
    "        json.dump(list(cats), writer, ensure_ascii=False)\n",
    "\n",
    "    # 保存标签集合到JSON文件\n",
    "    with open(str(output_dir / 'labels.json'), 'w', encoding='utf-8') as writer:\n",
    "        json.dump(list(labels), writer, ensure_ascii=False)\n",
    "\n",
    "    # 保存词汇集合到JSON文件\n",
    "    with open(str(output_dir / 'tokens.json'), 'w', encoding='utf-8') as writer:\n",
    "        json.dump(tokens, writer, ensure_ascii=False, sort_keys=True, indent=2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这个代码段是将一个名为 cats 的集合保存到一个 JSON 文件中。这个操作涉及几个步骤，包括文件路径的构建、文件的打开、数据的序列化以及数据的写入。以下是代码的详细解释：\n",
    "\n",
    "output_dir / 'cats.json'：这里使用了 pathlib 模块中的 Path 对象的 / 运算符来构建一个新的路径。这个路径指向 output_dir 下的 cats.json 文件。\n",
    "\n",
    "str(output_dir / 'cats.json')：因为 open 函数需要一个字符串路径，所以这里将 Path 对象转换为字符串。\n",
    "\n",
    "with open(...) as writer:：这是一个上下文管理器，用于打开文件并确保文件在操作完成后正确关闭。变量 writer 是一个文件对象，用于写入数据。\n",
    "\n",
    "'w'：这是文件打开模式，表示写入模式。如果文件已存在，它会被覆盖；如果不存在，会创建一个新文件。\n",
    "\n",
    "encoding='utf-8'：这指定了文件的编码格式为 UTF-8，这对于处理包含非ASCII字符的数据非常重要。\n",
    "\n",
    "json.dump(list(cats), writer, ensure_ascii=False)：这是将 cats 集合转换为列表（因为 json.dump 需要一个可序列化的 Python 对象，而集合不是直接可序列化的），然后使用 json.dump 函数将这个列表序列化为 JSON 格式并写入文件。ensure_ascii=False 参数确保非ASCII字符被正确处理，而不是被转义成 ASCII 字符。.json 文件是一种基于文本的轻量级数据交换格式，它使用 JavaScript 对象表示法（JSON）来存储和传输数据。JSON 文件的作用包括：\n",
    "\n",
    "### .json文件\n",
    "\n",
    "将数据提取成 `.json` 文件然后再读取，这种做法在数据处理和软件工程中有几个好处：\n",
    "\n",
    "1. **数据持久化**：\n",
    "   - `.json` 文件可以将数据持久化存储在磁盘上，这样即使在程序关闭后，数据也不会丢失，可以在任何时候重新加载。\n",
    "\n",
    "2. **数据共享和传输**：\n",
    "   - `.json` 文件格式通用且易于阅读，便于在不同的系统、应用程序或团队成员之间共享和传输数据。\n",
    "\n",
    "3. **简化数据处理**：\n",
    "   - 将数据提取到 `.json` 文件中，可以在不同的程序或不同的处理阶段之间提供一个清晰的数据接口，简化数据处理流程。\n",
    "\n",
    "4. **版本控制**：\n",
    "   - `.json` 文件可以被版本控制系统（如Git）跟踪，这有助于管理数据的变化历史和协作开发。\n",
    "\n",
    "5. **数据备份和恢复**：\n",
    "   - `.json` 文件可以作为数据备份的手段，便于数据恢复和灾难恢复。\n",
    "\n",
    "6. **减少内存使用**：\n",
    "   - 对于大型数据集，将数据保存到文件中可以减少内存的使用，因为不需要一次性将所有数据加载到内存中。\n",
    "\n",
    "7. **提高性能**：\n",
    "   - 对于复杂的数据处理任务，将数据写入文件然后分批次读取可以提高处理性能，尤其是在处理大数据时。\n",
    "\n",
    "8. **数据标准化**：\n",
    "   - 使用 `.json` 文件可以确保数据的格式一致性，有助于标准化数据处理流程。\n",
    "\n",
    "9. **跨平台兼容性**：\n",
    "   - `.json` 文件在不同的操作系统和平台上具有很好的兼容性，不受特定环境的限制。\n",
    "\n",
    "10. **易于调试和测试**：\n",
    "    - 将数据保存在 `.json` 文件中，可以方便地进行数据的调试和测试，因为可以直接查看和修改文件内容。\n",
    "\n",
    "11. **解耦数据生成和消费**：\n",
    "    - 将数据保存为 `.json` 文件可以实现数据生成和数据消费的解耦，不同的组件或服务可以独立地生成和读取数据，提高系统的模块化。\n",
    "\n",
    "12. **缓存机制**：\n",
    "    - 在某些情况下，将数据缓存为 `.json` 文件可以避免重复的数据处理或网络请求，提高效率。\n",
    "\n",
    "总之，将数据提取成 `.json` 文件然后再读取，是一种常见的实践，它有助于提高数据处理的灵活性、可维护性和效率。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### .pkl文件\n",
    ".pkl` 文件是 Python 中用于序列化和反序列化数据的文件格式，全称为 \"pickle\" 文件。以下是 `.pkl` 文件的一些主要作用和特点：\n",
    "\n",
    "1. **保存模型**：\n",
    "   - 在机器学习和深度学习中，`.pkl` 文件常用于保存训练好的模型，以便在需要时加载和使用。\n",
    "\n",
    "2. **缓存数据**：\n",
    "   - 当需要频繁地读取和处理数据时，可以将数据序列化为 `.pkl` 文件，以提高读取和处理的速度。\n",
    "\n",
    "3. **数据传输**：\n",
    "   - `.pkl` 文件可以用于将数据传输到不同的系统和环境中，确保数据的完整性和一致性。\n",
    "\n",
    "4. **数据持久化**：\n",
    "   - `.pkl` 文件可以用于数据的持久化，即将程序中的数据保存到 `.pkl` 文件中，以防止数据丢失。在程序终止运行或重启后，可以从 `.pkl` 文件中加载数据，恢复到终止或重启前的状态。\n",
    "\n",
    "5. **跨平台和跨语言**：\n",
    "   - `.pkl` 文件可以在不同平台和不同编程语言之间互相转换和共享，但主要限于 Python 环境。\n",
    "\n",
    "6. **二进制格式**：\n",
    "   - `.pkl` 文件以二进制格式存储数据，因此在存储和加载过程中，不会丢失数据的精度和类型信息。\n",
    "\n",
    "7. **可压缩性**：\n",
    "   - `.pkl` 文件可以使用压缩算法进行压缩，以减小文件的大小。\n",
    "\n",
    "8. **序列化对象**：\n",
    "   - `.pkl` 文件可以保存任何 Python 对象，包括基本数据类型、自定义类、列表、字典等。\n",
    "\n",
    "9. **安全性问题**：\n",
    "   - 由于 `pickle` 模块可以执行任意代码，加载未知来源的 `.pkl` 文件存在安全性风险，因此应确保文件来源可信。\n",
    "\n",
    "10. **版本兼容性问题**：\n",
    "    - `pickle` 模块的版本兼容性问题可能导致 `.pkl` 文件在不同的 Python 解释器版本中加载失败，因此在使用时应确保版本一致性。\n",
    "\n",
    "在实际应用中，`.pkl` 文件因其方便的数据存储和加载特性而被广泛使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset 是一个抽象类，用于定义数据集的结构和行为。当你创建自己的数据集时，你需要继承 Dataset 类并实现以下三个方法：\n",
    "\n",
    "- `__init__`：初始化函数，用于设置数据集的属性，例如数据文件的路径、转换操作等。\n",
    "- `__len__`：返回数据集中样本的数量。\n",
    "- `__getitem__`：根据索引获取单个样本，并进行必要的处理，如数据预处理和数据增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomDataset\n",
    "它继承自 PyTorch 的 Dataset 类。这个类可以用来封装任何类型的数据和对应的标签，\n",
    "使其可以被 PyTorch 的数据加载器（DataLoader）使用，从而在训练神经网络时进行批处理、打乱数据顺序等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集的长度: 3\n",
      "样本数据: data1\n",
      "样本标签: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取数据和标签\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        # 这里可以添加数据预处理和增强的代码\n",
    "        return data, label\n",
    "\n",
    "# 假设 data 和 labels 是两个列表，分别包含数据和对应的标签\n",
    "data = [\"data1\", \"data2\", \"data3\"]  # 示例数据\n",
    "labels = [0, 1, 2]  # 示例标签\n",
    "\n",
    "# 创建 CustomDataset 实例\n",
    "custom_dataset = CustomDataset(data, labels)\n",
    "\n",
    "# 获取数据集的长度\n",
    "dataset_length = len(custom_dataset)\n",
    "print(\"数据集的长度:\", dataset_length)\n",
    "\n",
    "# 获取单个样本\n",
    "sample_data, sample_label = custom_dataset[0]  # 获取索引为 0 的样本\n",
    "print(\"样本数据:\", sample_data)\n",
    "print(\"样本标签:\", sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassifyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集的长度: 50218\n",
      "样本数据 ids: [96, 21, 3, 25, 894, 3, 238, 231, 3, 2006, 697, 2858]\n",
      "样本数据 文本: ['裤子', '不错', '<PUN>', '没有', '色差', '<PUN>', '大小', '合适', '<PUN>', '祝', '店家', '生意兴隆']\n",
      "样本数据 文本: 裤子不错<PUN>没有色差<PUN>大小合适<PUN>祝店家生意兴隆\n",
      "样本标签: 1\n",
      "样本 len_x: 12\n",
      "样本类别: 7\n",
      "==================================================\n",
      "x: tensor([[  60,    8,  136,    3,  342,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  96,   21,    3,   25,  894,    3,  238,  231,    3, 2006,  697, 2858]])\n",
      "y: tensor([0, 1])\n",
      "mask: tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "cats: (7, 7)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ClassifyDataset(Dataset):\n",
    "    def __init__(self, file_path, pad_token_idx=0):\n",
    "        super(ClassifyDataset, self).__init__()\n",
    "        self.PAD_IDX = pad_token_idx\n",
    "        with open(file_path, 'rb') as reader:\n",
    "            self.datas = pickle.load(reader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cat, y, x = self.datas[index]\n",
    "        return copy.deepcopy(x), y, len(x), cat\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        数据聚合\n",
    "        :param batch: list列表，列表中的每个元素都是调用了dataset的__getitem__方法得到的\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, y, lengths, cats = list(zip(*batch))\n",
    "        max_length = max(lengths)\n",
    "        mask = np.zeros(shape=(len(x), max_length), dtype=np.float32)\n",
    "        for i in range(len(x)):\n",
    "            x[i].extend([self.PAD_IDX] * (max_length - lengths[i]))  # 数据填充\n",
    "            mask[i][:lengths[i]] = 1\n",
    "        x = torch.tensor(x, dtype=torch.long)  # [N,T]\n",
    "        y = torch.tensor(y, dtype=torch.long)  # [N,]\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)  # [N,]\n",
    "        mask = torch.from_numpy(mask)\n",
    "        return x, y, mask, cats\n",
    "\n",
    "# 创建 CustomDataset 实例\n",
    "custom_dataset = ClassifyDataset(\"./datas/train.pkl\")\n",
    "\n",
    "# 获取数据集的长度\n",
    "dataset_length = len(custom_dataset)\n",
    "print(\"数据集的长度:\", dataset_length)\n",
    "\n",
    "# 获取单个样本\n",
    "sample_data, sample_label, len_x, cat = custom_dataset[1]  # 获取索引为 0 的样本\n",
    "print(\"样本数据 ids:\", sample_data)\n",
    "tokens = json.load(open(r'./datas/tokens.json', \"r\", encoding=\"utf-8\"))\n",
    "x_text = [tokens[token_id] for token_id in sample_data if token_id > 0]\n",
    "print(\"样本数据 文本:\", x_text)\n",
    "x_text = ''.join([tokens[token_id] for token_id in sample_data if token_id > 0])\n",
    "print(\"样本数据 文本:\", x_text)\n",
    "print(\"样本标签:\", sample_label)\n",
    "print(\"样本 len_x:\", len_x)\n",
    "print(\"样本类别:\", cat)\n",
    "\n",
    "print (\"=\"*50)\n",
    "x, y, mask, cats = custom_dataset.collate_fn([custom_dataset[0], custom_dataset[1]])\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"mask:\", mask)\n",
    "print(\"cats:\", cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('x1', 'x2', 'x3'), ('y1', 'y2', 'y3'), ('l1', 'l2', 'l3'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\n",
    "    ('x1', 'y1', 'l1'),\n",
    "    ('x2', 'y2', 'l2'),\n",
    "    ('x3', 'y3', 'l3')\n",
    "]\n",
    "\n",
    "x, y, lengths = zip(*batch)\n",
    "x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个操作是将一个包含多个元组的迭代器（在这里是 batch）解包，将每个元组的元素分别收集到不同的迭代器中。\n",
    "- 这是一个“解压缩”操作，将一个包含元组的序列拆分成多个序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('x1', 'y1', 'l1'), ('x2', 'y2', 'l2'), ('x3', 'y3', 'l3')]\n"
     ]
    }
   ],
   "source": [
    "batch_item = zip(x, y, lengths)\n",
    "batch_new = []\n",
    "for item in batch_item:\n",
    "    batch_new.append(item)\n",
    "    \n",
    "print (batch_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(['a', 'b', 'c'],[1,2,3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个操作是将多个可迭代对象（在这里是 x, y, lengths）打包成一个迭代器，其中每个元素是一个元组，包含来自每个可迭代对象的相应元素。\n",
    "- 这是一个“压缩”操作，将多个序列合并成一个序列，其中每个元素是元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "DataLoader 是 PyTorch 中的一个类，它封装了 Dataset 对象，并提供了一种便捷的方式来迭代数据集。DataLoader 不仅可以自动将数据批量化，还能进行数据打乱、多线程加载等操作，从而提高数据加载的效率和灵活性。\n",
    "\n",
    "以下是 DataLoader 的一些主要功能和特点：\n",
    "\n",
    "- 批量化（Batching）：DataLoader 可以将数据集中的样本自动分批，每个批次包含指定数量的样本（batch_size）。这意味着你可以在每次迭代中处理一批数据，而不是单个样本。\n",
    "\n",
    "- 打乱数据（Shuffling）：通过设置 shuffle=True，DataLoader 可以在每个epoch开始时随机打乱数据集的顺序。这有助于模型训练时的泛化能力，因为它确保了模型不会对数据集中的特定顺序产生依赖。\n",
    "\n",
    "- 多线程加载（Multithreading）：DataLoader 支持多线程数据加载，通过设置 num_workers 参数来指定同时运行的子进程数量。这样可以在加载数据时利用多核CPU的优势，提高数据加载速度。\n",
    "\n",
    "- 预处理和数据增强：DataLoader 可以与自定义的 Dataset 类一起使用，允许你在 Dataset 的 `__getitem__ `方法中进行数据预处理和数据增强操作。\n",
    "\n",
    "- 动态填充（Dynamic Padding）：对于不等长的序列数据，DataLoader 可以配合自定义的 `collate_fn` 函数来动态地对数据进行填充，使得每个批次中的数据长度一致，便于模型处理。\n",
    "  \n",
    "- prefetch_factor 是 PyTorch DataLoader 类的一个参数，它用于控制数据加载的预取（prefetching）行为。这个参数只在多进程数据加载时有效（即当 num_workers 大于0时）。prefetch_factor 的作用是提高数据加载的效率，特别是在处理 I/O 密集型任务时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  60,    8,  136,    3,  342,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  96,   21,    3,   25,  894,    3,  238,  231,    3, 2006,  697, 2858,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 257,    3,  455,  105,    3,  258,   14,   21,    3,    4,    6,   32,\n",
      "          414,    6,  144,   14,   34,    3,   37,  505,  256,    3,  183,  344,\n",
      "          505,    3,   64,    1,  180,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 200,    4,  241,  260,   13,   19,  109, 3346,  842,    3,  428,   21,\n",
      "            3, 2055, 7115,  164,  428,   21,    3,   25, 8046,    3,   37,   45,\n",
      "           84, 2202,    3,  959,  415,  228,   42, 9427,    6,    3,  164,   12,\n",
      "          428,   21,    3,  312,  468, 1147,  842,    3,  181,  475, 2858,    3]])\n",
      "tensor([0, 1, 1, 1])\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "['小了一点<PUN>不行', '裤子不错<PUN>没有色差<PUN>大小合适<PUN>祝店家生意兴隆', '干净<PUN>交通方便<PUN>服务态度也不错<PUN><NUM>的房间带的早餐也可以<PUN>就是老宾馆<PUN>设施较老<PUN>价格<UNK>高', '从<NUM>年开始我就一直再用沙宣<PUN>真心不错<PUN>去屑止痒效果真心不错<PUN>没有吹嘘<PUN>就是这个有点小贵<PUN>有时搞活动还是挺贵的<PUN>效果是真心不错<PUN>还会继续关注沙宣<PUN>希望商家生意兴隆<PUN>']\n",
      "(7, 7, 9, 5)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_dataloader(file_path, batch_size, shuffle=False, num_workers=0, prefetch_factor=2):\n",
    "    # 1. 构建DataSet对象\n",
    "    dataset = ClassifyDataset(file_path)\n",
    "    # 2. 构建dataloader对象\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        prefetch_factor=prefetch_factor if num_workers > 0 else None,\n",
    "        collate_fn=dataset.collate_fn  # 批次数据聚合函数\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "data = create_dataloader(\"./datas/train.pkl\", 4)\n",
    "for x_batch, y_batch, lengths_batch, cat in data:\n",
    "        print(x_batch)\n",
    "        print(y_batch)\n",
    "        print(lengths_batch)\n",
    "        x_text = [''.join([tokens[token_id] for token_id in token_ids if token_id > 0]) for token_ids in\n",
    "                  x_batch.detach().numpy()]\n",
    "        print(x_text)\n",
    "        print(cat)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\<PUN\\>：这个标识符通常用来表示句子或短语之间的停顿，相当于中文中的句号、逗号、分号等标点符号。在文本分析中，它可以被用来识别句子的边界，从而帮助进行句子分割。\n",
    "- \\<NUM\\>：这个标识符用来标记文本中的数字。在处理文本时，识别数字是非常重要的，因为它们可能包含重要的信息，如价格、年份、数量等。在这段文本中，\\<NUM\\> 可能用来标记年份、价格或其他数值信息。\n",
    "- \\<UNK\\>：这个标识符通常用来表示未知的词或者无法识别的字符。在文本处理中，如果遇到无法识别或者不在词汇表中的词，可能会用 \\<UNK\\> 来代替。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tqdm\n",
    "\n",
    "bar.set_postfix() 是 tqdm 进度条对象的一个方法，它用于在进度条后面添加一些自定义的后缀信息。这些信息通常用于显示当前循环的额外统计数据，比如损失值、准确率、epoch 数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.46it/s, epoch=1, train_loss=0.798]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.38it/s, epoch=2, train_loss=0.588]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.47it/s, epoch=3, train_loss=0.731]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "train_dataloader = range(10)\n",
    "# 假设我们有一个模拟的训练循环，总共有10个批次\n",
    "for epoch in range(1, 4):  # 模拟3个epoch\n",
    "    bar = tqdm(train_dataloader)\n",
    "    for i in bar:\n",
    "        bar.set_postfix(epoch=epoch, train_loss=np.random.random())  # 设置后缀信息\n",
    "        time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.34it/s, epoch=1, train_loss=0.253]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.46it/s, epoch=2, train_loss=0.126]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.46it/s, epoch=3, train_loss=0.655]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "logdir = '/Users/hayden/Desktop/Hayden/99_Learning/02_DL/lecture06/text_classify/output/01/logs_tmp'\n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "# writer.add_graph(net, torch.randint(vocab_size, size=(4, 8)))\n",
    "\n",
    "train_dataloader = range(10)\n",
    "# 假设我们有一个模拟的训练循环，总共有10个批次\n",
    "for epoch in range(1, 4):  # 模拟3个epoch\n",
    "    bar = tqdm(train_dataloader)\n",
    "    for i in bar:\n",
    "        bar.set_postfix(epoch=epoch, train_loss=np.random.random())  # 设置后缀信息\n",
    "        time.sleep(0.1)\n",
    "        writer.add_scalar('train_loss', np.random.random())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir /Users/hayden/Desktop/Hayden/99_Learning/02_DL/lecture06/text_classify/output/01/logs_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script结构的模型\n",
    "\n",
    "Torch Script 是 PyTorch 提供的一种中间表示（Intermediate Representation, IR），用于将 PyTorch 模型转换为一种可序列化和可优化的格式。Torch Script 使得模型可以在没有 Python 解释器的环境中运行，从而提高了模型的可移植性和性能。\n",
    "\n",
    "Torch Script 的主要特点\n",
    "- 可序列化：Torch Script 模型可以被序列化为文件（通常是 .pt 或 .pth 文件），可以方便地保存和加载。这使得模型的存储和分发变得简单。\n",
    "- 跨平台支持：Torch Script 模型可以在不同的环境中运行，包括 C++ 环境和移动设备（如 Android 和 iOS）。这使得模型可以在生产环境中高效地部署，而无需依赖 Python。\n",
    "- 性能优化：Torch Script 允许对模型进行优化，例如通过图优化和内存管理来提高推理性能。它可以通过静态分析来优化计算图，从而减少运行时开销。\n",
    "- 支持静态类型：Torch Script 允许使用静态类型检查，这有助于捕获潜在的错误，并提高代码的可读性和可维护性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX（Open Neural Network Exchange）\n",
    "\n",
    "ONNX（Open Neural Network Exchange）模型文件是一种开放的格式，用于表示深度学习模型。它由社区驱动，旨在促进不同深度学习框架之间的互操作性。以下是 ONNX 模型文件的一些主要特点：\n",
    "\n",
    "1. **跨框架兼容性**：\n",
    "   - ONNX 模型文件可以被不同的深度学习框架使用，如 PyTorch、TensorFlow、Keras、Caffe2 等。这意味着你可以在一个框架中训练模型，然后在另一个框架中进行部署和推理。\n",
    "\n",
    "2. **跨平台支持**：\n",
    "   - ONNX 模型可以在多种硬件和操作系统上运行，包括 Windows、Linux、macOS、Android 和 iOS。\n",
    "\n",
    "3. **优化和加速**：\n",
    "   - ONNX 提供了优化工具，可以对模型进行图优化，减少运行时的开销，提高推理速度。\n",
    "\n",
    "4. **模型简化**：\n",
    "   - ONNX 支持模型简化，可以移除不必要的节点和操作，减少模型大小，同时保持模型的准确性。\n",
    "\n",
    "5. **动态轴**：\n",
    "   - ONNX 支持动态轴，允许模型在不同的输入尺寸下运行，增加了模型的灵活性。\n",
    "\n",
    "6. **版本控制**：\n",
    "   - ONNX 定义了不同的操作集版本（opset versions），以支持新操作和改进现有操作。这使得模型可以针对不同的操作集版本进行优化。\n",
    "\n",
    "7. **易于部署**：\n",
    "   - ONNX 模型可以被直接部署到支持 ONNX 的运行时和推理引擎中，如 ONNX Runtime、TensorRT、OpenVINO 等。\n",
    "\n",
    "8. **模型验证**：\n",
    "   - ONNX 提供了工具来验证模型的结构和语义，确保模型的正确性和一致性。\n",
    "\n",
    "9. **模型转换**：\n",
    "   - 许多深度学习框架提供了将模型转换为 ONNX 格式的工具，使得模型转换变得简单。\n",
    "\n",
    "10. **社区支持**：\n",
    "    - 由于 ONNX 是一个社区驱动的项目，它得到了业界的广泛支持，不断有新的功能和改进被加入。\n",
    "\n",
    "ONNX 模型文件的这些特点使其成为深度学习模型部署和推理的一个重要工具，特别是在需要跨框架和跨平台支持的场景中。通过使用 ONNX，开发者可以更容易地将模型集成到不同的应用和服务中，同时保持高性能和灵活性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script VS ONNX \n",
    "\n",
    "Torch Script 和 ONNX 是两种不同的模型格式，它们都用于序列化和优化深度学习模型，但有一些关键的区别。以下是 Torch Script 和 ONNX 格式模型文件的对比：\n",
    "\n",
    "### 1. 格式和规范\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 是 PyTorch 特有的格式，用于将 PyTorch 模型转换为一种中间表示（IR）。\n",
    "  - 可以包含 PyTorch 操作和一些 Python 动态特性（如条件语句和循环）。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 是一个开放的格式，由微软和 Facebook 等公司共同发起，支持多种深度学习框架。\n",
    "  - 定义了一套标准的神经网络操作，旨在实现跨框架的模型互操作性。\n",
    "\n",
    "### 2. 兼容性\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 主要用于 PyTorch 内部，但也可以被其他支持 Torch Script 的框架使用。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 支持多种框架，包括 PyTorch、TensorFlow、Keras、Caffe2 等，以及它们的模型互转换。\n",
    "\n",
    "### 3. 部署和推理\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 可以被直接加载到 PyTorch 中，也可以在没有 Python 环境的 C++ 环境中运行。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 可以被加载到多种支持 ONNX 的推理引擎中，如 ONNX Runtime、TensorRT、OpenVINO 等。\n",
    "\n",
    "### 4. 动态特性\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 通过 tracing 或 scripting 可以保留模型中的控制流和动态计算图。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 通过定义动态轴（dynamic axes）来支持模型的动态输入尺寸。\n",
    "\n",
    "### 5. 性能优化\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 可以利用 PyTorch 的优化工具，如自动微分、图优化等。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 可以利用 ONNX 的优化工具，如 ONNX Simplifier，以及推理引擎的优化。\n",
    "\n",
    "### 6. 模型转换\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 可以通过 tracing 或 scripting 将 PyTorch 模型转换为 Torch Script。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 可以通过多种工具将不同框架的模型转换为 ONNX，包括 PyTorch、TensorFlow 等。\n",
    "\n",
    "### 7. 社区和生态系统\n",
    "\n",
    "- **Torch Script**：\n",
    "  - 主要依赖于 PyTorch 社区和生态系统。\n",
    "\n",
    "- **ONNX**：\n",
    "  - 由一个广泛的社区支持，包括多个框架和硬件供应商。\n",
    "\n",
    "### 总结\n",
    "\n",
    "Torch Script 和 ONNX 都是强大的工具，用于模型的序列化和优化，但它们在设计目标和使用场景上有所不同。Torch Script 更适合 PyTorch 内部和 C++ 部署，而 ONNX 提供了跨框架和跨平台的互操作性。选择哪种格式取决于具体的应用需求、目标平台和性能要求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
