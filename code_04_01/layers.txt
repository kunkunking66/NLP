神经网络常见网络层：
# 二维卷积层 Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
# 激活函数 ReLU(inplace=True)
# 归一化层 BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
# 最大池化层 MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
# 自适应最大池化层 AdaptiveMaxPool2d(output_size=(4, 4))
# 自适应平均池化层 AdaptiveAvgPool2d(output_size=(1, 1))
# 丢弃层Dropout(p=0.7, inplace=False)
# 全连接层 Linear(in_features=2048, out_features=1024, bias=True)
这些是构建卷积神经网络时常用的层类型。下面是每种层的简要说明和它们的参数：

1. **Conv2d**:
   - 作用：二维卷积层，用于在图像数据上应用卷积。
   - 参数：
     - `in_channels=528`：输入数据的通道数。
     - `out_channels=160`：卷积后输出的通道数。
     - `kernel_size=(1, 1)`：卷积核的大小，这里是一个1x1的卷积核，通常用于在通道上应用逐点卷积。
     - `stride=(1, 1)`：卷积的步长，这里为1，表示卷积核每次移动一个像素。
     - `bias=False`：是否在卷积层中添加偏置项。

2. **ReLU**:
   - 作用：激活函数，使用修正线性单元（Rectified Linear Unit）来引入非线性。
   - 参数：
     - `inplace=True`：在原地（in-place）进行计算，这意味着它将覆盖输入数据，节省内存。

3. **BatchNorm2d**:
   - 作用：批量归一化层，用于规范化上一层的输出，以提高训练速度和性能。
   - 参数：
     - `num_features=160`：归一化层的通道数。
     - `eps=0.001`：添加到方差中的小常数，以防止除以零。
     - `momentum=0.1`：用于计算运行均值和方差的动量。
     - `affine=True`：是否使用可学习的仿射变换参数。
     - `track_running_stats=True`：是否跟踪运行均值和方差。

4. **MaxPool2d**:
   - 作用：最大池化层，用于降低特征图的空间维度。
   - 参数：
     - `kernel_size=3`：池化窗口的大小。
     - `stride=1`：池化的步长。
     - `padding=1`：池化层的填充。
     - `dilation=1`：池化层的膨胀率。
     - `ceil_mode=True`：是否使用天花板函数来计算输出大小。

5. **AdaptiveMaxPool2d**:
   - 作用：自适应最大池化层，可以自动调整池化窗口的大小以匹配所需的输出大小。
   - 参数：
     - `output_size=(4, 4)`：输出特征图的大小。

6. **AdaptiveAvgPool2d**:
   - 作用：自适应平均池化层，类似于自适应最大池化，但是使用平均值而不是最大值。
   - 参数：
     - `output_size=(1, 1)`：输出特征图的大小。

7. **Dropout**:
   - 作用：丢弃层，用于在训练过程中随机丢弃一部分神经元的输出，以减少过拟合。
   - 参数：
     - `p=0.7`：丢弃率，即每个神经元被丢弃的概率。

8. **Linear**:
   - 作用：全连接层，用于将特征图展平后连接到指定数量的输出节点。
   - 参数：
     - `in_features=2048`：输入特征的数量。
     - `out_features=1024`：输出特征的数量。
     - `bias=True`：是否在全连接层中添加偏置项。

这些层可以组合起来构建复杂的神经网络模型，用于执行各种任务，如图像分类、目标检测、语义分割等。
