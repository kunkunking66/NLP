{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:25:29.187594Z",
     "start_time": "2024-11-20T10:25:25.744049Z"
    }
   },
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:36:15.941465Z",
     "start_time": "2024-11-20T10:36:15.936125Z"
    }
   },
   "source": [
    "class CBOW(nn.Module):  # 用于上下文求中心词\n",
    "    def __init__(self, vocab_size, embedding_dim=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        # 用于将词汇表中的每个词映射到一个固定维度的向量\n",
    "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        # 用于将嵌入向量转换为词汇表大小的输出，这意味着线性层的输出将是一个向量，其长度与词汇表的大小相同，每个元素代表当前上下文预测词汇表中每个词的概率\n",
    "        # 将维度再改到与vocabulary 并用sigmoid投射到(0, 1) 表示概率\n",
    "        self.output_layer = nn.Linear(in_features=embedding_dim, out_features=vocab_size)  # 参数量：embedding_dim*vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [N,T] long\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z1 = self.emb_layer(x)  # [N,T] --> [N,T,embedding_dim]\n",
    "        z2 = torch.mean(z1, dim=1)  # [N,T,embedding_dim] --> [N,embedding_dim]\n",
    "        scores = self.output_layer(z2)  # [N,embedding_dim] --> [N,vocab_size]  得到的是每个样本对应各个单词类别的置信度（概率）\n",
    "        return scores"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scores"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:36:18.500751Z",
     "start_time": "2024-11-20T10:36:18.441699Z"
    }
   },
   "source": [
    "vocab_size = 50000  # 词汇表大小，也就是单词类别数目\n",
    "batch_size=16  # 定义了批处理的大小，即每次输入到模型的数据样本数\n",
    "window_size = 4     # 多个老师教一个学生\n",
    "\n",
    "net = CBOW(vocab_size=vocab_size, embedding_dim=128)\n",
    "# 形状为[batch_size, window_size]，即[16, 4]。这个张量包含了从0到vocab_size-1（即0到49999）之间的随机整数，代表词汇表中的单词索引\n",
    "x = torch.randint(vocab_size, size=(batch_size, window_size), dtype=torch.long)  # [N,T]\n",
    "# 形状为[batch_size]，即[16]。这个张量包含了从0到vocab_size-1之间的随机整数，代表每个样本的中心词索引\n",
    "y = torch.randint(vocab_size, size=(batch_size,), dtype=torch.long)  # [N,]\n",
    "\n",
    "scores = net(x)  # [N,vocab_size]\n",
    "print(scores.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50000])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:36:20.341106Z",
     "start_time": "2024-11-20T10:36:20.334817Z"
    }
   },
   "source": [
    "scores[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0255,  0.1011,  0.3229,  ..., -0.1217,  0.1916,  0.1309],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid & BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:42:50.246266Z",
     "start_time": "2024-11-20T10:42:50.236148Z"
    }
   },
   "source": [
    "# 损失：希望样本预测属于实际类别的置信度/概率要越大越好，如果可以的话，要求预测不属于实际类别的置信度越小越好\n",
    "# 二分类\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "y_onehot = F.one_hot(y, vocab_size).to(torch.float32)  # [N, vocab_size]\n",
    "loss = loss_fn(scores, y_onehot)\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7041, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:42:54.330523Z",
     "start_time": "2024-11-20T10:42:54.288280Z"
    }
   },
   "source": [
    "prob = torch.sigmoid(scores)  # [N, vocab_size]\n",
    "loss2 = -torch.mean(y_onehot * torch.log(prob + 1e-8) + (1 - y_onehot) * torch.log(1.0 - prob + 1e-8))\n",
    "print(loss2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7041, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:44:36.645570Z",
     "start_time": "2024-11-20T10:44:36.631482Z"
    }
   },
   "source": [
    "res = y_onehot * torch.log(prob + 1e-8) + (1 - y_onehot) * torch.log(1.0 - prob + 1e-8)\n",
    "res.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 50000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax & CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:44:44.777974Z",
     "start_time": "2024-11-20T10:44:44.768333Z"
    }
   },
   "source": [
    "# 多分类\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(scores, y)\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7697, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:44:47.064734Z",
     "start_time": "2024-11-20T10:44:47.052378Z"
    }
   },
   "source": [
    "prob = torch.softmax(scores, dim=1)  # [N, vocab_size]\n",
    "y_onehot = F.one_hot(y, vocab_size)  # [N, vocab_size]\n",
    "loss2 = -torch.mean(torch.sum(y_onehot * torch.log(prob), dim=1))\n",
    "print(loss2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7697, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW backward"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:44:57.019082Z",
     "start_time": "2024-11-20T10:44:57.016422Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:02.784818Z",
     "start_time": "2024-11-20T10:46:02.781331Z"
    }
   },
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        # 当前embedding layer和全连接中使用的是同一个w\n",
    "        weight = nn.Parameter(torch.randn((vocab_size, embedding_dim), dtype=torch.float32))\n",
    "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.output_layer = nn.Linear(in_features=embedding_dim, out_features=vocab_size, bias=False)\n",
    "        self.emb_layer.weight = weight\n",
    "        self.output_layer.weight = weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [batch_size,window_size] long\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z1 = self.emb_layer(x)  # [batch_size,window_size] --> [batch_size,window_size,embedding_dim]\n",
    "        z2 = torch.mean(z1, dim=1)  # [batch_size,window_size,embedding_dim] --> [batch_size,embedding_dim]\n",
    "        scores = self.output_layer(z2)  # [batch_size,embedding_dim] --> [batch_size,vocab_size]  得到的是每个样本对应各个单词类别的置信度\n",
    "        return scores"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:11.386392Z",
     "start_time": "2024-11-20T10:46:06.469647Z"
    }
   },
   "source": [
    "vocab_size = 500  # 词汇表大小，也就是单词类别数目\n",
    "batch_size = 1\n",
    "window_size = 4\n",
    "\n",
    "net = CBOW(vocab_size=vocab_size, embedding_dim=128)\n",
    "opt = optim.SGD(net.parameters(), lr=0.1)\n",
    "for para in net.parameters():\n",
    "    print(para.shape)\n",
    "    print (para)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 0.5794,  0.5382,  0.0938,  ...,  0.3112, -1.4108,  0.3746],\n",
      "        [-1.5388, -1.2390,  0.6408,  ..., -1.7857,  0.4686,  0.2899],\n",
      "        [ 1.1819,  1.3431,  0.3455,  ..., -0.2958, -0.0559,  1.4537],\n",
      "        ...,\n",
      "        [ 0.3409, -1.5993,  1.0788,  ...,  1.0522, -0.6956,  0.2514],\n",
      "        [ 0.5822,  0.3264, -0.0511,  ...,  0.1181, -1.2590,  0.4036],\n",
      "        [ 1.0954,  0.6918, -0.3589,  ...,  1.1529,  0.1128, -0.4262]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:22.003333Z",
     "start_time": "2024-11-20T10:46:21.999691Z"
    }
   },
   "source": [
    "for para in net.parameters():\n",
    "    print(para.shape)\n",
    "    print (para)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 0.5794,  0.5382,  0.0938,  ...,  0.3112, -1.4108,  0.3746],\n",
      "        [-1.5388, -1.2390,  0.6408,  ..., -1.7857,  0.4686,  0.2899],\n",
      "        [ 1.1819,  1.3431,  0.3455,  ..., -0.2958, -0.0559,  1.4537],\n",
      "        ...,\n",
      "        [ 0.3409, -1.5993,  1.0788,  ...,  1.0522, -0.6956,  0.2514],\n",
      "        [ 0.5822,  0.3264, -0.0511,  ...,  0.1181, -1.2590,  0.4036],\n",
      "        [ 1.0954,  0.6918, -0.3589,  ...,  1.1529,  0.1128, -0.4262]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:29.570490Z",
     "start_time": "2024-11-20T10:46:29.566305Z"
    }
   },
   "source": [
    "para[0][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5794, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:40.091858Z",
     "start_time": "2024-11-20T10:46:40.085706Z"
    }
   },
   "source": [
    "x = torch.tensor([[3, 5, 8, 1]], dtype=torch.long)\n",
    "y = torch.tensor([3], dtype=torch.long)\n",
    "\n",
    "scores = net(x)  # [n,vocab_size]\n",
    "print(scores.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:50.282905Z",
     "start_time": "2024-11-20T10:46:50.276268Z"
    }
   },
   "source": [
    "# 损失：希望样本预测属于实际类别的置信度要越大越好，如果可以的话，要求预测不属于实际类别的置信度越小越好\n",
    "y_onehot = F.one_hot(y, vocab_size).to(torch.float32)  # [n, vocab_size]\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(scores, y_onehot)\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6414, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:54.006494Z",
     "start_time": "2024-11-20T10:46:54.001336Z"
    }
   },
   "source": [
    "prob = torch.sigmoid(scores)  # [n, vocab_size]\n",
    "# loss2 = -torch.mean(torch.sum(y_onehot * torch.log(prob + 1e-8), dim=1))  # 只更新当前样本对应类别的参数w\n",
    "loss2 = -torch.mean(y_onehot * torch.log(prob + 1e-8) + (1 - y_onehot) * torch.log(1.0 - prob + 1e-8))\n",
    "print(loss2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5586, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:46:58.355509Z",
     "start_time": "2024-11-20T10:46:58.336875Z"
    }
   },
   "source": [
    "opt.zero_grad()\n",
    "loss2.backward()\n",
    "opt.step()"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:01.801902Z",
     "start_time": "2024-11-20T10:47:01.797457Z"
    }
   },
   "source": [
    "print(\"debug查看梯度值\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug查看梯度值\n",
      "emb_layer.weight tensor([[-1.5216e-06, -1.6656e-07, -1.9131e-07,  ...,  4.1428e-08,\n",
      "          6.4631e-07,  2.4276e-07],\n",
      "        [-1.1703e-02,  8.0924e-03, -2.6683e-02,  ...,  2.1885e-03,\n",
      "          2.6752e-03,  3.7523e-03],\n",
      "        [-2.9923e-07, -3.2756e-08, -3.7622e-08,  ...,  8.1473e-09,\n",
      "          1.2710e-07,  4.7741e-08],\n",
      "        ...,\n",
      "        [-5.2767e-04, -5.7762e-05, -6.6343e-05,  ...,  1.4367e-05,\n",
      "          2.2413e-04,  8.4185e-05],\n",
      "        [-2.4033e-07, -2.6309e-08, -3.0217e-08,  ...,  6.5436e-09,\n",
      "          1.0208e-07,  3.8343e-08],\n",
      "        [-2.8944e-06, -3.1685e-07, -3.6392e-07,  ...,  7.8808e-08,\n",
      "          1.2294e-06,  4.6179e-07]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:05.478523Z",
     "start_time": "2024-11-20T10:47:05.474256Z"
    }
   },
   "source": [
    "param.grad.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:07.632521Z",
     "start_time": "2024-11-20T10:47:07.629758Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:23.265570Z",
     "start_time": "2024-11-20T10:47:23.260550Z"
    }
   },
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.output_layer = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [N,1] long\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z1 = self.emb_layer(x)  # [N,1] --> [N,1,embedding_dim]\n",
    "        z2 = z1[:, 0, :]  # [N,1,embedding_dim] --> [N,embedding_dim]\n",
    "        scores = self.output_layer(z2)  # [N,embedding_dim] --> [N,vocab_size]  得到的是每个样本对应各个单词类别的置信度\n",
    "        return scores"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:37.154336Z",
     "start_time": "2024-11-20T10:47:37.102997Z"
    }
   },
   "source": [
    "vocab_size = 50000  # 词汇表大小，也就是单词类别数目\n",
    "batch_size = 16\n",
    "window_size = 4  # 一个老师教多个学生\n",
    "\n",
    "net = SkipGram(vocab_size=vocab_size, embedding_dim=128)\n",
    "\n",
    "x = torch.randint(vocab_size, size=(batch_size, 1), dtype=torch.long)  # [N,1]\n",
    "y = torch.randint(vocab_size, size=(batch_size, window_size), dtype=torch.long)  # [N,T]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:51.457278Z",
     "start_time": "2024-11-20T10:47:51.451880Z"
    }
   },
   "source": [
    "print (y[0])\n",
    "for i in range(window_size):\n",
    "    y[0, i] = 2 + i\n",
    "    y[1, i] = 0 + i\n",
    "print (f\"y shape is {y.shape}\")\n",
    "print (y[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43908, 20204, 45632, 14861])\n",
      "y shape is torch.Size([16, 4])\n",
      "tensor([2, 3, 4, 5])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:54.064210Z",
     "start_time": "2024-11-20T10:47:54.058189Z"
    }
   },
   "source": [
    "scores = net(x)  # [N,vocab_size] N个样本每个样本在vocab_size个类别上的置信度\n",
    "print(scores.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50000])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCEWithLogitsLoss\n",
    "\n",
    "y降维"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:47:56.373620Z",
     "start_time": "2024-11-20T10:47:56.345685Z"
    }
   },
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "y_onehot = F.one_hot(y, vocab_size).to(torch.float32)  # [N, T, vocab_size]\n",
    "y_onehot_2 = torch.sum(y_onehot, dim=1)  # [N, T, vocab_size] -> [N, vocab_size]\n",
    "loss = loss_fn(scores, y_onehot_2)   # [N, vocab_size]\n",
    "print(loss) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7330, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:48:00.565894Z",
     "start_time": "2024-11-20T10:48:00.551931Z"
    }
   },
   "source": [
    "ori_prob = torch.sigmoid(scores)  # [N, vocab_size]\n",
    "loss3 = y_onehot_2 * torch.log(ori_prob) + (1 - y_onehot_2) * torch.log(1.0 - ori_prob)\n",
    "print(loss3[0].detach().numpy()[:20])\n",
    "loss3 = -torch.mean(loss3)\n",
    "print(loss3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.67824835 -0.91499376 -0.5686203  -0.79391515 -0.5762589  -0.5153297\n",
      " -0.7610896  -0.9129652  -0.4830457  -0.46847996 -0.68798566 -0.50904596\n",
      " -0.6055947  -0.72263414 -0.81640095 -1.5237985  -0.5609002  -0.48854607\n",
      " -0.7609143  -0.5639324 ]\n",
      "tensor(0.7330, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unsqueeze & tile\n",
    "\n",
    "x升维"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:48:04.122917Z",
     "start_time": "2024-11-20T10:48:04.095107Z"
    }
   },
   "source": [
    "prob = torch.unsqueeze(ori_prob, dim=1)  # [N, vocab_size] -> [N, 1, vocab_size]\n",
    "prob = torch.tile(prob, [1, y.shape[1], 1])  # [N, 1, vocab_size] -> [N, T, vocab_size]\n",
    "loss2 = y_onehot * torch.log(prob) + (1 - y_onehot) * torch.log(1.0 - prob)  # [N,T,vocab_size]\n",
    "loss2 = torch.mean(loss2, dim=1)  # [N,T,vocab_size] -> [N, vocab_size]\n",
    "print(loss2[0].detach().numpy()[:20])\n",
    "loss2 = -torch.mean(loss2)\n",
    "print(loss2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.67824835 -0.91499376 -0.7687175  -0.6496866  -0.7632118  -0.8110118\n",
      " -0.7610896  -0.9129652  -0.4830457  -0.46847996 -0.68798566 -0.50904596\n",
      " -0.6055947  -0.72263414 -0.81640095 -1.5237985  -0.5609002  -0.48854607\n",
      " -0.7609143  -0.5639324 ]\n",
      "tensor(0.7330, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
