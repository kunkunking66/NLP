{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a72a8f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size=None, num_layers=1, bidirectional=False):\n",
    "        super(EncoderModule, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        hidden_size = hidden_size or embedding_dim\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True, bidirectional=bidirectional\n",
    "        )\n",
    "        self.output_dim = hidden_size * num_layers * (2 if bidirectional else 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        编码器前向过程\n",
    "        :param x: [N,T] token id tensor对象\n",
    "        :return: ([N,T,hidden_size],[N,L])\n",
    "                [N,T,hidden_size] 针对每个文本、每个时刻使用hidden_size维大小的向量进行特征表示\n",
    "                [N,L] 向量矩阵，针对每个文本用一个L维的向量进行表示\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)  # [N,T] -> [N,T,E]\n",
    "        ho, hn = self.rnn(x)  # ho [N,T,hidden_size] hn [?,N,E]\n",
    "        hz = torch.permute(hn, dims=[1, 0, 2])  # [?,N,E] -> [N,?,E]\n",
    "        hz = torch.reshape(hz, shape=(hz.shape[0], self.output_dim))  # [N,?,E] -> [N,?*E] ?*E就是L\n",
    "        return ho, hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModule(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, embedding_dim, encoder_state_dim,\n",
    "                 hidden_size=None, num_layers=1, eos_token_id=0, max_seq_length=20\n",
    "                 ):\n",
    "        super(DecoderModule, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.hidden_size = hidden_size or embedding_dim\n",
    "        assert num_layers == 1, \"当前解码器仅支持单层结构!\"\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_state_proj = nn.Sequential(\n",
    "            nn.Linear(in_features=encoder_state_dim, out_features=self.hidden_size * self.num_layers),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim + self.hidden_size, hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers, batch_first=True, bidirectional=False\n",
    "        )\n",
    "        # 当前模拟代码中，类别数目和词汇表数目一致\n",
    "        self.proj = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "        # 解码器属性\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.rnn_cell = nn.RNNCell(input_size=embedding_dim, hidden_size=self.hidden_size)\n",
    "        self.rnn_cell.weight_ih = self.rnn.weight_ih_l0\n",
    "        self.rnn_cell.weight_hh = self.rnn.weight_hh_l0\n",
    "        self.rnn_cell.bias_ih = self.rnn.bias_ih_l0\n",
    "        self.rnn_cell.bias_hh = self.rnn.bias_hh_l0\n",
    "\n",
    "    def forward(self, x, encoder_state, encoder_outputs):\n",
    "        \"\"\"\n",
    "        解码器的前向过程\n",
    "        :param x: [N,T] 训练的时候，是token id列表，T为实际长度；预测的时候T为1\n",
    "        :param encoder_state: [N,encoder_state_dim] 解码器的初始状态信息 ---> 一般来源于编码器的输出\n",
    "        :param encoder_outputs: [N,T,hidden_size]\n",
    "        :return: [N,T,vocab_size] N个文本，对应T个时刻，每个时刻预测的类别置信度值\n",
    "            NOTE: 训练的时候返回值中的T和x中的T一致，推理预测的时候不一致\n",
    "        \"\"\"\n",
    "        # 将编码器传递过来的状态信息进行转换，作为解码器的初始状态信息\n",
    "        init_state = self.rnn_state_proj(encoder_state)  # [N,encoder_state_dim] -> [N,hidden_size*num_layers]\n",
    "        init_state = torch.reshape(init_state, shape=(-1, self.hidden_size, self.num_layers))\n",
    "        init_state = torch.permute(init_state, dims=[2, 0, 1])\n",
    "\n",
    "        # embedding操作\n",
    "        x = self.embedding_layer(x)  # [N,T] -> [N,T,E]\n",
    "\n",
    "        if self.training:\n",
    "            # 将当前实际输入和编码器的输出合并，形成新的解码器输入\n",
    "            x = torch.concat([x, encoder_outputs], dim=2)  # [N,T,E+hidden_size]\n",
    "            output, _ = self.rnn(x, init_state)  # output -> [n,T,hidden_size]\n",
    "            scores = self.proj(output)  # [n,T,vocab_size]\n",
    "            return scores\n",
    "        else:\n",
    "            # 需要进行遍历操作，每个时刻每个时刻进行预测，直到预测结果为eos_token_id或者预测的序列长度超过阈值的时候，结束预测\n",
    "            outputs = []\n",
    "            hx = init_state[0]  # 第一层的rnn的状态信息\n",
    "            xi = x[:, 0, :]\n",
    "            n, _ = xi.shape\n",
    "            eos_token_ids, is_eos = None, None\n",
    "            max_seq_length = encoder_outputs.shape[1]\n",
    "            encoder_output_i = encoder_outputs[:, 0, :]\n",
    "            while len(outputs) < max_seq_length:\n",
    "                # 当前rnn的输入: x和状态信息 --> 获取当前rnn的输出\n",
    "                xi = torch.concat([xi, encoder_output_i], dim=1)\n",
    "                hx = self.rnn_cell(xi, hx)  # [N,E]\n",
    "                oi = hx  # RNN的状态信息就是输出信息\n",
    "\n",
    "                # 进一步的特征提取转换，获取当前时刻的预测token id\n",
    "                scores_i = self.proj(oi)  # 得到当前时刻的预测置信度[N,vocab_size]\n",
    "                token_ids_i = torch.argmax(scores_i, dim=1, keepdim=True)  # 当前预测id [N, 1]\n",
    "                outputs.append(token_ids_i)\n",
    "\n",
    "                if len(outputs) >= max_seq_length:\n",
    "                    break\n",
    "\n",
    "                # 更新下一个时刻的输入 --> 将当前时刻的预测token id作为下一个时刻的输入\n",
    "                xi = self.embedding_layer(token_ids_i)[:, 0, :]\n",
    "                encoder_output_i = encoder_outputs[:, len(outputs), :]\n",
    "                # encoder_output_i = attention_f(encoder_outputs, hx) # [N,hidden_size]\n",
    "            outputs = torch.concat(outputs, dim=1)  # [N,T2]\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,\n",
    "                 encoder_num_layers=1, encoder_bidirectional=True, encoder_hidden_size=None,\n",
    "                 decoder_num_layers=1, decoder_vocab_size=None, decoder_embedding_dim=None, decoder_hidden_size=None,\n",
    "                 eos_token_id=0,\n",
    "\n",
    "                 ):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderModule(\n",
    "            vocab_size, embedding_dim, hidden_size=encoder_hidden_size,\n",
    "            num_layers=encoder_num_layers, bidirectional=encoder_bidirectional\n",
    "        )\n",
    "        self.decoder = DecoderModule(\n",
    "            decoder_vocab_size or vocab_size, decoder_embedding_dim or embedding_dim,\n",
    "            hidden_size=encoder_hidden_size * (2 if encoder_bidirectional else 1),\n",
    "            encoder_state_dim=self.encoder.output_dim, num_layers=decoder_num_layers,\n",
    "            # eos_token_id=eos_token_id,\n",
    "            eos_token_id=6,  # 临时更改，为了预测退出逻辑\n",
    "            max_seq_length=200\n",
    "        )\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, encoder_input_ids, label_ids=None):\n",
    "        \"\"\"\n",
    "        前向过程： 前向预测 + loss\n",
    "        NOTE: loss仅在训练的时候计算\n",
    "        :param encoder_input_ids: [N,T1] token id tensor列表\n",
    "        :param label_ids: [N,T2] 训练时候给定的标签id列表，推理预测的时候为None\n",
    "        :return: [N,T2,vocab_size], loss\n",
    "        \"\"\"\n",
    "        # 1. 基于编码器提取特征\n",
    "        o, c = self.encoder(encoder_input_ids)\n",
    "        # 2. 解码器操作\n",
    "        if self.training:\n",
    "            # 获取解码器的信息：解码器的输入label_ids的偏移 + 编码器的状态信息\n",
    "            eos_ids = torch.zeros(size=(label_ids.shape[0], 1), dtype=label_ids.dtype)\n",
    "            torch.fill_(eos_ids, self.eos_token_id)\n",
    "            shift_decoder_input_ids = torch.concat([eos_ids, label_ids], dim=1)  # [N,T2+1]\n",
    "            scores = self.decoder(shift_decoder_input_ids[:, :-1], c, o)  # [N,T2+1,vocab_size]\n",
    "            # 损失的计算\n",
    "            loss = self.loss_fn(torch.permute(scores, dims=[0, 2, 1]), label_ids)\n",
    "            return scores, loss\n",
    "        else:\n",
    "            # 构建解码器第一个时刻的输入\n",
    "            eos_ids = torch.zeros(size=(encoder_input_ids.shape[0], 1), dtype=torch.long)\n",
    "            torch.fill_(eos_ids, self.eos_token_id)\n",
    "            token_ids = self.decoder(eos_ids, c, o)  # [N,T2+1,vocab_size]\n",
    "            return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 28])\n",
      "tensor(3.2971, grad_fn=<NllLoss2DBackward0>)\n",
      "tensor([[23, 17, 23, 17,  2],\n",
      "        [18,  3, 23, 17, 17]])\n"
     ]
    }
   ],
   "source": [
    "# 假定场景: 词典大小为26个字母 + 一个特殊值<EOS> + 一个特殊值<PAD>\n",
    "# 有一条样本，编码器的输入: a b c，解码器的最终输出: w x y z\n",
    "# 对数据做转换:\n",
    "# ** 编码器输入: a b c\n",
    "# ** 解码器输入: <EOS> w x y z\n",
    "# ** 解码器输出: w x y z <EOS>\n",
    "# 解码器理解成序列生成，生成序列的时候是不是要一个字符/token一个字符/token来生成，在生成当前token/字符的时候，和是之前的token有强烈的关联关系的\n",
    "# 词典映射关系: {<EOS>:0, a:1, b:2, c:3, ......, w:23, x:24, y:25, z:26, <PAD>:27}\n",
    "x_id = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 5, 5, 5]\n",
    "])\n",
    "label_ids = torch.tensor([\n",
    "    [23, 24, 25, 26, 10],\n",
    "    [23, 24, 25, 26, 11]\n",
    "])\n",
    "net = Seq2SeqModel(\n",
    "    vocab_size=28,\n",
    "    embedding_dim=4,\n",
    "    encoder_num_layers=1,\n",
    "    encoder_hidden_size=16,\n",
    "    eos_token_id=0\n",
    ")\n",
    "_scores, _loss = net(x_id, label_ids)\n",
    "print(_scores.shape)\n",
    "print(_loss)\n",
    "\n",
    "net.eval()\n",
    "_predict_token_ids = net(x_id)\n",
    "print(_predict_token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
