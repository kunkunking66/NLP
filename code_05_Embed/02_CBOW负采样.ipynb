{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T10:48:13.648862Z",
     "start_time": "2024-11-20T10:48:12.612501Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, k=20, vocab_weights=None):\n",
    "        super(CBOW, self).__init__()\n",
    "        # 当前embedding layer和全连接中使用的是同一个w\n",
    "        weight = nn.Parameter(torch.randn((vocab_size, embedding_dim), dtype=torch.float32))\n",
    "        self.emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.output_layer = nn.Linear(in_features=embedding_dim, out_features=vocab_size, bias=False)\n",
    "        self.emb_layer.weight = weight\n",
    "        self.output_layer.weight = weight\n",
    "\n",
    "        # 为了负采样的时候，更好的提取类别对应的参数\n",
    "        self.output_emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.output_emb_layer.weight = self.output_layer.weight  # 参数覆盖\n",
    "        self.k = k  # 负采样的类别数量\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_indexes = np.asarray(list(range(vocab_size)))\n",
    "        self.vocab_weights = vocab_weights  # [] 或者 None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def random_negative_indexes(self, k, pos_indexes):\n",
    "        \"\"\"\n",
    "        随机获取k个负样本的index值\n",
    "        :param k: int值\n",
    "        :param pos_indexes: tensor [N,]\n",
    "        :return:  tensor [k+?,]\n",
    "        \"\"\"\n",
    "        pos_ = pos_indexes.detach().numpy()\n",
    "        k = k + len(pos_)\n",
    "        # replace: True允许抽样的过程中存在重复的数据； False表示不允许\n",
    "        samples_ = np.random.choice(self.vocab_indexes, size=k, replace=True, p=self.vocab_weights)\n",
    "        neg_indexes = []\n",
    "        for _lab in samples_:\n",
    "            if _lab in pos_:\n",
    "                continue\n",
    "            neg_indexes.append(_lab)\n",
    "        if len(neg_indexes) == 0:\n",
    "            return self.random_negative_indexes(k - len(pos_), pos_indexes)\n",
    "        else:\n",
    "            return torch.tensor(neg_indexes, dtype=torch.long)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [N,T] long\n",
    "        :param y: [N,] long 类别单词标签\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z1 = self.emb_layer(x)  # [N,T] --> [N,T,embedding_dim]\n",
    "        z2 = torch.mean(z1, dim=1)  # [N,T,embedding_dim] -+-> [N,embedding_dim]\n",
    "\n",
    "        if self.training:\n",
    "            # 希望获取每个样本属于1+k个单词类别的置信度，1表示实际类别置信度，k表示\"负样本\"/不是实际类别的置信度\n",
    "            # 获取每个样本实际类别的置信度\n",
    "            pos_weights = self.output_emb_layer(y)  # [N,] -> [N,embedding_dim]\n",
    "            # [N,embedding_dim]*[N,embedding_dim] --> [N,embedding_dim] --> [N,] --> [N,1]\n",
    "            pos_scores = torch.reshape(torch.sum(z2 * pos_weights, dim=1), (-1, 1))  # [N, 1]\n",
    "            # 获取负样本的置信度\n",
    "            k = self.k\n",
    "            neg_y = self.random_negative_indexes(k=k, pos_indexes=y)\n",
    "            # neg_y = torch.randint(self.vocab_size, size=(k,))  # 产生k个随机数 --> NOTE: 需要修改\n",
    "            neg_weights = self.output_emb_layer(neg_y)  # [k,] -> [k,embedding_dim]\n",
    "            print (f\"neg_weights.shape: {neg_weights.shape}\")\n",
    "            # [N,embedding_dim] dot [k,embedding_dim].T --> [N,k]\n",
    "            neg_scores = torch.matmul(z2, neg_weights.T)  # [N, k]\n",
    "            # 合并到一起\n",
    "            scores = torch.cat([pos_scores, neg_scores], dim=1)  # [N,1+k]\n",
    "        else:\n",
    "            scores = self.output_layer(z2)  # [N,embedding_dim] --> [N,vocab_size]  得到的是每个样本对应各个单词类别的置信度\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50000])\n",
      "tensor(2.4187, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(2.0211, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000  # 词汇表大小，也就是单词类别数目\n",
    "batch_size = 16\n",
    "window_size = 4\n",
    "\n",
    "net = CBOW(vocab_size=vocab_size, embedding_dim=128)\n",
    "\n",
    "x = torch.randint(vocab_size, size=(batch_size, window_size), dtype=torch.long)  # [N,T]\n",
    "y = torch.randint(vocab_size, size=(batch_size,), dtype=torch.long)  # [N,]\n",
    "\n",
    "net.eval()\n",
    "scores = net(x, y)  # [n,vocab_size]\n",
    "print(scores.shape)\n",
    "\n",
    "# 损失：希望样本预测属于实际类别的置信度要越大越好，如果可以的话，要求预测不属于实际类别的置信度越小越好\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "y_onehot = F.one_hot(y, vocab_size).to(torch.float32)  # [n, vocab_size]\n",
    "loss = loss_fn(scores, y_onehot)\n",
    "print(loss)\n",
    "prob = torch.sigmoid(scores)  # [n, vocab_size]\n",
    "loss2 = -torch.mean(torch.sum(y_onehot * torch.log(prob + 1e-8), dim=1))  # 只更新当前样本对应类别的参数w\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_weights.shape: torch.Size([22, 128])\n",
      "torch.Size([2, 23])\n",
      "==================================================\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.]])\n",
      "tensor(2.7845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(2.7841, grad_fn=<NegBackward0>)\n",
      "debug查看梯度值\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500  # 词汇表大小，也就是单词类别数目\n",
    "word_counts = np.random.randint(1, 100, size=(vocab_size,))  # 保存的是每个单词/类别出现的数量 --> 从数据集统计出来的\n",
    "word_counts = np.power(word_counts, 0.75)\n",
    "word_weights = word_counts / np.sum(word_counts)\n",
    "batch_size = 1\n",
    "window_size = 4\n",
    "\n",
    "net = CBOW(vocab_size=vocab_size, embedding_dim=128, vocab_weights=word_weights)\n",
    "opt = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "x = torch.tensor([\n",
    "    [3, 5, 8, 1],\n",
    "    [3, 2, 9, 1]\n",
    "], dtype=torch.long)\n",
    "y = torch.tensor([12, 13], dtype=torch.long)\n",
    "\n",
    "scores = net(x, y)  # [n,vocab_size]\n",
    "print(scores.shape)\n",
    "print (\"=\"*50)\n",
    "\n",
    "y = torch.zeros_like(y)  # 当前实际类别为0\n",
    "# 损失：希望样本预测属于实际类别的置信度要越大越好，如果可以的话，要求预测不属于实际类别的置信度越小越好\n",
    "y_onehot = F.one_hot(y, scores.shape[1]).to(torch.float32)  # [n, vocab_size]\n",
    "print (y_onehot)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(scores, y_onehot)\n",
    "print(loss)\n",
    "\n",
    "prob = torch.sigmoid(scores)  # [n, 1+k]\n",
    "# loss2 = -torch.mean(torch.sum(y_onehot * torch.log(prob + 1e-8), dim=1))  # 只更新当前样本对应类别的参数w\n",
    "loss2 = -torch.mean(y_onehot * torch.log(prob + 1e-8) + (1 - y_onehot) * torch.log(1.0 - prob + 1e-8))\n",
    "print(loss2)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss2.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
