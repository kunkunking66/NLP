{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:54:58.266881Z",
     "start_time": "2024-10-31T12:54:53.926609Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.onnx"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:55:01.706796Z",
     "start_time": "2024-10-31T12:55:01.699285Z"
    }
   },
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 定义卷积层\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # Pool1\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv2\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # AdaptiveMaxPool2d\n",
    "            nn.AdaptiveMaxPool2d(output_size=(4, 4)),\n",
    "        )\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.classify = nn.Sequential(\n",
    "            # FC1\n",
    "            nn.Linear(50 * 4 * 4, 500),\n",
    "            nn.ReLU(),\n",
    "            # FC2\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过卷积层\n",
    "        x = self.features(x)\n",
    "        # 展平为一维向量\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 通过全连接层\n",
    "        x = self.classify(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = LeNet()\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): AdaptiveMaxPool2d(output_size=(4, 4))\n",
      "  )\n",
      "  (classify): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:55:04.632172Z",
     "start_time": "2024-10-31T12:55:04.626796Z"
    }
   },
   "source": [
    "model.features[2]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx\n",
    "Open Neural Network Exchange\n",
    "\n",
    "导出模型为 ONNX 格式有许多重要的作用，主要体现在以下几个方面：\n",
    "\n",
    "1. **跨平台兼容性**：\n",
    "   - ONNX 支持多种深度学习框架（如 TensorFlow、PyTorch、Caffe2、MXNet 等）之间的模型互操作性。这意味着您可以在一个框架中训练模型，然后在另一个框架中部署它，从而实现模型的迁移和复用。\n",
    "\n",
    "2. **硬件加速**：\n",
    "   - ONNX 支持多种硬件加速库，如 Intel 的 OpenVINO、NVIDIA 的 TensorRT 等。这些库可以针对特定硬件（如 CPU、GPU、FPGA 等）优化模型的推理速度和性能。\n",
    "\n",
    "3. **简化部署**：\n",
    "   - 将模型导出为 ONNX 格式可以简化从训练环境到生产环境的部署流程。ONNX 模型可以在多种平台上运行，包括移动设备、边缘计算设备和服务器。\n",
    "\n",
    "4. **模型验证**：\n",
    "   - 导出为 ONNX 格式可以帮助验证模型的正确性和一致性。ONNX 提供了工具来检查模型的有效性，并且可以在不同的框架中测试模型的一致性。\n",
    "\n",
    "5. **可视化和调试**：\n",
    "   - ONNX 模型可以使用工具（如 Netron）进行可视化，这对于理解模型结构、调试模型和优化模型都非常有用。\n",
    "\n",
    "6. **统一接口**：\n",
    "   - ONNX 提供了一种标准化的方式来表示机器学习模型，使得不同框架之间的接口更加统一，降低了跨框架使用的难度。\n",
    "\n",
    "7. **模型优化**：\n",
    "   - ONNX 支持多种优化技术，如量化、剪枝等，这些技术可以帮助减少模型大小、提高推理速度和降低功耗。\n",
    "\n",
    "8. **多框架支持**：\n",
    "   - ONNX 社区持续增长，越来越多的框架和工具开始支持 ONNX 格式，这为开发者提供了更多的选择和灵活性。\n",
    "\n",
    "### 示例应用\n",
    "\n",
    "- **移动应用开发**：在移动应用中部署深度学习模型时，通常会将模型导出为 ONNX 格式，然后使用支持 ONNX 的移动框架（如 CoreML 或 MLKit）进行推理。\n",
    "  \n",
    "- **边缘计算**：在边缘设备上部署模型时，ONNX 格式可以帮助优化模型在有限资源下的性能。\n",
    "\n",
    "- **云计算**：在云服务中，ONNX 可以帮助实现模型的快速部署和优化，提高服务的响应速度。\n",
    "\n",
    "导出模型为 ONNX 格式是一个非常有用的步骤，特别是在将模型从研究阶段过渡到生产阶段时。通过 ONNX，您可以更好地管理和优化模型的整个生命周期。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:29:30.800461Z",
     "start_time": "2024-10-31T13:29:28.286453Z"
    }
   },
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 创建示例输入\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# 导出模型为 ONNX 格式\n",
    "output_file = \"lenet.onnx\"\n",
    "torch.onnx.export(model, dummy_input, output_file,\n",
    "                  export_params=True,        # 存储训练过的参数\n",
    "                  opset_version=10,         # ONNX 版本\n",
    "                  do_constant_folding=True, # 是否执行常量折叠优化\n",
    "                  input_names=['input'],    # 输入名称\n",
    "                  output_names=['output'],  # 输出名称\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}} # 批次大小动态\n",
    "                  )\n",
    "\n",
    "print(f\"ONNX model exported to {output_file}\")"
   ],
   "outputs": [
    {
     "ename": "OnnxExporterError",
     "evalue": "Module onnx is not installed!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:220\u001B[0m, in \u001B[0;36m_add_onnxscript_fn\u001B[1;34m(model_bytes, custom_opsets)\u001B[0m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 220\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01monnx\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\onnx\\__init__.py:77\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx_cpp2py_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ONNX_ML\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexternal_data_helper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     79\u001B[0m     load_external_data_for_model,\n\u001B[0;32m     80\u001B[0m     write_external_data_tensors,\n\u001B[0;32m     81\u001B[0m     convert_model_to_external_data,\n\u001B[0;32m     82\u001B[0m )\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing onnx_cpp2py_export: 动态链接库(DLL)初始化例程失败。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOnnxExporterError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 导出模型为 ONNX 格式\u001B[39;00m\n\u001B[0;32m      8\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlenet.onnx\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 9\u001B[0m torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mexport(model, dummy_input, output_file,\n\u001B[0;32m     10\u001B[0m                   export_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,        \u001B[38;5;66;03m# 存储训练过的参数\u001B[39;00m\n\u001B[0;32m     11\u001B[0m                   opset_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,         \u001B[38;5;66;03m# ONNX 版本\u001B[39;00m\n\u001B[0;32m     12\u001B[0m                   do_constant_folding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;66;03m# 是否执行常量折叠优化\u001B[39;00m\n\u001B[0;32m     13\u001B[0m                   input_names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m],    \u001B[38;5;66;03m# 输入名称\u001B[39;00m\n\u001B[0;32m     14\u001B[0m                   output_names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m],  \u001B[38;5;66;03m# 输出名称\u001B[39;00m\n\u001B[0;32m     15\u001B[0m                   dynamic_axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m}, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m}} \u001B[38;5;66;03m# 批次大小动态\u001B[39;00m\n\u001B[0;32m     16\u001B[0m                   )\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mONNX model exported to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\onnx\\utils.py:551\u001B[0m, in \u001B[0;36mexport\u001B[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001B[0m\n\u001B[0;32m    546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    547\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    548\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExport destination must be specified for torchscript-onnx export.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    549\u001B[0m     )\n\u001B[1;32m--> 551\u001B[0m _export(\n\u001B[0;32m    552\u001B[0m     model,\n\u001B[0;32m    553\u001B[0m     args,\n\u001B[0;32m    554\u001B[0m     f,\n\u001B[0;32m    555\u001B[0m     export_params,\n\u001B[0;32m    556\u001B[0m     verbose,\n\u001B[0;32m    557\u001B[0m     training,\n\u001B[0;32m    558\u001B[0m     input_names,\n\u001B[0;32m    559\u001B[0m     output_names,\n\u001B[0;32m    560\u001B[0m     operator_export_type\u001B[38;5;241m=\u001B[39moperator_export_type,\n\u001B[0;32m    561\u001B[0m     opset_version\u001B[38;5;241m=\u001B[39mopset_version,\n\u001B[0;32m    562\u001B[0m     do_constant_folding\u001B[38;5;241m=\u001B[39mdo_constant_folding,\n\u001B[0;32m    563\u001B[0m     dynamic_axes\u001B[38;5;241m=\u001B[39mdynamic_axes,\n\u001B[0;32m    564\u001B[0m     keep_initializers_as_inputs\u001B[38;5;241m=\u001B[39mkeep_initializers_as_inputs,\n\u001B[0;32m    565\u001B[0m     custom_opsets\u001B[38;5;241m=\u001B[39mcustom_opsets,\n\u001B[0;32m    566\u001B[0m     export_modules_as_functions\u001B[38;5;241m=\u001B[39mexport_modules_as_functions,\n\u001B[0;32m    567\u001B[0m     autograd_inlining\u001B[38;5;241m=\u001B[39mautograd_inlining,\n\u001B[0;32m    568\u001B[0m )\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\onnx\\utils.py:1722\u001B[0m, in \u001B[0;36m_export\u001B[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     (\n\u001B[0;32m   1704\u001B[0m         proto,\n\u001B[0;32m   1705\u001B[0m         export_map,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1719\u001B[0m         node_attr_to_name,\n\u001B[0;32m   1720\u001B[0m     )\n\u001B[0;32m   1721\u001B[0m \u001B[38;5;66;03m# insert function_proto into model_proto.\u001B[39;00m\n\u001B[1;32m-> 1722\u001B[0m proto \u001B[38;5;241m=\u001B[39m onnx_proto_utils\u001B[38;5;241m.\u001B[39m_add_onnxscript_fn(\n\u001B[0;32m   1723\u001B[0m     proto,\n\u001B[0;32m   1724\u001B[0m     custom_opsets,\n\u001B[0;32m   1725\u001B[0m )\n\u001B[0;32m   1726\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[0;32m   1727\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExported graph: \u001B[39m\u001B[38;5;124m\"\u001B[39m, graph)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:222\u001B[0m, in \u001B[0;36m_add_onnxscript_fn\u001B[1;34m(model_bytes, custom_opsets)\u001B[0m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01monnx\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOnnxExporterError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModule onnx is not installed!\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;66;03m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;66;03m# in _export_onnx, the tensors should be saved separately if the proto\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;66;03m# size > 2GB, and if it for some reason did not, the model would fail on\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;66;03m# serialization anyway in terms of the protobuf limitation. So we don't\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# need to worry about > 2GB model getting here.\u001B[39;00m\n\u001B[0;32m    229\u001B[0m model_proto \u001B[38;5;241m=\u001B[39m onnx\u001B[38;5;241m.\u001B[39mload_model_from_string(model_bytes)  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[1;31mOnnxExporterError\u001B[0m: Module onnx is not installed!"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:05:50.411833Z",
     "start_time": "2024-10-31T12:05:50.399823Z"
    }
   },
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# 加载 ONNX 运行时\n",
    "ort_session = onnxruntime.InferenceSession(output_file)\n",
    "\n",
    "# 创建输入数据\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "input_data = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "# 运行模型\n",
    "ort_outputs = ort_session.run(None, {input_name: input_data})\n",
    "\n",
    "print(\"ONNX Runtime output:\")\n",
    "print(ort_outputs)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01monnxruntime\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 加载 ONNX 运行时\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## netron可视化"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:26:28.756878Z",
     "start_time": "2024-10-31T12:26:28.743186Z"
    }
   },
   "source": [
    "import netron\n",
    "\n",
    "# 使用 netron 查看 ONNX 模型\n",
    "netron.start(output_file)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netron'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetron\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 使用 netron 查看 ONNX 模型\u001B[39;00m\n\u001B[0;32m      4\u001B[0m netron\u001B[38;5;241m.\u001B[39mstart(output_file)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'netron'"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
