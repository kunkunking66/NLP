{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T12:44:10.084628Z",
     "start_time": "2024-11-12T12:44:10.066959Z"
    }
   },
   "source": [
    "# 安装方式： pip install pyhanlp -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "import pyhanlp\n",
    "from pyhanlp import *\n",
    "\n",
    "import jpype\n",
    "from jpype import *\n",
    "\n",
    "import time"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyhanlp'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 安装方式： pip install pyhanlp -i https://pypi.tuna.tsinghua.edu.cn/simple/\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyhanlp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyhanlp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjpype\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pyhanlp'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyhanlp 是一个 Python 包，它提供了对 HanLP 的接口，HanLP 是由北京大学自然语言处理与机器翻译研究室开发的一个 Java 库，用于处理中文文本，包括分词、词性标注、命名实体识别等功能。\n",
    "\n",
    "- HanLP 是一个 Java 库，因此首先需要确保你的系统中已安装 Java。你可以通过运行 java -version 来检查是否已安装 Java。\n",
    "- 如果没有安装 Java，你可以访问 Oracle 的官方网站下载最新版的 JDK，并按照指示安装。\n",
    "- java下载：https://www.oracle.com/java/technologies/downloads/?er=221886#jdk23-mac\n",
    "- pyhanlp 依赖于 conjp 和 swig，这两个工具在安装 pyhanlp 之前需要安装。\n",
    "- conda install -c conda-forge python=3.8 openjdk jpype1==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"你好，欢迎使用HanLP汉语处理包！\"\n",
    "sentence2 = \"下雨天地面积水\"\n",
    "sentence3 = \"我新造一个词叫幻想乡你能识别并标注正确词性吗？\"\n",
    "sentence4 = \"我的希望是希望张晚霞的背影被晚霞映红\"\n",
    "sentence5 = \"微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。\"\n",
    "sentence6 = \"wo shi lai zi hu nan de xiao ming\"\n",
    "sentence7 = \"罗伯特和王老师带着学生们去郊游了，但是谭飞和莉莉由于身体原因没有去！\"\n",
    "sentence8 = \"普京和川普两位总统使用四川普通话进行了沟通\"\n",
    "sentence9 = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"\n",
    "sentences = [\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    sentence3,\n",
    "    sentence4,\n",
    "    sentence5,\n",
    "    sentence6,\n",
    "    sentence7,\n",
    "    sentence8,\n",
    "    sentence9\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、分词、词性标注\n",
    "- HanLP中默认在分词的时候，自带词性标注功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基本分词/标准分词\n",
    "基于最短路径方式进行分词，要求内存至少120M以上；功能类似jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T12:44:04.631083Z",
     "start_time": "2024-11-12T12:44:04.586777Z"
    }
   },
   "source": [
    "# 最基本的分词\n",
    "print(HanLP.segment(sentence1))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HanLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 最基本的分词\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(HanLP\u001B[38;5;241m.\u001B[39msegment(sentence1))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'HanLP' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    【单词】\t    【词性】\n",
      "      你好\t      vl\n",
      "       ，\t       w\n",
      "      欢迎\t       v\n",
      "      使用\t       v\n",
      "   HanLP\t      nx\n",
      "      汉语\t      gi\n",
      "      处理\t      vn\n",
      "       包\t       v\n",
      "       ！\t       w\n"
     ]
    }
   ],
   "source": [
    "print(\"%8s\\t%8s\" % (\"【单词】\", \"【词性】\"))\n",
    "for term in HanLP.segment(sentence1):\n",
    "    print(\"%8s\\t%8s\" % (term.word, term.nature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你好/vl, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/gi, 处理/vn, 包/v, ！/w]\n",
      "[下雨天/n, 地面/n, 积水/n]\n",
      "[我/rr, 新造/nz, 一个/mq, 词/n, 叫/vi, 幻想/n, 乡/n, 你/rr, 能/v, 识别/vn, 并/cc, 标注/v, 正确/a, 词性/n, 吗/y, ？/w]\n",
      "[我/rr, 的/ude1, 希望/v, 是/vshi, 希望/v, 张晚霞/nr, 的/ude1, 背影/n, 被/pbei, 晚霞/n, 映红/v]\n",
      "[微软公司/ntc, 於/nz, 1975/m, 年/qt, 由/p, 比/p, 爾/w, ·/w, 蓋茲/n, 和/cc, 保/v, 羅/n, ·/w, 艾/nz, 倫創/w, 立/v, ，/w, 18/m, 年/qt, 啟動/w, 以/p, 智慧/n, 雲/n, 端/v, 、/w, 前端/f, 為/nz, 導/w, 向/p, 的/ude1, 大/a, 改/v, 組/n, 。/w]\n",
      "[wo/nx,  /w, shi/nx,  /w, lai/nx,  /w, zi/nx,  /w, hu/nx,  /w, nan/nx,  /w, de/nx,  /w, xiao/nx,  /w, ming/nx]\n",
      "[罗伯特/nrf, 和/cc, 王/n, 老师/nnt, 带着/vn, 学生/nnt, 们/k, 去/vf, 郊游/vi, 了/ule, ，/w, 但/c, 是/vshi, 谭飞和/nr, 莉莉/nrf, 由于/p, 身体/n, 原因/n, 没有/v, 去/vf, ！/w]\n",
      "[普京/nrf, 和/cc, 川普/nz, 两/m, 位/q, 总统/nnt, 使用/v, 四川/ns, 普通话/n, 进行/vn, 了/ule, 沟通/v]\n",
      "[攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(HanLP.segment(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NLP分词\n",
    "- com.hankcs.hanlp.tokenizer.NLPTokenizer：同时执行词性标注和命名实体识别；底层为感知机模型；模型训练来自9970万字的大型综合语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载类\n",
    "nlp_tokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你好/l, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/nz, 处理/vn, 包/n, ！/w]\n"
     ]
    }
   ],
   "source": [
    "# 基本用法\n",
    "print(nlp_tokenizer.segment(sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(jpype._jclass.java.util.ArrayList,\n",
       " jpype._jclass.com.hankcs.hanlp.seg.common.Term)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg11 = nlp_tokenizer.segment(sentence1)\n",
    "type(seg11), type(seg11[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    【单词】\t    【词性】\n",
      "      你好\t       l\n",
      "       ，\t       w\n",
      "      欢迎\t       v\n",
      "      使用\t       v\n",
      "   HanLP\t      nx\n",
      "      汉语\t      nz\n",
      "      处理\t      vn\n",
      "       包\t       n\n",
      "       ！\t       w\n"
     ]
    }
   ],
   "source": [
    "print(\"%8s\\t%8s\" % (\"【单词】\", \"【词性】\"))\n",
    "for term in nlp_tokenizer.segment(sentence1):\n",
    "    print(\"%8s\\t%8s\" % (term.word, term.nature.toString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你好/l, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/nz, 处理/vn, 包/n, ！/w]\n",
      "[下雨/v, 天地/n, 面积/n, 水/n]\n",
      "[我/r, 新/d, 造/v, 一个/m, 词/n, 叫/v, 幻想乡/ns, 你/r, 能/v, 识别/v, 并/c, 标注/v, 正确/a, 词性/n, 吗/y, ？/w]\n",
      "[我/r, 的/u, 希望/vn, 是/v, 希望/v, 张/q, 晚霞/n, 的/u, 背影/n, 被/p, 晚霞/n, 映红/v]\n",
      "[微软公司/ntc, 於/p, 1975年/t, 由/p, 比爾·蓋茲/nr, 和/c, 保羅·艾倫/nr, 創立/v, ，/w, 18年/t, 啟動/v, 以/p, 智慧/n, 雲端/n, 、/w, 前端/n, 為/v, 導向/n, 的/u, 大/a, 改組/vn, 。/w]\n",
      "[wo shi lai zi hu nan de xiao ming/nx]\n",
      "[罗伯特/nrf, 和/c, 王/nr, 老师/n, 带着/v, 学生/n, 们/k, 去/v, 郊游/v, 了/y, ，/w, 但是/c, 谭飞/nr, 和/c, 莉莉/nrf, 由于/c, 身体/n, 原因/n, 没有/d, 去/v, ！/w]\n",
      "[普京/nrf, 和/c, 川普/nz, 两/m, 位/q, 总统/n, 使用/v, 四川/ns, 普通话/n, 进行/v, 了/u, 沟通/v]\n",
      "[攻城/ns, 狮/Ng, 逆袭/v, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(nlp_tokenizer.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    【单词】\t    【词性】\n",
      "       我\t       r\n",
      "       的\t       u\n",
      "      希望\t      vn\n",
      "       是\t       v\n",
      "      希望\t       v\n",
      "       张\t       q\n",
      "      晚霞\t       n\n",
      "       的\t       u\n",
      "      背影\t       n\n",
      "       被\t       p\n",
      "      晚霞\t       n\n",
      "      映红\t       v\n"
     ]
    }
   ],
   "source": [
    "print(\"%8s\\t%8s\" % (\"【单词】\", \"【词性】\"))\n",
    "for term in nlp_tokenizer.analyze(sentence4):\n",
    "    print(\"%8s\\t%8s\" % (term.getValue(), term.getLabel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 CRF分词\n",
    "- com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer：对新词的识别能力比较强，但是开销大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载类\n",
    "CRFLexicalAnalyzer = SafeJClass(\"com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer\")\n",
    "# 构造对象\n",
    "crf = CRFLexicalAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你, 好, ，, 欢迎, 使用, HanLP汉语, 处理, 包, ！]\n"
     ]
    }
   ],
   "source": [
    "# 基本用法\n",
    "print(crf.segment(sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你, 好, ，, 欢迎, 使用, HanLP汉语, 处理, 包, ！]\n",
      "[下雨天, 地面, 积水]\n",
      "[我, 新造, 一个, 词, 叫, 幻想乡, 你, 能, 识别, 并, 标注, 正确, 词性, 吗, ？]\n",
      "[我, 的, 希望, 是, 希望, 张晚霞, 的, 背影, 被, 晚霞, 映, 红]\n",
      "[微软公司, 於, 1975年, 由, 比爾·蓋茲, 和, 保羅·艾倫, 創立, ，, 18年, 啟動, 以, 智慧, 雲端, 、, 前端, 為, 導向, 的, 大, 改組, 。]\n",
      "[wo shi lai z, i hu nan de xiao ming]\n",
      "[罗伯特, 和, 王, 老师, 带, 着, 学生, 们, 去, 郊游, 了, ，, 但是, 谭飞, 和, 莉莉, 由于, 身体, 原因, 没有, 去, ！]\n",
      "[普京, 和, 川普, 两, 位, 总统, 使用, 四, 川普, 通话, 进行, 了, 沟通]\n",
      "[攻城, 狮逆袭, 单身狗, ，, 迎娶, 白富美, ，, 走, 上, 人生, 巅峰]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(crf.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%8s\\t%8s\" % (\"【单词】\", \"【词性】\"))\n",
    "for term in crf.analyze(sentence4):\n",
    "    print(\"%8s\\t%8s\" % (term.value, term.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(crf.analyze(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 极速分词\n",
    "- com.hankcs.hanlp.tokenizer.SpeedTokenizer：词典的最长分词，速度极快，精度一般，在i7-6700K上跑出4500万字每秒的速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载类\n",
    "SpeedTokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.SpeedTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[罗伯特, 和, 王老, 师, 带着, 学生, 们, 去, 郊游, 了, ，, 但是, 谭, 飞, 和, 莉莉, 由于, 身体, 原因, 没有, 去, ！]\n",
      "SpeedTokenizer分词的速度::2647426.54字每秒\n"
     ]
    }
   ],
   "source": [
    "# 设置属性，关闭词性显示\n",
    "HanLP.Config.ShowTermNature = False\n",
    "# 开始极速分词\n",
    "print(SpeedTokenizer.segment(sentence7))\n",
    "\n",
    "# 计算时间\n",
    "start = time.time()\n",
    "pressure = 100000\n",
    "for i in range(pressure):\n",
    "    SpeedTokenizer.segment(sentence7)\n",
    "cost_time = time.time() - start\n",
    "print(\"SpeedTokenizer分词的速度::%.2f字每秒\" % \n",
    "      (len(sentence) * pressure / cost_time))\n",
    "HanLP.Config.ShowTermNature = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 自定义词典\n",
    "- com.hankcs.hanlp.dictionary.CustomDictionary：CustomDictionary是全局用户自定义词典，可以随时增删，影响所有分词器；代码中添加的单词，不会进行保存。\n",
    "- 可以在hanlp.properties中配置多个用户自定义词典信息，eg：```CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 用户词典1路径;用户词典2路径;```\n",
    "    - 每行一个单词，格式为: 单词 词性A A的词频 词性B B的词频...\n",
    "    - HanLP中全局默认词性为名词n\n",
    "    - 如果词典路径后面空格接词性，表示当前词典中的所有单词默认词性均为该值，比如：全国地名大全.txt ns; 那么表示这个文件中的所有单词默认词性为ns。\n",
    "    - HanLP主要通过统计分词来实现，故添加的单词不一样会识别出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【添加词典前】：\n",
      "攻城 狮 逆袭 单身 狗 ， 迎娶 白富美 ， 走上 人生 巅峰\n"
     ]
    }
   ],
   "source": [
    "result = HanLP.segment(sentence9)\n",
    "result = ' '.join(map(lambda term: term.word, result))\n",
    "print(\"【添加词典前】：\\n{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nz 1024 n 1 \n"
     ]
    }
   ],
   "source": [
    "# 添加自定义单词\n",
    "CustomDictionary.add(\"攻城狮\") # 动态增加\n",
    "CustomDictionary.insert(\"白富美\", \"nz 1024\") # 强行插入\n",
    "# CustomDictionary.remove(\"攻城狮\") # 删除词语\n",
    "CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\") # 动态增加\n",
    "print(CustomDictionary.get(\"单身狗\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【添加词典后】：\n",
      "攻城狮 逆袭 单身狗 ， 迎娶 白富美 ， 走上 人生 巅峰\n"
     ]
    }
   ],
   "source": [
    "result = HanLP.segment(sentence9)\n",
    "result = ' '.join(map(lambda term: term.word, result))\n",
    "print(\"【添加词典后】：\\n{}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、中国人名识别\n",
    "- 在分词器中默认开启中国人名的识别。比如HanLP.segment中使用的分词器\n",
    "- 可以通过下列代码强调调用人名识别\n",
    "- 建议使用NLP或者CRF词法分析器来提取人名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "       \"签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。\",\n",
    "       \"武大靖创世界纪录夺冠，中国代表团平昌首金\",\n",
    "       \"区长庄木弟新年致辞\",\n",
    "       \"朱立伦：两岸都希望共创双赢 习朱历史会晤在即\",\n",
    "       \"陕西首富吴一坚被带走 与令计划妻子有交集\",\n",
    "       \"据美国之音电台网站4月28日报道，8岁的凯瑟琳·克罗尔（凤甫娟）和很多华裔美国小朋友一样，小小年纪就开始学小提琴了。她的妈妈是位虎妈么？\",\n",
    "       \"凯瑟琳和露西（庐瑞媛），跟她们的哥哥们有一些不同。\",\n",
    "       \"王国强、高峰、汪洋、张朝阳光着头、韩寒、小四\",\n",
    "       \"张浩和胡健康复员回家了\",\n",
    "       \"王总和小丽结婚了\",\n",
    "       \"编剧邵钧林和稽道青说\",\n",
    "       \"这里有关天培的有关事迹\",\n",
    "       \"龚学平等领导说,邓颖超生前杜绝超生\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w]\n",
      "[武大靖/nr, 创/v, 世界纪录/nz, 夺冠/vi, ，/w, 中国代表团/nt, 平昌/ns, 首金/n]\n",
      "[区长/nnt, 庄木弟/nr, 新年/t, 致辞/vi]\n",
      "[朱立伦/nr, ：/w, 两岸/n, 都/d, 希望/v, 共创/v, 双赢/n,  /w, 习/v, 朱/ag, 历史/n, 会晤/vn, 在即/vi]\n",
      "[陕西/ns, 首富/n, 吴一坚/nr, 被/pbei, 带走/v,  /w, 与/cc, 令计划/nr, 妻子/n, 有/vyou, 交集/v]\n",
      "[据/p, 美国/nsf, 之/uzhi, 音/n, 电台/nis, 网站/n, 4月/t, 28/m, 日/b, 报道/v, ，/w, 8/m, 岁/qt, 的/ude1, 凯瑟琳·克罗尔/nrf, （/w, 凤甫娟/nr, ）/w, 和/cc, 很多/m, 华裔/n, 美国/nsf, 小朋友/n, 一样/uyy, ，/w, 小小年纪/n, 就/d, 开始/v, 学/v, 小提琴/n, 了/ule, 。/w, 她/rr, 的/ude1, 妈妈/n, 是/vshi, 位/q, 虎妈/nz, 么/y, ？/w]\n",
      "[凯瑟琳/nrf, 和/cc, 露西/nrf, （/w, 庐瑞媛/nr, ）/w, ，/w, 跟/p, 她们/rr, 的/ude1, 哥哥/n, 们/k, 有/vyou, 一些/m, 不同/a, 。/w]\n",
      "[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小四/nr]\n",
      "[张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule]\n",
      "[王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule]\n",
      "[编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v]\n",
      "[这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n]\n",
      "[龚学平/nr, 等/udeng, 领导/n, 说/v, ,/w, 邓颖超/nr, 生前/t, 杜绝/v, 超生/vi]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    term_list = HanLP.segment(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w]\n",
      "[武大靖/nr, 创/v, 世界纪录/nz, 夺冠/vi, ，/w, 中国代表团/nt, 平昌/ns, 首金/n]\n",
      "[区长/nnt, 庄木弟/nr, 新年/t, 致辞/vi]\n",
      "[朱立伦/nr, ：/w, 两岸/n, 都/d, 希望/v, 共创/v, 双赢/n,  /w, 习/v, 朱/ag, 历史/n, 会晤/vn, 在即/vi]\n",
      "[陕西/ns, 首富/n, 吴一坚/nr, 被/pbei, 带走/v,  /w, 与/cc, 令计划/nr, 妻子/n, 有/vyou, 交集/v]\n",
      "[据/p, 美国/nsf, 之/uzhi, 音/n, 电台/nis, 网站/n, 4月/t, 28/m, 日/b, 报道/v, ，/w, 8/m, 岁/qt, 的/ude1, 凯瑟琳·克罗尔/nrf, （/w, 凤甫娟/nr, ）/w, 和/cc, 很多/m, 华裔/n, 美国/nsf, 小朋友/n, 一样/uyy, ，/w, 小小年纪/n, 就/d, 开始/v, 学/v, 小提琴/n, 了/ule, 。/w, 她/rr, 的/ude1, 妈妈/n, 是/vshi, 位/q, 虎妈/nz, 么/y, ？/w]\n",
      "[凯瑟琳/nrf, 和/cc, 露西/nrf, （/w, 庐瑞媛/nr, ）/w, ，/w, 跟/p, 她们/rr, 的/ude1, 哥哥/n, 们/k, 有/vyou, 一些/m, 不同/a, 。/w]\n",
      "[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小四/nr]\n",
      "[张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule]\n",
      "[王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule]\n",
      "[编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v]\n",
      "[这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n]\n",
      "[龚学平/nr, 等/udeng, 领导/n, 说/v, ,/w, 邓颖超/nr, 生前/t, 杜绝/v, 超生/vi]\n"
     ]
    }
   ],
   "source": [
    "# enableNameRecognize：开启/关闭中文名的命名实体识别, 默认为True，表示开启\n",
    "segment = HanLP.newSegment().enableNameRecognize(True);\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "签约/vn 仪式/n 前/f ，/w 秦光荣/nr 、/w 李纪恒/nr 、/w 仇和/nr 等/u 一同/d 会见/v 了/u 参加/v 签约/v 的/u 企业家/n 。/w\n",
      "[武大/ns 靖创/nz 世界纪录/nz]/nt 夺冠/v ，/w 中国代表团/nt 平昌/ns 首金/n\n",
      "区长/n 庄木弟/nr 新年/t 致辞/v\n",
      "朱立伦/nr ：/w 两岸/n 都/d 希望/v 共/d 创/v 双/m 赢/v  习/v 朱/nr 历史/n 会晤/vn 在即/v\n",
      "陕西/ns 首富/n 吴一坚/nr 被/p 带走/v  /Ng 与/c 令/v 计划/n 妻子/n 有/v 交集/n\n",
      "据/p 美国/ns 之/u 音/n 电台/n 网站/n 4月/t 28日/t 报道/v ，/w 8/m 岁/q 的/u 凯瑟琳·克罗尔/nr （/w 凤甫娟/nr ）/w 和/c 很多/m 华裔/n 美国/ns 小朋友/n 一样/a ，/w 小小年纪/n 就/d 开始/v 学/v 小提琴/n 了/y 。/w 她/r 的/u 妈妈/n 是/v 位/q 虎/n 妈么/n ？/w\n",
      "凯瑟琳/nrf 和/c 露西/nrf （/w 庐瑞媛/nr ）/w ，/w 跟/p 她们/r 的/u 哥哥/n 们/k 有/v 一些/m 不同/a 。/w\n",
      "王国强/nr 、/w 高峰/n 、/w 汪洋/nr 、/w 张朝/nr 阳光/n 着/u 头/n 、/w 韩寒/nr 、/w 小四/nr\n",
      "张浩/nr 和/c 胡健康/nr 复员/n 回家/v 了/y\n",
      "王总/nr 和/c 小丽/nr 结婚/v 了/y\n",
      "编剧/n 邵钧林/nr 和/c 稽道/n 青说/n\n",
      "这里/r 有关/p 天培/i 的/u 有关/vn 事迹/n\n",
      "龚学平/nr 等/u 领导/n 说/v ,/w 邓颖超/nr 生前/t 杜绝/v 超生/v\n",
      "==================================================\n",
      "秦光荣\t李纪恒\t仇和\t\n",
      "庄木弟\t\n",
      "朱立伦\t朱\t\n",
      "吴一坚\t\n",
      "凯瑟琳·克罗尔\t凤甫娟\t\n",
      "庐瑞媛\t\n",
      "王国强\t汪洋\t张朝\t韩寒\t小四\t\n",
      "张浩\t胡健康\t\n",
      "王总\t小丽\t\n",
      "邵钧林\t\n",
      "龚学平\t邓颖超\t\n"
     ]
    }
   ],
   "source": [
    "# 加载类\n",
    "CRFLexicalAnalyzer = SafeJClass(\"com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer\")\n",
    "# 构造对象\n",
    "crf = CRFLexicalAnalyzer()\n",
    "\n",
    "# 分词然后查看词性\n",
    "for sentence in sentences:\n",
    "    term_list = crf.analyze(sentence)\n",
    "    print(term_list)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 分词然后查看词性\n",
    "for sentence in sentences:\n",
    "    term_list = crf.analyze(sentence)\n",
    "    flag = False\n",
    "    for word in term_list:\n",
    "        if word.label == 'nr':\n",
    "            print(\"{}\".format(word.getValue()), end='\\t')\n",
    "            flag = True\n",
    "    if flag:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、音译人名识别\n",
    "- 目前分词默认开始音译人名的识别，不用手动开启；下列代码仅为强调；\n",
    "- 目前仅支持HMM的viterbi算法来识别，也就是默认方式下识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "        \"一桶冰水当头倒下，微软的比尔盖茨、Facebook的扎克伯格跟桑德博格、亚马逊的贝索斯、苹果的库克全都不惜湿身入镜，这些硅谷的科技人，飞蛾扑火似地牺牲演出，其实全为了慈善。\",\n",
    "        \"世界上最长的姓名是简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿。\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[一桶/nz, 冰水/n, 当头/vi, 倒下/v, ，/w, 微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nrf, 跟/p, 桑德博格/nrf, 、/w, 亚马逊/nrf, 的/ude1, 贝索斯/nrf, 、/w, 苹果/nf, 的/ude1, 库克/nrf, 全都/d, 不惜/v, 湿身/nz, 入镜/nz, ，/w, 这些/rz, 硅谷/ns, 的/ude1, 科技/n, 人/n, ，/w, 飞蛾扑火/nz, 似地/d, 牺牲/v, 演出/vn, ，/w, 其实/d, 全/a, 为了/p, 慈善/a, 。/w]\n",
      "[世界/n, 上/f, 最长/d, 的/ude1, 姓名/n, 是/vshi, 简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/nrf, 。/w]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    term_list = HanLP.segment(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[一桶/nz, 冰水/n, 当头/vi, 倒下/v, ，/w, 微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nrf, 跟/p, 桑德博格/nrf, 、/w, 亚马逊/nrf, 的/ude1, 贝索斯/nrf, 、/w, 苹果/nf, 的/ude1, 库克/nrf, 全都/d, 不惜/v, 湿身/nz, 入镜/nz, ，/w, 这些/rz, 硅谷/ns, 的/ude1, 科技/n, 人/n, ，/w, 飞蛾扑火/nz, 似地/d, 牺牲/v, 演出/vn, ，/w, 其实/d, 全/a, 为了/p, 慈善/a, 。/w]\n",
      "[世界/n, 上/f, 最长/d, 的/ude1, 姓名/n, 是/vshi, 简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/nrf, 。/w]\n"
     ]
    }
   ],
   "source": [
    "# enableTranslatedNameRecognize：开启/关闭英译名的命名实体识别, 默认为True，表示开启\n",
    "segment = HanLP.newSegment().enableTranslatedNameRecognize(True);\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[一桶/nz, 冰水/n, 当头/vi, 倒下/v, ，/w, 微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nrf, 跟/p, 桑德博格/nrf, 、/w, 亚马逊/nrf, 的/ude1, 贝索斯/nrf, 、/w, 苹果/nf, 的/ude1, 库克/nrf, 全都/d, 不惜/v, 湿身/nz, 入镜/nz, ，/w, 这些/rz, 硅谷/ns, 的/ude1, 科技/n, 人/n, ，/w, 飞蛾扑火/nz, 似地/d, 牺牲/v, 演出/vn, ，/w, 其实/d, 全/a, 为了/p, 慈善/a, 。/w]\n",
      "[世界/n, 上/f, 最长/d, 的/ude1, 姓名/n, 是/vshi, 简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/nrf, 。/w]\n",
      "==================================================\n",
      "比尔盖茨\t扎克伯格\t桑德博格\t亚马逊\t贝索斯\t库克\t\n",
      "简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿\t\n"
     ]
    }
   ],
   "source": [
    "# 修改词性(从识别的nr修改为nt的词性)\n",
    "CustomDictionary.add(\"亚马逊\", \"nt 2\")\n",
    "for sentence in sentences:\n",
    "    term_list = HanLP.segment(sentence)\n",
    "    print(term_list)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 分词然后查看词性\n",
    "for sentence in sentences:\n",
    "    term_list = HanLP.segment(sentence)\n",
    "    flag = False\n",
    "    for term in term_list:\n",
    "        if term.nature.toString() == 'nrf':\n",
    "            print(\"{}\".format(term.word), end='\\t')\n",
    "            flag = True\n",
    "    if flag:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一桶/m 冰水/n 当头/d 倒下/v ，/w 微软/ntc 的/u 比尔盖茨/nrf 、/w Facebook/l 的/u 扎克伯格/nrf 跟/p 桑德博格/n 、/w 亚马逊/nrf 的/u 贝索斯/nrf 、/w 苹果/n 的/u 库克/nrf 全都/d 不惜/v 湿身/n 入镜/v ，/w 这些/r 硅谷/n 的/u 科技/n 人/n ，/w 飞蛾/v 扑火似/v 地/u 牺牲/v 演出/v ，/w 其实/d 全/d 为了/p 慈善/a 。/w\n",
      "世界/n 上/f 最/d 长/a 的/u 姓名/n 是/v 简森·乔伊·亚历/n 山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/n 。/w\n"
     ]
    }
   ],
   "source": [
    "# 不支持音译名识别(CRF)\n",
    "# 加载类\n",
    "CRFLexicalAnalyzer = SafeJClass(\"com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer\")\n",
    "# 构造对象\n",
    "crf = CRFLexicalAnalyzer()\n",
    "# 分词然后查看词性\n",
    "for sentence in sentences:\n",
    "    term_list = crf.analyze(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、日本名识别\n",
    "- 标准分词器默认关闭日本名识别，需要手动开启"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[\n",
    "       \"北川景子参演了林诣彬导演的《速度与激情3》\",\n",
    "       \"林志玲亮相网友:确定不是波多野结衣？\",\n",
    "       \"龟山千广和近藤公园在龟山公园里喝酒赏花\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]\n",
      "[林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不/d, 是/vshi, 波多野结衣/nrj, ？/w]\n",
      "[龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n"
     ]
    }
   ],
   "source": [
    "# enableJapaneseNameRecognize: 开启/关闭日本名识别功能，默认为False，表示关闭\n",
    "segment = HanLP.newSegment().enableJapaneseNameRecognize(True)\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、地名识别\n",
    "- 可以通过词典来识别地名\n",
    "- 标准分类器中地名识别是关闭的，需要手动开启\n",
    "- 建议直接使用NLP感知器词法分析器来提取地名（同时提取地名和机构名）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用NLP词法解析器来实现地名的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"武胜县新学乡政府大楼门前锣鼓喧天\",\n",
    "    \"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word(word, result=[]):\n",
    "    if hasattr(word, \"innerList\"):\n",
    "        for  inner_word in word.innerList:\n",
    "            parse_word(inner_word, result)\n",
    "    result.append([word.getValue(), word.getLabel()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[武胜县/ns 新学乡/ns 政府/n]/nt 大楼/n 门前/s 锣鼓喧天/nr\n",
      "蓝翔/nt 给/v [宁夏/ns 固原市/ns 彭阳县/ns 红河镇/ns 黑牛沟村/ns]/ns 捐赠/v 了/u 挖掘机/n\n",
      "==================================================\n",
      "[['武胜县', 'ns'], ['新学乡', 'ns'], ['政府', 'n'], ['武胜县新学乡政府', 'nt'], ['大楼', 'n'], ['门前', 's'], ['锣鼓喧天', 'nr']]\n",
      "\n",
      "[['蓝翔', 'nt'], ['给', 'v'], ['宁夏', 'ns'], ['固原市', 'ns'], ['彭阳县', 'ns'], ['红河镇', 'ns'], ['黑牛沟村', 'ns'], ['宁夏固原市彭阳县红河镇黑牛沟村', 'ns'], ['捐赠', 'v'], ['了', 'u'], ['挖掘机', 'n']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载类\n",
    "nlp_tokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "for sentence in sentences:\n",
    "    term_list = nlp_tokenizer.analyze(sentence)\n",
    "    print(term_list)\n",
    "print(\"=\" * 50)\n",
    "for sentence in sentences:\n",
    "    word_list = nlp_tokenizer.analyze(sentence)\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        result = parse_word(word, result)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启地名识别前:\n",
      "[武胜县/ns, 新/a, 学/v, 乡政府/n, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑/a, 牛/n, 沟/n, 村/n, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启地名识别前:\")\n",
    "for sentence in sentences:\n",
    "    print(HanLP.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启地名识别后:\n",
      "[武胜县/ns, 新/a, 学/v, 乡政府/n, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启地名识别后:\")\n",
    "segment = HanLP.newSegment().enablePlaceRecognize(True)\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、机构名识别\n",
    "- 可以通过词典来识别机构名\n",
    "- 标准分类器中机构名识别是关闭的，需要手动开启\n",
    "- 建议直接使用NLP感知器词法分析器来提取机构名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用NLP词法解析器来实现地名的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"我在上海小鱼科技有限公司兼职工作，\",\n",
    "    \"我经常在台电大厦的台川喜宴餐厅吃饭，\",\n",
    "    \"偶尔去地中海影城看电影。\",\n",
    "    \"武胜县新学乡政府大楼门前锣鼓喧天\",\n",
    "    \"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word(word, result=[]):\n",
    "    if hasattr(word, \"innerList\"):\n",
    "        for  inner_word in word.innerList:\n",
    "            parse_word(inner_word, result)\n",
    "    result.append(\"%s/%s\" % (word.getValue(), word.getLabel()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/r 在/p [上海/ns 小/a 鱼/n 科技/n 有限公司/n]/nt 兼职/v 工作/vn ，/w\n",
      "我/r 经常/d 在/p 台电/j 大厦/n 的/u 台川/ns 喜宴/n 餐厅/n 吃饭/v ，/w\n",
      "偶尔/d 去/v 地中海/ns 影城/n 看/v 电影/n 。/w\n",
      "[武胜县/ns 新学乡/ns 政府/n]/nt 大楼/n 门前/s 锣鼓喧天/nr\n",
      "蓝翔/nt 给/v [宁夏/ns 固原市/ns 彭阳县/ns 红河镇/ns 黑牛沟村/ns]/ns 捐赠/v 了/u 挖掘机/n\n",
      "==================================================\n",
      "['我/r', '在/p', '上海/ns', '小/a', '鱼/n', '科技/n', '有限公司/n', '上海小鱼科技有限公司/nt', '兼职/v', '工作/vn', '，/w']\n",
      "\n",
      "['我/r', '经常/d', '在/p', '台电/j', '大厦/n', '的/u', '台川/ns', '喜宴/n', '餐厅/n', '吃饭/v', '，/w']\n",
      "\n",
      "['偶尔/d', '去/v', '地中海/ns', '影城/n', '看/v', '电影/n', '。/w']\n",
      "\n",
      "['武胜县/ns', '新学乡/ns', '政府/n', '武胜县新学乡政府/nt', '大楼/n', '门前/s', '锣鼓喧天/nr']\n",
      "\n",
      "['蓝翔/nt', '给/v', '宁夏/ns', '固原市/ns', '彭阳县/ns', '红河镇/ns', '黑牛沟村/ns', '宁夏固原市彭阳县红河镇黑牛沟村/ns', '捐赠/v', '了/u', '挖掘机/n']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载类\n",
    "nlp_tokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "for sentence in sentences:\n",
    "    term_list = nlp_tokenizer.analyze(sentence)\n",
    "    print(term_list)\n",
    "print(\"=\" * 50)\n",
    "for sentence in sentences:\n",
    "    word_list = nlp_tokenizer.analyze(sentence)\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        result = parse_word(word, result)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启机构名识别前:\n",
      "[我/rr, 在/p, 上海/ns, 小鱼/n, 科技/n, 有限公司/nis, 兼职/vn, 工作/vn, ，/w]\n",
      "[我/rr, 经常/d, 在/p, 台电/j, 大厦/n, 的/ude1, 台川/nr, 喜宴/n, 餐厅/nis, 吃饭/vi, ，/w]\n",
      "[偶尔/d, 去/vf, 地中海/nsf, 影城/n, 看/v, 电影/n, 。/w]\n",
      "[武胜县/ns, 新/a, 学/v, 乡政府/n, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑/a, 牛/n, 沟/n, 村/n, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启机构名识别前:\")\n",
    "for sentence in sentences:\n",
    "    print(HanLP.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启机构名识别后:\n",
      "[我/rr, 在/p, 上海/ns, 小鱼/n, 科技/n, 有限公司/nis, 兼职/vn, 工作/vn, ，/w]\n",
      "[我/rr, 经常/d, 在/p, 台电/j, 大厦/n, 的/ude1, 台川喜宴餐厅/nt, 吃饭/vi, ，/w]\n",
      "[偶尔/d, 去/vf, 地中海/nsf, 影城/n, 看/v, 电影/n, 。/w]\n",
      "[武胜县/ns, 新学乡政府/nt, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/nt, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启机构名识别后:\")\n",
    "segment = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、开启所有命名实体的识别\n",
    "- 当前版本仅支持：人名、地名、机构名识别\n",
    "- 可以考虑使用NLP感知器词法分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"微软的比尔盖茨、Facebook的扎克伯格相约来到湖南张家界市永定区合作桥乡八家村永红组进行捐赠活动，在张德彪等人的陪同下，一起在 嘟嘟乐餐馆的醉仙阁餐厅享受了中国美食，席间北川景子等影星也参与了\",\n",
    "    \"我经常在台电大厦的台川喜宴餐厅吃饭，\",\n",
    "    \"偶尔去地中海影城看电影。\",\n",
    "    \"武胜县新学乡政府大楼门前锣鼓喧天\",\n",
    "    \"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word(word, result=[]):\n",
    "    if hasattr(word, \"innerList\"):\n",
    "        for  inner_word in word.innerList:\n",
    "            parse_word(inner_word, result)\n",
    "    result.append(\"%s/%s\" % (word.getValue(), word.getLabel()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微软/ntc 的/u 比尔盖茨/nrf 、/w Facebook/nx 的/u 扎克伯格/nrf 相约/v 来到/v [湖南/ns 张家界市/ns 永定区/ns 合作桥乡/ns 八家村/ns]/ns 永红/nz 组/n 进行/v 捐赠/vn 活动/vn ，/w 在/p 张德彪/nr 等/u 人/n 的/u 陪同/vn 下/f ，/w 一起在 嘟嘟乐/m 餐馆/n 的/u 醉仙阁/vn 餐厅/n 享受/v 了/u 中国/ns 美食/n ，/w 席间/n 北川景子/nrj 等/u 影星/n 也/d 参与/v 了/y\n",
      "我/r 经常/d 在/p 台电/j 大厦/n 的/u 台川/ns 喜宴/n 餐厅/n 吃饭/v ，/w\n",
      "偶尔/d 去/v 地中海/ns 影城/n 看/v 电影/n 。/w\n",
      "[武胜县/ns 新学乡/ns 政府/n]/nt 大楼/n 门前/s 锣鼓喧天/nr\n",
      "蓝翔/nt 给/v [宁夏/ns 固原市/ns 彭阳县/ns 红河镇/ns 黑牛沟村/ns]/ns 捐赠/v 了/u 挖掘机/n\n",
      "==================================================\n",
      "['微软/ntc', '的/u', '比尔盖茨/nrf', '、/w', 'Facebook/nx', '的/u', '扎克伯格/nrf', '相约/v', '来到/v', '湖南/ns', '张家界市/ns', '永定区/ns', '合作桥乡/ns', '八家村/ns', '湖南张家界市永定区合作桥乡八家村/ns', '永红/nz', '组/n', '进行/v', '捐赠/vn', '活动/vn', '，/w', '在/p', '张德彪/nr', '等/u', '人/n', '的/u', '陪同/vn', '下/f', '，/w', '一起在 嘟嘟乐/m', '餐馆/n', '的/u', '醉仙阁/vn', '餐厅/n', '享受/v', '了/u', '中国/ns', '美食/n', '，/w', '席间/n', '北川景子/nrj', '等/u', '影星/n', '也/d', '参与/v', '了/y']\n",
      "\n",
      "['我/r', '经常/d', '在/p', '台电/j', '大厦/n', '的/u', '台川/ns', '喜宴/n', '餐厅/n', '吃饭/v', '，/w']\n",
      "\n",
      "['偶尔/d', '去/v', '地中海/ns', '影城/n', '看/v', '电影/n', '。/w']\n",
      "\n",
      "['武胜县/ns', '新学乡/ns', '政府/n', '武胜县新学乡政府/nt', '大楼/n', '门前/s', '锣鼓喧天/nr']\n",
      "\n",
      "['蓝翔/nt', '给/v', '宁夏/ns', '固原市/ns', '彭阳县/ns', '红河镇/ns', '黑牛沟村/ns', '宁夏固原市彭阳县红河镇黑牛沟村/ns', '捐赠/v', '了/u', '挖掘机/n']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载类\n",
    "nlp_tokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "for sentence in sentences:\n",
    "    term_list = nlp_tokenizer.analyze(sentence)\n",
    "    print(term_list)\n",
    "print(\"=\" * 50)\n",
    "for sentence in sentences:\n",
    "    word_list = nlp_tokenizer.analyze(sentence)\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        result = parse_word(word, result)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启识别前:\n",
      "[微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nrf, 相约/v, 来到/v, 湖南/ns, 张家界市/ns, 永定区/ns, 合作桥乡/ns, 八家村/ns, 永红/nz, 组/n, 进行/vn, 捐赠/v, 活动/vn, ，/w, 在/p, 张德彪/nr, 等/udeng, 人/n, 的/ude1, 陪同/v, 下/f, ，/w, 一起/s, 在/p,  /w, 嘟嘟/o, 乐/a, 餐馆/nis, 的/ude1, 醉仙/nz, 阁/ng, 餐厅/nis, 享受/v, 了/ule, 中国/ns, 美食/n, ，/w, 席间/n, 北川景子/nrj, 等/udeng, 影星/nnd, 也/d, 参与/v, 了/ule]\n",
      "[我/rr, 经常/d, 在/p, 台电/j, 大厦/n, 的/ude1, 台川/nr, 喜宴/n, 餐厅/nis, 吃饭/vi, ，/w]\n",
      "[偶尔/d, 去/vf, 地中海/nsf, 影城/n, 看/v, 电影/n, 。/w]\n",
      "[武胜县/ns, 新/a, 学/v, 乡政府/n, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑/a, 牛/n, 沟/n, 村/n, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启识别前:\")\n",
    "for sentence in sentences:\n",
    "    print(HanLP.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开启识别后:\n",
      "[微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nrf, 相约/v, 来到/v, 湖南/ns, 张家界市/ns, 永定区/ns, 合作桥乡/ns, 八家村/ns, 永红组/nt, 进行/vn, 捐赠/v, 活动/vn, ，/w, 在/p, 张德彪/nr, 等/udeng, 人/n, 的/ude1, 陪同/v, 下/f, ，/w, 一起/s, 在/p,  /w, 嘟嘟/o, 乐/a, 餐馆/nis, 的/ude1, 醉仙/nz, 阁餐厅/nt, 享受/v, 了/ule, 中国/ns, 美食/n, ，/w, 席间/n, 北川景子/nrj, 等/udeng, 影星/nnd, 也/d, 参与/v, 了/ule]\n",
      "[我/rr, 经常/d, 在/p, 台电/j, 大厦/n, 的/ude1, 台川喜宴餐厅/nt, 吃饭/vi, ，/w]\n",
      "[偶尔/d, 去/vf, 地中海/nsf, 影城/n, 看/v, 电影/n, 。/w]\n",
      "[武胜县/ns, 新学乡政府/nt, 大楼/n, 门前/s, 锣鼓喧天/vl]\n",
      "[蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]\n"
     ]
    }
   ],
   "source": [
    "print(\"开启识别后:\")\n",
    "segment = HanLP.newSegment().enableAllNamedEntityRecognize(True)\n",
    "for sentence in sentences:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、关键词抽取\n",
    "- 默认为TextRank关键词抽取: HanLP.extractKeyword\n",
    "- 可选：com.hankcs.hanlp.mining.word.TfIdfCounter（TFIDF关键词抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_10141364001547692745%22%7D&n_type=0&p_from=1\n",
    "sentence = \"\"\"\n",
    "新华社澳门8月1日电（方钊、郭鑫）1日上午，解放军驻澳门部队在新口岸军营隆重举行升国旗仪式和“八一”招待会，庆祝中国人民解放军建军92周年。\n",
    "8时，驻澳部队威武的仪仗队护送国旗，步伐铿锵走向升旗台。军乐队奏响雄壮的《中华人民共和国国歌》，驻澳门部队官兵在司令员徐良才和政委孙文举带领下整齐列队，面向国旗庄严敬礼，目送鲜艳的五星红旗冉冉升起，献上深情祝福。\n",
    "“八一”招待会于11时举行，主礼嘉宾与各界来宾一同观看了纪录片《濠江战旗别样红》，全面了解驻军进澳门20年来履行防务情况。驻澳门部队司令员徐良才和澳门特区行政长官崔世安致辞。\n",
    "徐良才深情回顾了中国人民解放军的光辉历程。他表示，今年是中国人民解放军进驻澳门20周年，驻军自进驻之日起，就坚定不移地贯彻“一国两制”伟大方针，坚定不移地遵守澳门基本法和驻军法，坚定不移地维护澳门繁荣稳定，始终视国家和民族利益高于一切，始终遵守澳门现行社会制度，尊重和支持特区政府依法施政，积极参加社会公益事业，把澳门同胞当亲人。\n",
    "徐良才说，近年来，驻军官兵时刻牢记习主席重要嘱托，深入贯彻习近平强军思想，坚持政治建军、服务大局，坚持任务牵引、练兵备战，坚持依法从严、锤炼作风，部队履行防务能力稳步提升。驻军部队的建设发展，离不开特区各界和澳门同胞的关心，离不开中联办、外交公署等中央驻澳机构的支持，特别是特区政府为驻军有效履行防务创造了良好环境和条件，对此致以衷心的感谢和崇高的敬意。\n",
    "崔世安向驻澳门部队官兵致以节日的祝贺，对驻军一直以来对特区发展的有力支持表示感谢。他表示，20年来，驻澳部队与澳门特区同呼吸、共命运，视驻地为故乡，把居民当亲人，支持特区政府依法施政，积极开展多元化的、丰富多彩的爱民活动，主动参与献血、植树等社会公益活动；与特区政府合办“澳门青年学生军事夏令营”，培养青年“爱国爱澳”的核心价值；在防灾救灾工作上，以高度的责任感，大力支持特区政府。事实证明，驻澳部队是维护“一国两制”的重要力量，是维护澳门繁荣稳定的重要基石，为澳门特区各项事业的进步作出了不懈的努力和巨大的贡献。\n",
    "全国政协副主席何厚铧、中央政府驻澳门联络办公室主任傅自应、外交部驻澳门特派员公署特派员沈蓓莉、驻澳部队政委孙文举、澳门特区立法会主席高开贤等，以及澳门特区政府、中央驻澳机构、澳区全国人大代表、政协委员、社团、高校、往届军事夏令营学生代表等300余人出席了招待会。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【默认TextRank关键词抽取】\n",
      "[澳门, 驻, 部队, 驻军, 特区政府, 驻澳门部队, 支持, 徐, 澳门特区, 依法, 良才, 官兵, 履行, 重要, 防务, 招待会, 中国人民解放军, 国旗, 深情, 坚持]\n"
     ]
    }
   ],
   "source": [
    "print(\"【默认TextRank关键词抽取】\")\n",
    "words = HanLP.extractKeyword(sentence, 20)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【TFIDF关键词抽取】\n",
      "[，, 的, 、, 。, 澳门, 和, \n",
      ", 澳, 驻, 驻军, “, ”, 部队, 了, 特区政府, 徐, 驻澳门部队, 等, 是, 良才]\n"
     ]
    }
   ],
   "source": [
    "# 必须加入停止词(不建议使用)\n",
    "print(\"【TFIDF关键词抽取】\")\n",
    "# class加载\n",
    "tfidf_class = SafeJClass(\"com.hankcs.hanlp.mining.word.TfIdfCounter\")\n",
    "# 对象创建\n",
    "tfidf = tfidf_class()\n",
    "# 抽取\n",
    "words = tfidf.getKeywords(sentence, 20)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、自动摘要\n",
    "- 默认为TextRankSentence抽取: HanLP.extractSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_10141364001547692745%22%7D&n_type=0&p_from=1\n",
    "# sentence = \"\"\"\n",
    "# 算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。\n",
    "# 算法可以宽泛的分为三类，\n",
    "# 一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。\n",
    "# 二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。\n",
    "# 三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。\n",
    "# \"\"\"\n",
    "sentence = \"\"\"\n",
    "新华社澳门8月1日电（方钊、郭鑫）1日上午，解放军驻澳门部队在新口岸军营隆重举行升国旗仪式和“八一”招待会，庆祝中国人民解放军建军92周年。\n",
    "8时，驻澳部队威武的仪仗队护送国旗，步伐铿锵走向升旗台。军乐队奏响雄壮的《中华人民共和国国歌》，驻澳门部队官兵在司令员徐良才和政委孙文举带领下整齐列队，面向国旗庄严敬礼，目送鲜艳的五星红旗冉冉升起，献上深情祝福。\n",
    "“八一”招待会于11时举行，主礼嘉宾与各界来宾一同观看了纪录片《濠江战旗别样红》，全面了解驻军进澳门20年来履行防务情况。驻澳门部队司令员徐良才和澳门特区行政长官崔世安致辞。\n",
    "徐良才深情回顾了中国人民解放军的光辉历程。他表示，今年是中国人民解放军进驻澳门20周年，驻军自进驻之日起，就坚定不移地贯彻“一国两制”伟大方针，坚定不移地遵守澳门基本法和驻军法，坚定不移地维护澳门繁荣稳定，始终视国家和民族利益高于一切，始终遵守澳门现行社会制度，尊重和支持特区政府依法施政，积极参加社会公益事业，把澳门同胞当亲人。\n",
    "徐良才说，近年来，驻军官兵时刻牢记习主席重要嘱托，深入贯彻习近平强军思想，坚持政治建军、服务大局，坚持任务牵引、练兵备战，坚持依法从严、锤炼作风，部队履行防务能力稳步提升。驻军部队的建设发展，离不开特区各界和澳门同胞的关心，离不开中联办、外交公署等中央驻澳机构的支持，特别是特区政府为驻军有效履行防务创造了良好环境和条件，对此致以衷心的感谢和崇高的敬意。\n",
    "崔世安向驻澳门部队官兵致以节日的祝贺，对驻军一直以来对特区发展的有力支持表示感谢。他表示，20年来，驻澳部队与澳门特区同呼吸、共命运，视驻地为故乡，把居民当亲人，支持特区政府依法施政，积极开展多元化的、丰富多彩的爱民活动，主动参与献血、植树等社会公益活动；与特区政府合办“澳门青年学生军事夏令营”，培养青年“爱国爱澳”的核心价值；在防灾救灾工作上，以高度的责任感，大力支持特区政府。事实证明，驻澳部队是维护“一国两制”的重要力量，是维护澳门繁荣稳定的重要基石，为澳门特区各项事业的进步作出了不懈的努力和巨大的贡献。\n",
    "全国政协副主席何厚铧、中央政府驻澳门联络办公室主任傅自应、外交部驻澳门特派员公署特派员沈蓓莉、驻澳部队政委孙文举、澳门特区立法会主席高开贤等，以及澳门特区政府、中央驻澳机构、澳区全国人大代表、政协委员、社团、高校、往届军事夏令营学生代表等300余人出席了招待会。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【默认TextRankSentence摘要抽取】\n",
      "坚定不移地维护澳门繁荣稳定\n",
      "是维护澳门繁荣稳定的重要基石\n",
      "驻澳门部队司令员徐良才和澳门特区行政长官崔世安致辞\n",
      "驻澳部队是维护\n",
      "支持特区政府依法施政\n"
     ]
    }
   ],
   "source": [
    "print(\"【默认TextRankSentence摘要抽取】\")\n",
    "summary_sentences = HanLP.extractSummary(sentence,5)\n",
    "for summary_sentence in summary_sentences:\n",
    "    print(summary_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十、短语提取\n",
    "- 默认为MutualInformationEntropyPhraseExtractor抽取: HanLP.extractPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "算法工程师 \n",
    "算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。 \n",
    "1职位简介 \n",
    "算法工程师是一个非常高端的职位； \n",
    "专业要求：计算机、电子、通信、数学等相关专业； \n",
    "学历要求：本科及其以上的学历，大多数是硕士学历及其以上； \n",
    "语言要求：英语要求是熟练，基本上能阅读国外专业书刊； \n",
    "必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。 \n",
    "2研究方向 \n",
    "视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师 \n",
    "3目前国内外状况 \n",
    "目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。 \n",
    "另外数据挖掘、互联网搜索算法也成为当今的热门方向。 \n",
    "算法工程师逐渐往人工智能方向发展。\n",
    "\"\"\"\n",
    "# sentence = \"\"\"\n",
    "# 新华社澳门8月1日电（方钊、郭鑫）1日上午，解放军驻澳门部队在新口岸军营隆重举行升国旗仪式和“八一”招待会，庆祝中国人民解放军建军92周年。\n",
    "# 8时，驻澳部队威武的仪仗队护送国旗，步伐铿锵走向升旗台。军乐队奏响雄壮的《中华人民共和国国歌》，驻澳门部队官兵在司令员徐良才和政委孙文举带领下整齐列队，面向国旗庄严敬礼，目送鲜艳的五星红旗冉冉升起，献上深情祝福。\n",
    "# “八一”招待会于11时举行，主礼嘉宾与各界来宾一同观看了纪录片《濠江战旗别样红》，全面了解驻军进澳门20年来履行防务情况。驻澳门部队司令员徐良才和澳门特区行政长官崔世安致辞。\n",
    "# 徐良才深情回顾了中国人民解放军的光辉历程。他表示，今年是中国人民解放军进驻澳门20周年，驻军自进驻之日起，就坚定不移地贯彻“一国两制”伟大方针，坚定不移地遵守澳门基本法和驻军法，坚定不移地维护澳门繁荣稳定，始终视国家和民族利益高于一切，始终遵守澳门现行社会制度，尊重和支持特区政府依法施政，积极参加社会公益事业，把澳门同胞当亲人。\n",
    "# 徐良才说，近年来，驻军官兵时刻牢记习主席重要嘱托，深入贯彻习近平强军思想，坚持政治建军、服务大局，坚持任务牵引、练兵备战，坚持依法从严、锤炼作风，部队履行防务能力稳步提升。驻军部队的建设发展，离不开特区各界和澳门同胞的关心，离不开中联办、外交公署等中央驻澳机构的支持，特别是特区政府为驻军有效履行防务创造了良好环境和条件，对此致以衷心的感谢和崇高的敬意。\n",
    "# 崔世安向驻澳门部队官兵致以节日的祝贺，对驻军一直以来对特区发展的有力支持表示感谢。他表示，20年来，驻澳部队与澳门特区同呼吸、共命运，视驻地为故乡，把居民当亲人，支持特区政府依法施政，积极开展多元化的、丰富多彩的爱民活动，主动参与献血、植树等社会公益活动；与特区政府合办“澳门青年学生军事夏令营”，培养青年“爱国爱澳”的核心价值；在防灾救灾工作上，以高度的责任感，大力支持特区政府。事实证明，驻澳部队是维护“一国两制”的重要力量，是维护澳门繁荣稳定的重要基石，为澳门特区各项事业的进步作出了不懈的努力和巨大的贡献。\n",
    "# 全国政协副主席何厚铧、中央政府驻澳门联络办公室主任傅自应、外交部驻澳门特派员公署特派员沈蓓莉、驻澳部队政委孙文举、澳门特区立法会主席高开贤等，以及澳门特区政府、中央驻澳机构、澳区全国人大代表、政协委员、社团、高校、往届军事夏令营学生代表等300余人出席了招待会。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【默认MutualInformationEntropyPhraseExtractor短语抽取】\n",
      "算法工程师\n",
      "算法处理\n",
      "信息算法\n",
      "视频算法\n",
      "一维信息\n",
      "互联网搜索算法\n",
      "人工智能方向\n",
      "信号处理领域\n",
      "图像处理算法\n",
      "基带算法\n"
     ]
    }
   ],
   "source": [
    "print(\"【默认MutualInformationEntropyPhraseExtractor短语抽取】\")\n",
    "phrases = HanLP.extractPhrase(sentence, 10)\n",
    "for phrase in phrases:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十一、拼音转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汉字转拼音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【中文转换为拼音】\n",
      "原文:我的希望是希望张晚霞的背影被晚霞映红\n",
      "[wo3, de5, xi1, wang4, shi4, xi1, wang4, zhang1, wan3, xia2, de5, bei4, ying3, bei4, wan3, xia2, ying4, hong2]\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence4\n",
    "print(\"【中文转换为拼音】\")\n",
    "print(\"原文:{}\".format(sentence))\n",
    "print(HanLP.convertToPinyinList(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汉字转拼音\n",
    "- 仅获取具体拼音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【中文转换为拼音】\n",
      "原文:我的希望是希望张晚霞的背影被晚霞映红\n",
      "【转换结果】:\n",
      "拼音:wo, 音调:3\n",
      "拼音:de, 音调:5\n",
      "拼音:xi, 音调:1\n",
      "拼音:wang, 音调:4\n",
      "拼音:shi, 音调:4\n",
      "拼音:xi, 音调:1\n",
      "拼音:wang, 音调:4\n",
      "拼音:zhang, 音调:1\n",
      "拼音:wan, 音调:3\n",
      "拼音:xia, 音调:2\n",
      "拼音:de, 音调:5\n",
      "拼音:bei, 音调:4\n",
      "拼音:ying, 音调:3\n",
      "拼音:bei, 音调:4\n",
      "拼音:wan, 音调:3\n",
      "拼音:xia, 音调:2\n",
      "拼音:ying, 音调:4\n",
      "拼音:hong, 音调:2\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence4\n",
    "print(\"【中文转换为拼音】\")\n",
    "print(\"原文:{}\".format(sentence))\n",
    "pinyins = HanLP.convertToPinyinList(sentence)\n",
    "print(\"【转换结果】:\")\n",
    "for pinyin in pinyins:\n",
    "    print(\"拼音:{}, 音调:{}\".format(pinyin.getPinyinWithoutTone(), pinyin.getTone()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换获取首字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w d x w s x w z w x d b y b w x y h\n"
     ]
    }
   ],
   "source": [
    "first_char = HanLP.convertToPinyinFirstCharString(sentence, \" \", True)\n",
    "print(first_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十二、简繁体转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【简体中文转换为繁体中文】\n",
      "【原文】:用笔记本电脑写程序，并使用打印机打印出来\n",
      "【转换】:用筆記本電腦寫程序，並使用打印機打印出來\n"
     ]
    }
   ],
   "source": [
    "sentence = \"用笔记本电脑写程序，并使用打印机打印出来\"\n",
    "print(\"【简体中文转换为繁体中文】\")\n",
    "print(\"【原文】:{}\".format(sentence))\n",
    "print(\"【转换】:{}\".format(HanLP.convertToTraditionalChinese(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【繁体中文转换为简体中文】\n",
      "【原文】:以後等妳當上皇后，就能買士多啤梨慶祝了\n",
      "【转换】:以后等你当上皇后，就能买草莓庆祝了\n"
     ]
    }
   ],
   "source": [
    "sentence = \"以後等妳當上皇后，就能買士多啤梨慶祝了\"\n",
    "print(\"【繁体中文转换为简体中文】\")\n",
    "print(\"【原文】:{}\".format(sentence))\n",
    "print(\"【转换】:{}\".format(HanLP.convertToSimplifiedChinese(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十三、依存句法分析\n",
    "- 内部默认使用NeuralNetworkDependencyParser来进行句法分析，对外API为：HanLP.parseDependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【最简单的使用】\n",
      "1\t徐先生\t徐先生\tnh\tnr\t_\t4\t主谓关系\t_\t_\n",
      "2\t还\t还\td\td\t_\t4\t状中结构\t_\t_\n",
      "3\t具体\t具体\ta\tad\t_\t4\t状中结构\t_\t_\n",
      "4\t帮助\t帮助\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "5\t他\t他\tr\tr\t_\t4\t兼语\t_\t_\n",
      "6\t确定\t确定\tv\tv\t_\t4\t动宾关系\t_\t_\n",
      "7\t了\t了\tu\tu\t_\t6\t右附加关系\t_\t_\n",
      "8\t把\t把\tp\tp\t_\t15\t状中结构\t_\t_\n",
      "9\t画\t画\tv\tv\t_\t8\t介宾关系\t_\t_\n",
      "10\t雄鹰\t雄鹰\tn\tn\t_\t9\t动宾关系\t_\t_\n",
      "11\t、\t、\twp\tw\t_\t12\t标点符号\t_\t_\n",
      "12\t松鼠\t松鼠\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "13\t和\t和\tc\tc\t_\t14\t左附加关系\t_\t_\n",
      "14\t麻雀\t麻雀\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "15\t作为\t作为\tv\tv\t_\t6\t动宾关系\t_\t_\n",
      "16\t主攻\t主攻\tv\tvn\t_\t17\t定中关系\t_\t_\n",
      "17\t目标\t目标\tn\tn\t_\t15\t动宾关系\t_\t_\n",
      "18\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"【最简单的使用】\")\n",
    "# 序号、当前词语/标点的原型或词干、当前词语/标点的原型或词干、当前词语的词性（粗粒度）、当前词语的词性（细粒度）、-、\n",
    "# 和谁的关系(当前词语的中心词)、当前词语与中心词的依存关系、-、-\n",
    "print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPOSTAG',\n",
       " 'DEPREL',\n",
       " 'HEAD',\n",
       " 'ID',\n",
       " 'LEMMA',\n",
       " 'NAME',\n",
       " 'NULL',\n",
       " 'POSTAG',\n",
       " 'ROOT',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__javaclass__',\n",
       " '__javavalue__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " 'equals',\n",
       " 'getClass',\n",
       " 'hashCode',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'toString',\n",
       " 'wait']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 句法分析返回的最终对象的属性方法\n",
    "dir(next(HanLP.parseDependency(\"小明把苹果吃了\").iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【依存关系遍历】\n",
      "小明 --(主谓关系)--> 吃\n",
      "把 --(状中结构)--> 吃\n",
      "苹果 --(介宾关系)--> 把\n",
      "吃 --(核心关系)--> ##核心##\n",
      "了 --(右附加关系)--> 吃\n",
      "\n",
      "小明 --(主谓关系)--> 吃\n",
      "把 --(状中结构)--> 吃\n",
      "苹果 --(介宾关系)--> 把\n",
      "吃 --(核心关系)--> ##核心##\n",
      "了 --(右附加关系)--> 吃\n",
      "\n",
      "小明 --(主谓关系)--> \n",
      "吃 --(核心关系)--> \n",
      "##核心##\n"
     ]
    }
   ],
   "source": [
    "print(\"【依存关系遍历】\")\n",
    "sentence = HanLP.parseDependency(\"小明把苹果吃了\")\n",
    "for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "    # 当前单词 --> 关系 --> 上一个单词\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()\n",
    "\n",
    "# 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "word_array = sentence.getWordArray()\n",
    "for word in word_array:\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()\n",
    "\n",
    "# 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "CoNLLWord = SafeJClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "head = word_array[0]\n",
    "while head.HEAD:\n",
    "    if (head == CoNLLWord.ROOT):\n",
    "        print(head.LEMMA)\n",
    "    else:\n",
    "        print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))\n",
    "    head = head.HEAD\n",
    "if (head == CoNLLWord.ROOT):\n",
    "    print(head.LEMMA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
